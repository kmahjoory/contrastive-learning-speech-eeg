{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"id":"V9GXcBGFsH7M","executionInfo":{"status":"ok","timestamp":1677885026940,"user_tz":-60,"elapsed":246,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y4oavag4sH7N","executionInfo":{"status":"ok","timestamp":1677885027297,"user_tz":-60,"elapsed":6,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"a32aa21b-a4b8-4121-92f3-41f2db7e46c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["['.git', '.DS_Store', '.gitignore', 'EEG', 'LICENSE', 'train_cl_eeg2speech_rochester_v1.ipynb', 'train_cl_eeg2speech_rochester_v2.ipynb', '.ipynb_checkpoints', 'train_cl_eeg2speech_rochester_v3_test_old.ipynb', 'runs', 'train_cl_eeg2speech_rochester_v3_test.ipynb', 'train_cl_eeg2speech_rochester_v4_gridseaerch.ipynb', 'train_cl_eeg2speech_2.ipynb', 'train_cl_eeg2speech_rochester_subj_2.ipynb', 'README.md', 'train_eeg2speech_rochester.ipynb', 'train_cl_eeg2speech_rochester_v3.ipynb', 'models', 'run_training_gridsearch.py', 'tools', 'train_cl_eeg2speech_rochester_gridsearch.ipynb', 'train_cl_eeg2speech_rochester_gridsearch_test.ipynb', 'train_cl_eeg2speech_rochester_v4_gridsearch.ipynb', 'run_training_gridserch.ipynb']\n"]}],"source":["\n","import os\n","import sys\n","\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"Colab Notebooks/prj_neuroread_analysis/neuroread/\"\n","GOOGLE_DRIVE_PATH = os.path.join(\"/content\", \"drive\", \"MyDrive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))\n","\n","# Add to sys so we can import .py files.\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","os.chdir(GOOGLE_DRIVE_PATH)\n","\n","# Install unavailable packages\n","import pip\n","def import_or_install(package):\n","    try:\n","        __import__(package)\n","    except ImportError:\n","        pip.main(['install', package])\n","\n","import_or_install(\"mne\")\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sXZzZce9t49N","executionInfo":{"status":"ok","timestamp":1677915696868,"user_tz":-60,"elapsed":18271,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"4a51cc52-1a01-4f6f-a982-5e85737e0256"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7lXE3PqdsH7O","executionInfo":{"status":"ok","timestamp":1677885029107,"user_tz":-60,"elapsed":9,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"31c18c0f-064c-4020-b8ec-448f6098212c"},"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FVG0hQV6sH7P","executionInfo":{"status":"ok","timestamp":1677885029366,"user_tz":-60,"elapsed":267,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"b6dceff1-9344-4920-f368-f6ec606acab9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 89.6 gigabytes of available RAM\n","\n","Fri Mar  3 23:10:29 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    51W / 400W |    963MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}],"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n"," print('Not connected to a GPU')\n","else:\n"," print(gpu_info)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4T_-IJ1psH7P","executionInfo":{"status":"ok","timestamp":1677885029367,"user_tz":-60,"elapsed":5,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"17618c5a-0eaf-4444-a459-eef019d699bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["import os, sys, glob\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","import numpy as np\n","\n","import mne\n","import time\n","\n","\n","from torchsummary import summary\n","from torch.utils.tensorboard import SummaryWriter\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","from tools.train import eval_model_cl\n","from tools.data import unfold_raw, rm_repeated_annotations\n","from tools.load_data import load_data\n","\n","from models.eeg_encoder import EEGEncoder\n","from models.envelope_encoder import EnvelopeEncoder\n","from models.contrastive_eeg_speech import CLEE\n","# -------------------------------------"]},{"cell_type":"code","source":[],"metadata":{"id":"uJ5O0IHQvbBG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"H6PuqpvYsH7P","executionInfo":{"status":"ok","timestamp":1677891402755,"user_tz":-60,"elapsed":1902294,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"655c6123-a07d-43da-e991-663d58b07661"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["-------------------------------------\n","window_size: 640  stride_size_test: 640\n","data_path: ../outputs/rochester_data/natural_speech\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_1_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 48  Num of removed annots: 19  Num of retained annots:  29\n","-------------------------------------\n","N train: 26  N val: 1  N test: 1\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_2_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 65  Num of removed annots: 19  Num of retained annots:  46\n","-------------------------------------\n","N train: 68  N val: 2  N test: 2\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_3_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 47  Num of removed annots: 19  Num of retained annots:  28\n","-------------------------------------\n","N train: 93  N val: 3  N test: 3\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_4_after_ica_raw.fif...\n","    Range : 0 ... 464394 =      0.000 ...  3628.078 secs\n","Ready.\n","Reading 0 ... 464394  =      0.000 ...  3628.078 secs...\n","Initial num of annots: 41  Num of removed annots: 19  Num of retained annots:  22\n","-------------------------------------\n","N train: 112  N val: 4  N test: 4\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_5_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 41  Num of removed annots: 19  Num of retained annots:  22\n","-------------------------------------\n","N train: 131  N val: 5  N test: 5\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_6_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 58  Num of removed annots: 19  Num of retained annots:  39\n","-------------------------------------\n","N train: 167  N val: 6  N test: 6\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_7_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 74  Num of removed annots: 18  Num of retained annots:  56\n","-------------------------------------\n","N train: 220  N val: 7  N test: 7\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_8_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 42  Num of removed annots: 19  Num of retained annots:  23\n","-------------------------------------\n","N train: 240  N val: 8  N test: 8\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_9_after_ica_raw.fif...\n","    Range : 0 ... 464396 =      0.000 ...  3628.094 secs\n","Ready.\n","Reading 0 ... 464396  =      0.000 ...  3628.094 secs...\n","Initial num of annots: 50  Num of removed annots: 19  Num of retained annots:  31\n","-------------------------------------\n","N train: 268  N val: 9  N test: 9\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_10_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 69  Num of removed annots: 18  Num of retained annots:  51\n","-------------------------------------\n","N train: 315  N val: 10  N test: 10\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_11_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 78  Num of removed annots: 19  Num of retained annots:  59\n","-------------------------------------\n","N train: 371  N val: 11  N test: 11\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_12_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 51  Num of removed annots: 19  Num of retained annots:  32\n","-------------------------------------\n","N train: 400  N val: 12  N test: 12\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_13_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 41  Num of removed annots: 19  Num of retained annots:  22\n","-------------------------------------\n","N train: 419  N val: 13  N test: 13\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_14_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 39  Num of removed annots: 19  Num of retained annots:  20\n","-------------------------------------\n","N train: 436  N val: 14  N test: 14\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_15_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 58  Num of removed annots: 17  Num of retained annots:  41\n","-------------------------------------\n","N train: 474  N val: 15  N test: 15\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_16_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 67  Num of removed annots: 19  Num of retained annots:  48\n","-------------------------------------\n","N train: 519  N val: 16  N test: 16\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_17_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 41  Num of removed annots: 19  Num of retained annots:  22\n","-------------------------------------\n","N train: 538  N val: 17  N test: 17\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_18_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 73  Num of removed annots: 19  Num of retained annots:  54\n","-------------------------------------\n","N train: 589  N val: 18  N test: 18\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_19_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 63  Num of removed annots: 19  Num of retained annots:  44\n","-------------------------------------\n","N train: 630  N val: 19  N test: 19\n","Shape Trian: torch.Size([21765, 1, 129, 640])  Shape Val: torch.Size([714, 1, 129, 640])  Shape Test: torch.Size([685, 1, 129, 640])\n","-------------------------------------\n","Shape EEG Train: torch.Size([21765, 1, 128, 640])  Val: torch.Size([714, 1, 128, 640])  Test: torch.Size([685, 1, 128, 640])\n","Mean: 6.165832427962314e-11  Std: 5.718287411582423e-06\n","Shape Env Train: torch.Size([21765, 1, 1, 640])  Val: torch.Size([714, 1, 1, 640])  Test: torch.Size([685, 1, 1, 640])\n","Mean Env: 2.3698980808258057  Std Env: 2.5990054607391357\n","+--------------New model: lr_0.001_bs_256----------------------+\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n","</pre>\n"],"text/plain":["Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">NumExpr defaulting to 8 threads.\n","</pre>\n"],"text/plain":["NumExpr defaulting to 8 threads.\n"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["====== Epoch: 1\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)\n","  return F.conv2d(input, weight, bias, self.stride,\n"]},{"output_type":"stream","name":"stdout","text":["====> Validation loss: 5.6160,  X1 loss: 5.6028   X2 loss: 5.6292\n","====== Epoch: 2\n","====> Validation loss: 5.3987,  X1 loss: 5.3778   X2 loss: 5.4195\n","====== Epoch: 3\n","====> Validation loss: 5.3589,  X1 loss: 5.3378   X2 loss: 5.3800\n","====== Epoch: 4\n","====> Validation loss: 5.2886,  X1 loss: 5.2660   X2 loss: 5.3112\n","====== Epoch: 5\n","====> Validation loss: 5.1663,  X1 loss: 5.1441   X2 loss: 5.1886\n","====== Epoch: 6\n","====> Validation loss: 5.1270,  X1 loss: 5.1099   X2 loss: 5.1441\n","====== Epoch: 7\n","====> Validation loss: 5.0846,  X1 loss: 5.0701   X2 loss: 5.0992\n","====== Epoch: 8\n","====> Validation loss: 5.1721,  X1 loss: 5.1536   X2 loss: 5.1907\n","====== Epoch: 9\n","====> Validation loss: 5.0722,  X1 loss: 5.0601   X2 loss: 5.0843\n","====== Epoch: 10\n","====> Validation loss: 5.0839,  X1 loss: 5.0737   X2 loss: 5.0941\n","====== Epoch: 11\n","====> Validation loss: 5.1204,  X1 loss: 5.1089   X2 loss: 5.1318\n","====== Epoch: 12\n","====> Validation loss: 5.1363,  X1 loss: 5.1240   X2 loss: 5.1485\n","====== Epoch: 13\n","====> Validation loss: 5.0326,  X1 loss: 5.0223   X2 loss: 5.0429\n","====== Epoch: 14\n","====> Validation loss: 5.1234,  X1 loss: 5.1117   X2 loss: 5.1351\n","====== Epoch: 15\n","====> Validation loss: 4.9744,  X1 loss: 4.9595   X2 loss: 4.9892\n","====== Epoch: 16\n","====> Validation loss: 5.0924,  X1 loss: 5.0829   X2 loss: 5.1019\n","====== Epoch: 17\n","====> Validation loss: 5.0013,  X1 loss: 4.9904   X2 loss: 5.0122\n","====== Epoch: 18\n","====> Validation loss: 5.0356,  X1 loss: 5.0204   X2 loss: 5.0509\n","====== Epoch: 19\n","====> Validation loss: 5.0240,  X1 loss: 5.0095   X2 loss: 5.0384\n","====== Epoch: 20\n","====> Validation loss: 4.9759,  X1 loss: 4.9626   X2 loss: 4.9892\n","====== Epoch: 21\n","====> Validation loss: 4.9653,  X1 loss: 4.9501   X2 loss: 4.9805\n","====== Epoch: 22\n","====> Validation loss: 4.9313,  X1 loss: 4.9157   X2 loss: 4.9469\n","====== Epoch: 23\n","====> Validation loss: 5.0217,  X1 loss: 5.0112   X2 loss: 5.0321\n","====== Epoch: 24\n","====> Validation loss: 4.8871,  X1 loss: 4.8753   X2 loss: 4.8989\n","====== Epoch: 25\n","====> Validation loss: 4.8795,  X1 loss: 4.8648   X2 loss: 4.8943\n","====== Epoch: 26\n","====> Validation loss: 4.9688,  X1 loss: 4.9541   X2 loss: 4.9835\n","====== Epoch: 27\n","====> Validation loss: 4.9006,  X1 loss: 4.8878   X2 loss: 4.9134\n","====== Epoch: 28\n","====> Validation loss: 4.8614,  X1 loss: 4.8406   X2 loss: 4.8823\n","====== Epoch: 29\n","====> Validation loss: 4.9198,  X1 loss: 4.9012   X2 loss: 4.9385\n","====== Epoch: 30\n","====> Validation loss: 4.9449,  X1 loss: 4.9257   X2 loss: 4.9640\n","====== Epoch: 31\n","====> Validation loss: 4.8998,  X1 loss: 4.8840   X2 loss: 4.9155\n","====== Epoch: 32\n","====> Validation loss: 4.8634,  X1 loss: 4.8444   X2 loss: 4.8824\n","====== Epoch: 33\n","====> Validation loss: 4.9045,  X1 loss: 4.8876   X2 loss: 4.9213\n","====== Epoch: 34\n","====> Validation loss: 4.9331,  X1 loss: 4.9187   X2 loss: 4.9474\n","====== Epoch: 35\n","====> Validation loss: 4.9109,  X1 loss: 4.8931   X2 loss: 4.9287\n","====== Epoch: 36\n","====> Validation loss: 4.8533,  X1 loss: 4.8360   X2 loss: 4.8705\n","====== Epoch: 37\n","====> Validation loss: 4.8386,  X1 loss: 4.8225   X2 loss: 4.8547\n","====== Epoch: 38\n","====> Validation loss: 4.9548,  X1 loss: 4.9383   X2 loss: 4.9712\n","====== Epoch: 39\n","====> Validation loss: 4.8543,  X1 loss: 4.8396   X2 loss: 4.8690\n","====== Epoch: 40\n","====> Validation loss: 4.9346,  X1 loss: 4.9173   X2 loss: 4.9518\n","====== Epoch: 41\n","====> Validation loss: 4.8271,  X1 loss: 4.8089   X2 loss: 4.8454\n","====== Epoch: 42\n","====> Validation loss: 4.8785,  X1 loss: 4.8598   X2 loss: 4.8973\n","====== Epoch: 43\n","====> Validation loss: 4.8160,  X1 loss: 4.7984   X2 loss: 4.8337\n","====== Epoch: 44\n","====> Validation loss: 4.8862,  X1 loss: 4.8737   X2 loss: 4.8988\n","====== Epoch: 45\n","====> Validation loss: 4.8369,  X1 loss: 4.8186   X2 loss: 4.8553\n","====== Epoch: 46\n","====> Validation loss: 4.8262,  X1 loss: 4.8060   X2 loss: 4.8463\n","====== Epoch: 47\n","====> Validation loss: 4.9128,  X1 loss: 4.8931   X2 loss: 4.9326\n","====== Epoch: 48\n","====> Validation loss: 4.8691,  X1 loss: 4.8499   X2 loss: 4.8882\n","====== Epoch: 49\n","====> Validation loss: 4.9034,  X1 loss: 4.8811   X2 loss: 4.9257\n","====== Epoch: 50\n","====> Validation loss: 4.8858,  X1 loss: 4.8691   X2 loss: 4.9025\n","====== Epoch: 51\n","====> Validation loss: 4.8847,  X1 loss: 4.8687   X2 loss: 4.9008\n","====== Epoch: 52\n","====> Validation loss: 4.8107,  X1 loss: 4.7958   X2 loss: 4.8256\n","====== Epoch: 53\n","====> Validation loss: 4.8550,  X1 loss: 4.8380   X2 loss: 4.8721\n","====== Epoch: 54\n","====> Validation loss: 4.8334,  X1 loss: 4.8186   X2 loss: 4.8482\n","====== Epoch: 55\n","====> Validation loss: 4.8006,  X1 loss: 4.7792   X2 loss: 4.8221\n","====== Epoch: 56\n","====> Validation loss: 4.8698,  X1 loss: 4.8494   X2 loss: 4.8902\n","====== Epoch: 57\n","====> Validation loss: 4.8436,  X1 loss: 4.8301   X2 loss: 4.8572\n","====== Epoch: 58\n","====> Validation loss: 4.7945,  X1 loss: 4.7751   X2 loss: 4.8138\n","====== Epoch: 59\n","====> Validation loss: 4.8867,  X1 loss: 4.8690   X2 loss: 4.9045\n","====== Epoch: 60\n","====> Validation loss: 4.8353,  X1 loss: 4.8227   X2 loss: 4.8479\n","====== Epoch: 61\n","====> Validation loss: 4.8508,  X1 loss: 4.8338   X2 loss: 4.8677\n","====== Epoch: 62\n","====> Validation loss: 4.9003,  X1 loss: 4.8853   X2 loss: 4.9153\n","====== Epoch: 63\n","====> Validation loss: 4.8528,  X1 loss: 4.8348   X2 loss: 4.8709\n","====== Epoch: 64\n","====> Validation loss: 4.8299,  X1 loss: 4.8116   X2 loss: 4.8483\n","====== Epoch: 65\n","====> Validation loss: 4.8512,  X1 loss: 4.8317   X2 loss: 4.8706\n","====== Epoch: 66\n","====> Validation loss: 4.9071,  X1 loss: 4.8878   X2 loss: 4.9265\n","====== Epoch: 67\n","====> Validation loss: 4.8360,  X1 loss: 4.8202   X2 loss: 4.8518\n","====== Epoch: 68\n","====> Validation loss: 4.7818,  X1 loss: 4.7635   X2 loss: 4.8000\n","====== Epoch: 69\n","====> Validation loss: 4.8521,  X1 loss: 4.8354   X2 loss: 4.8688\n","====== Epoch: 70\n","====> Validation loss: 4.8415,  X1 loss: 4.8226   X2 loss: 4.8605\n","====== Epoch: 71\n","====> Validation loss: 4.8583,  X1 loss: 4.8412   X2 loss: 4.8754\n","====== Epoch: 72\n","====> Validation loss: 4.8403,  X1 loss: 4.8208   X2 loss: 4.8598\n","====== Epoch: 73\n","====> Validation loss: 4.8034,  X1 loss: 4.7849   X2 loss: 4.8219\n","====== Epoch: 74\n","====> Validation loss: 4.7271,  X1 loss: 4.7095   X2 loss: 4.7447\n","====== Epoch: 75\n","====> Validation loss: 4.7521,  X1 loss: 4.7332   X2 loss: 4.7709\n","====== Epoch: 76\n","====> Validation loss: 4.7898,  X1 loss: 4.7691   X2 loss: 4.8105\n","====== Epoch: 77\n","====> Validation loss: 4.7637,  X1 loss: 4.7436   X2 loss: 4.7838\n","====== Epoch: 78\n","====> Validation loss: 4.7513,  X1 loss: 4.7306   X2 loss: 4.7719\n","====== Epoch: 79\n","====> Validation loss: 4.8191,  X1 loss: 4.7992   X2 loss: 4.8390\n","====== Epoch: 80\n","====> Validation loss: 4.8184,  X1 loss: 4.8016   X2 loss: 4.8352\n","====== Epoch: 81\n","====> Validation loss: 4.7128,  X1 loss: 4.6920   X2 loss: 4.7337\n","====== Epoch: 82\n","====> Validation loss: 4.8156,  X1 loss: 4.7944   X2 loss: 4.8368\n","====== Epoch: 83\n","====> Validation loss: 4.7881,  X1 loss: 4.7638   X2 loss: 4.8123\n","====== Epoch: 84\n","====> Validation loss: 4.7019,  X1 loss: 4.6795   X2 loss: 4.7242\n","====== Epoch: 85\n","====> Validation loss: 4.8167,  X1 loss: 4.7955   X2 loss: 4.8379\n","====== Epoch: 86\n","====> Validation loss: 4.7436,  X1 loss: 4.7195   X2 loss: 4.7677\n","====== Epoch: 87\n","====> Validation loss: 4.7562,  X1 loss: 4.7345   X2 loss: 4.7779\n","====== Epoch: 88\n","====> Validation loss: 4.7393,  X1 loss: 4.7186   X2 loss: 4.7600\n","====== Epoch: 89\n","====> Validation loss: 4.8421,  X1 loss: 4.8215   X2 loss: 4.8626\n","====== Epoch: 90\n","====> Validation loss: 4.7761,  X1 loss: 4.7540   X2 loss: 4.7983\n","====== Epoch: 91\n","====> Validation loss: 4.7137,  X1 loss: 4.6930   X2 loss: 4.7345\n","====== Epoch: 92\n","====> Validation loss: 4.7279,  X1 loss: 4.7093   X2 loss: 4.7465\n","====== Epoch: 93\n","====> Validation loss: 4.7878,  X1 loss: 4.7661   X2 loss: 4.8094\n","====== Epoch: 94\n","====> Validation loss: 4.7354,  X1 loss: 4.7198   X2 loss: 4.7510\n","====== Epoch: 95\n","====> Validation loss: 4.7208,  X1 loss: 4.7037   X2 loss: 4.7380\n","====== Epoch: 96\n","====> Validation loss: 4.7476,  X1 loss: 4.7246   X2 loss: 4.7705\n","====== Epoch: 97\n","====> Validation loss: 4.7624,  X1 loss: 4.7454   X2 loss: 4.7794\n","====== Epoch: 98\n","====> Validation loss: 4.7621,  X1 loss: 4.7418   X2 loss: 4.7824\n","====== Epoch: 99\n","====> Validation loss: 4.7153,  X1 loss: 4.7003   X2 loss: 4.7303\n","====== Epoch: 100\n","====> Validation loss: 4.7906,  X1 loss: 4.7711   X2 loss: 4.8101\n","====== Epoch: 101\n","====> Validation loss: 4.7352,  X1 loss: 4.7139   X2 loss: 4.7566\n","====== Epoch: 102\n","====> Validation loss: 4.7218,  X1 loss: 4.6967   X2 loss: 4.7470\n","====== Epoch: 103\n","====> Validation loss: 4.7321,  X1 loss: 4.7126   X2 loss: 4.7516\n","====== Epoch: 104\n","====> Validation loss: 4.7513,  X1 loss: 4.7275   X2 loss: 4.7751\n","====== Epoch: 105\n","====> Validation loss: 4.7412,  X1 loss: 4.7259   X2 loss: 4.7565\n","====== Epoch: 106\n","====> Validation loss: 4.7355,  X1 loss: 4.7127   X2 loss: 4.7584\n","====== Epoch: 107\n","====> Validation loss: 4.8040,  X1 loss: 4.7824   X2 loss: 4.8257\n","====== Epoch: 108\n","====> Validation loss: 4.7440,  X1 loss: 4.7282   X2 loss: 4.7598\n","====== Epoch: 109\n","====> Validation loss: 4.6017,  X1 loss: 4.5861   X2 loss: 4.6172\n","====== Epoch: 110\n","====> Validation loss: 4.7895,  X1 loss: 4.7695   X2 loss: 4.8094\n","====== Epoch: 111\n","====> Validation loss: 4.6716,  X1 loss: 4.6500   X2 loss: 4.6932\n","====== Epoch: 112\n","====> Validation loss: 4.7866,  X1 loss: 4.7630   X2 loss: 4.8103\n","====== Epoch: 113\n","====> Validation loss: 4.7697,  X1 loss: 4.7502   X2 loss: 4.7893\n","====== Epoch: 114\n","====> Validation loss: 4.7601,  X1 loss: 4.7419   X2 loss: 4.7783\n","====== Epoch: 115\n","====> Validation loss: 4.7411,  X1 loss: 4.7199   X2 loss: 4.7623\n","====== Epoch: 116\n","====> Validation loss: 4.7309,  X1 loss: 4.7136   X2 loss: 4.7482\n","====== Epoch: 117\n","====> Validation loss: 4.7593,  X1 loss: 4.7349   X2 loss: 4.7837\n","====== Epoch: 118\n","====> Validation loss: 4.7557,  X1 loss: 4.7325   X2 loss: 4.7789\n","====== Epoch: 119\n","====> Validation loss: 4.7260,  X1 loss: 4.7024   X2 loss: 4.7496\n","====== Epoch: 120\n","====> Validation loss: 4.8265,  X1 loss: 4.8072   X2 loss: 4.8459\n","====== Epoch: 121\n","====> Validation loss: 4.7867,  X1 loss: 4.7637   X2 loss: 4.8097\n","====== Epoch: 122\n","====> Validation loss: 4.6886,  X1 loss: 4.6707   X2 loss: 4.7065\n","====== Epoch: 123\n","====> Validation loss: 4.7170,  X1 loss: 4.6980   X2 loss: 4.7359\n","====== Epoch: 124\n","====> Validation loss: 4.7813,  X1 loss: 4.7583   X2 loss: 4.8042\n","====== Epoch: 125\n","====> Validation loss: 4.6192,  X1 loss: 4.5954   X2 loss: 4.6430\n","====== Epoch: 126\n","====> Validation loss: 4.8258,  X1 loss: 4.8062   X2 loss: 4.8454\n","====== Epoch: 127\n","====> Validation loss: 4.7183,  X1 loss: 4.6962   X2 loss: 4.7405\n","====== Epoch: 128\n","====> Validation loss: 4.7953,  X1 loss: 4.7688   X2 loss: 4.8217\n","====== Epoch: 129\n","====> Validation loss: 4.7747,  X1 loss: 4.7504   X2 loss: 4.7990\n","====== Epoch: 130\n","====> Validation loss: 4.7860,  X1 loss: 4.7661   X2 loss: 4.8059\n","====== Epoch: 131\n","====> Validation loss: 4.7671,  X1 loss: 4.7507   X2 loss: 4.7834\n","====== Epoch: 132\n","====> Validation loss: 4.7135,  X1 loss: 4.6939   X2 loss: 4.7331\n","====== Epoch: 133\n","====> Validation loss: 4.7212,  X1 loss: 4.7015   X2 loss: 4.7410\n","====== Epoch: 134\n","====> Validation loss: 4.7214,  X1 loss: 4.7009   X2 loss: 4.7419\n","====== Epoch: 135\n","====> Validation loss: 4.7670,  X1 loss: 4.7455   X2 loss: 4.7886\n","====== Epoch: 136\n","====> Validation loss: 4.7727,  X1 loss: 4.7538   X2 loss: 4.7917\n","====== Epoch: 137\n","====> Validation loss: 4.7874,  X1 loss: 4.7684   X2 loss: 4.8064\n","====== Epoch: 138\n","====> Validation loss: 4.6876,  X1 loss: 4.6695   X2 loss: 4.7056\n","====== Epoch: 139\n","====> Validation loss: 4.7613,  X1 loss: 4.7389   X2 loss: 4.7838\n","====== Epoch: 140\n","====> Validation loss: 4.6811,  X1 loss: 4.6673   X2 loss: 4.6950\n","====== Epoch: 141\n","====> Validation loss: 4.8059,  X1 loss: 4.7857   X2 loss: 4.8262\n","====== Epoch: 142\n","====> Validation loss: 4.6474,  X1 loss: 4.6318   X2 loss: 4.6631\n","====== Epoch: 143\n","====> Validation loss: 4.7485,  X1 loss: 4.7280   X2 loss: 4.7690\n","====== Epoch: 144\n","====> Validation loss: 4.7546,  X1 loss: 4.7381   X2 loss: 4.7711\n","====== Epoch: 145\n","====> Validation loss: 4.7570,  X1 loss: 4.7362   X2 loss: 4.7778\n","====== Epoch: 146\n","====> Validation loss: 4.6795,  X1 loss: 4.6583   X2 loss: 4.7007\n","====== Epoch: 147\n","====> Validation loss: 4.7569,  X1 loss: 4.7344   X2 loss: 4.7794\n","====== Epoch: 148\n","====> Validation loss: 4.7275,  X1 loss: 4.7073   X2 loss: 4.7476\n","====== Epoch: 149\n","====> Validation loss: 4.7077,  X1 loss: 4.6846   X2 loss: 4.7308\n","====== Epoch: 150\n","====> Validation loss: 4.6833,  X1 loss: 4.6636   X2 loss: 4.7030\n","====== Epoch: 151\n","====> Validation loss: 4.6354,  X1 loss: 4.6112   X2 loss: 4.6595\n","====== Epoch: 152\n","====> Validation loss: 4.6205,  X1 loss: 4.6041   X2 loss: 4.6369\n","====== Epoch: 153\n","====> Validation loss: 4.7762,  X1 loss: 4.7550   X2 loss: 4.7974\n","====== Epoch: 154\n","====> Validation loss: 4.6951,  X1 loss: 4.6763   X2 loss: 4.7138\n","====== Epoch: 155\n","====> Validation loss: 4.6514,  X1 loss: 4.6324   X2 loss: 4.6703\n","====== Epoch: 156\n","====> Validation loss: 4.6105,  X1 loss: 4.5910   X2 loss: 4.6301\n","====== Epoch: 157\n","====> Validation loss: 4.6716,  X1 loss: 4.6483   X2 loss: 4.6949\n","====== Epoch: 158\n","====> Validation loss: 4.7362,  X1 loss: 4.7185   X2 loss: 4.7539\n","====== Epoch: 159\n","====> Validation loss: 4.6765,  X1 loss: 4.6577   X2 loss: 4.6954\n","====== Epoch: 160\n","====> Validation loss: 4.7030,  X1 loss: 4.6824   X2 loss: 4.7235\n","====== Epoch: 161\n","====> Validation loss: 4.7506,  X1 loss: 4.7306   X2 loss: 4.7707\n","====== Epoch: 162\n","====> Validation loss: 4.7344,  X1 loss: 4.7165   X2 loss: 4.7522\n","====== Epoch: 163\n","====> Validation loss: 4.7273,  X1 loss: 4.7118   X2 loss: 4.7428\n","====== Epoch: 164\n","====> Validation loss: 4.6905,  X1 loss: 4.6740   X2 loss: 4.7071\n","====== Epoch: 165\n","====> Validation loss: 4.6929,  X1 loss: 4.6753   X2 loss: 4.7105\n","====== Epoch: 166\n","====> Validation loss: 4.7420,  X1 loss: 4.7203   X2 loss: 4.7637\n","====== Epoch: 167\n","====> Validation loss: 4.7144,  X1 loss: 4.6974   X2 loss: 4.7314\n","====== Epoch: 168\n","====> Validation loss: 4.7671,  X1 loss: 4.7457   X2 loss: 4.7884\n","====== Epoch: 169\n","====> Validation loss: 4.7252,  X1 loss: 4.7105   X2 loss: 4.7400\n","====== Epoch: 170\n","====> Validation loss: 4.7289,  X1 loss: 4.7097   X2 loss: 4.7482\n","====== Epoch: 171\n","====> Validation loss: 4.7154,  X1 loss: 4.6951   X2 loss: 4.7357\n","====== Epoch: 172\n","====> Validation loss: 4.6643,  X1 loss: 4.6419   X2 loss: 4.6866\n","====== Epoch: 173\n","====> Validation loss: 4.5867,  X1 loss: 4.5667   X2 loss: 4.6068\n","====== Epoch: 174\n","====> Validation loss: 4.7126,  X1 loss: 4.6915   X2 loss: 4.7336\n","====== Epoch: 175\n","====> Validation loss: 4.7134,  X1 loss: 4.6920   X2 loss: 4.7347\n","====== Epoch: 176\n","====> Validation loss: 4.7914,  X1 loss: 4.7758   X2 loss: 4.8070\n","====== Epoch: 177\n","====> Validation loss: 4.7402,  X1 loss: 4.7257   X2 loss: 4.7547\n","====== Epoch: 178\n","====> Validation loss: 4.7041,  X1 loss: 4.6821   X2 loss: 4.7261\n","====== Epoch: 179\n","====> Validation loss: 4.6917,  X1 loss: 4.6703   X2 loss: 4.7130\n","====== Epoch: 180\n","====> Validation loss: 4.6737,  X1 loss: 4.6524   X2 loss: 4.6950\n","====== Epoch: 181\n","====> Validation loss: 4.6909,  X1 loss: 4.6700   X2 loss: 4.7118\n","====== Epoch: 182\n","====> Validation loss: 4.7860,  X1 loss: 4.7706   X2 loss: 4.8014\n","====== Epoch: 183\n","====> Validation loss: 4.6975,  X1 loss: 4.6804   X2 loss: 4.7145\n","====== Epoch: 184\n","====> Validation loss: 4.7308,  X1 loss: 4.7092   X2 loss: 4.7523\n","====== Epoch: 185\n","====> Validation loss: 4.7068,  X1 loss: 4.6845   X2 loss: 4.7291\n","====== Epoch: 186\n","====> Validation loss: 4.6412,  X1 loss: 4.6233   X2 loss: 4.6591\n","====== Epoch: 187\n","====> Validation loss: 4.7485,  X1 loss: 4.7287   X2 loss: 4.7682\n","====== Epoch: 188\n","====> Validation loss: 4.6704,  X1 loss: 4.6447   X2 loss: 4.6961\n","====== Epoch: 189\n","====> Validation loss: 4.7467,  X1 loss: 4.7333   X2 loss: 4.7602\n","====== Epoch: 190\n","====> Validation loss: 4.7494,  X1 loss: 4.7312   X2 loss: 4.7675\n","====== Epoch: 191\n","====> Validation loss: 4.6543,  X1 loss: 4.6322   X2 loss: 4.6764\n","====== Epoch: 192\n","====> Validation loss: 4.6920,  X1 loss: 4.6709   X2 loss: 4.7131\n","====== Epoch: 193\n","====> Validation loss: 4.7756,  X1 loss: 4.7563   X2 loss: 4.7949\n","====== Epoch: 194\n","====> Validation loss: 4.6735,  X1 loss: 4.6571   X2 loss: 4.6898\n","====== Epoch: 195\n","====> Validation loss: 4.6467,  X1 loss: 4.6281   X2 loss: 4.6654\n","====== Epoch: 196\n","====> Validation loss: 4.6346,  X1 loss: 4.6175   X2 loss: 4.6518\n","====== Epoch: 197\n","====> Validation loss: 4.7159,  X1 loss: 4.6962   X2 loss: 4.7356\n","====== Epoch: 198\n","====> Validation loss: 4.6949,  X1 loss: 4.6773   X2 loss: 4.7125\n","====== Epoch: 199\n","====> Validation loss: 4.7391,  X1 loss: 4.7212   X2 loss: 4.7571\n","====== Epoch: 200\n","====> Validation loss: 4.6519,  X1 loss: 4.6325   X2 loss: 4.6714\n","====== Epoch: 201\n","====> Validation loss: 4.7401,  X1 loss: 4.7219   X2 loss: 4.7584\n","====== Epoch: 202\n","====> Validation loss: 4.6885,  X1 loss: 4.6699   X2 loss: 4.7070\n","====== Epoch: 203\n","====> Validation loss: 4.7174,  X1 loss: 4.6965   X2 loss: 4.7383\n","====== Epoch: 204\n","====> Validation loss: 4.7254,  X1 loss: 4.7084   X2 loss: 4.7424\n","====== Epoch: 205\n","====> Validation loss: 4.7037,  X1 loss: 4.6836   X2 loss: 4.7239\n","====== Epoch: 206\n","====> Validation loss: 4.7868,  X1 loss: 4.7707   X2 loss: 4.8028\n","====== Epoch: 207\n","====> Validation loss: 4.6388,  X1 loss: 4.6200   X2 loss: 4.6576\n","====== Epoch: 208\n","====> Validation loss: 4.8074,  X1 loss: 4.7899   X2 loss: 4.8249\n","====== Epoch: 209\n","====> Validation loss: 4.7158,  X1 loss: 4.6995   X2 loss: 4.7322\n","====== Epoch: 210\n","====> Validation loss: 4.7233,  X1 loss: 4.7073   X2 loss: 4.7393\n","====== Epoch: 211\n","====> Validation loss: 4.7075,  X1 loss: 4.6883   X2 loss: 4.7266\n","====== Epoch: 212\n","====> Validation loss: 4.6960,  X1 loss: 4.6777   X2 loss: 4.7142\n","====== Epoch: 213\n","====> Validation loss: 4.6989,  X1 loss: 4.6814   X2 loss: 4.7164\n","====== Epoch: 214\n","====> Validation loss: 4.7211,  X1 loss: 4.7037   X2 loss: 4.7386\n","====== Epoch: 215\n","====> Validation loss: 4.6704,  X1 loss: 4.6491   X2 loss: 4.6917\n","====== Epoch: 216\n","====> Validation loss: 4.7169,  X1 loss: 4.6961   X2 loss: 4.7378\n","====== Epoch: 217\n","====> Validation loss: 4.6620,  X1 loss: 4.6440   X2 loss: 4.6799\n","====== Epoch: 218\n","====> Validation loss: 4.7325,  X1 loss: 4.7146   X2 loss: 4.7503\n","====== Epoch: 219\n","====> Validation loss: 4.7000,  X1 loss: 4.6809   X2 loss: 4.7192\n","====== Epoch: 220\n","====> Validation loss: 4.6702,  X1 loss: 4.6471   X2 loss: 4.6933\n","====== Epoch: 221\n","====> Validation loss: 4.6827,  X1 loss: 4.6632   X2 loss: 4.7023\n","====== Epoch: 222\n","====> Validation loss: 4.6827,  X1 loss: 4.6654   X2 loss: 4.7001\n","====== Epoch: 223\n","====> Validation loss: 4.6552,  X1 loss: 4.6342   X2 loss: 4.6761\n","====== Epoch: 224\n","====> Validation loss: 4.6831,  X1 loss: 4.6629   X2 loss: 4.7032\n","====== Epoch: 225\n","====> Validation loss: 4.6773,  X1 loss: 4.6537   X2 loss: 4.7009\n","====== Epoch: 226\n","====> Validation loss: 4.6292,  X1 loss: 4.6118   X2 loss: 4.6465\n","====== Epoch: 227\n","====> Validation loss: 4.6788,  X1 loss: 4.6599   X2 loss: 4.6977\n","====== Epoch: 228\n","====> Validation loss: 4.6662,  X1 loss: 4.6444   X2 loss: 4.6880\n","====== Epoch: 229\n","====> Validation loss: 4.7557,  X1 loss: 4.7364   X2 loss: 4.7750\n","====== Epoch: 230\n","====> Validation loss: 4.6400,  X1 loss: 4.6212   X2 loss: 4.6589\n","====== Epoch: 231\n","====> Validation loss: 4.6950,  X1 loss: 4.6770   X2 loss: 4.7130\n","====== Epoch: 232\n","====> Validation loss: 4.6718,  X1 loss: 4.6486   X2 loss: 4.6950\n","====== Epoch: 233\n","====> Validation loss: 4.7467,  X1 loss: 4.7299   X2 loss: 4.7635\n","====== Epoch: 234\n","====> Validation loss: 4.6878,  X1 loss: 4.6708   X2 loss: 4.7049\n","====== Epoch: 235\n","====> Validation loss: 4.7569,  X1 loss: 4.7356   X2 loss: 4.7782\n","====== Epoch: 236\n","====> Validation loss: 4.7323,  X1 loss: 4.7155   X2 loss: 4.7491\n","====== Epoch: 237\n","====> Validation loss: 4.6572,  X1 loss: 4.6385   X2 loss: 4.6759\n","====== Epoch: 238\n","====> Validation loss: 4.7423,  X1 loss: 4.7195   X2 loss: 4.7652\n","====== Epoch: 239\n","====> Validation loss: 4.6886,  X1 loss: 4.6673   X2 loss: 4.7100\n","====== Epoch: 240\n","====> Validation loss: 4.6813,  X1 loss: 4.6617   X2 loss: 4.7010\n","====== Epoch: 241\n","====> Validation loss: 4.7343,  X1 loss: 4.7129   X2 loss: 4.7557\n","====== Epoch: 242\n","====> Validation loss: 4.7479,  X1 loss: 4.7297   X2 loss: 4.7661\n","====== Epoch: 243\n","====> Validation loss: 4.6935,  X1 loss: 4.6713   X2 loss: 4.7158\n","====== Epoch: 244\n","====> Validation loss: 4.6910,  X1 loss: 4.6697   X2 loss: 4.7123\n","====== Epoch: 245\n","====> Validation loss: 4.6533,  X1 loss: 4.6365   X2 loss: 4.6701\n","====== Epoch: 246\n","====> Validation loss: 4.7847,  X1 loss: 4.7650   X2 loss: 4.8044\n","====== Epoch: 247\n","====> Validation loss: 4.7971,  X1 loss: 4.7741   X2 loss: 4.8201\n","====== Epoch: 248\n","====> Validation loss: 4.7158,  X1 loss: 4.6949   X2 loss: 4.7367\n","====== Epoch: 249\n","====> Validation loss: 4.7277,  X1 loss: 4.7089   X2 loss: 4.7465\n","====== Epoch: 250\n","====> Validation loss: 4.6959,  X1 loss: 4.6777   X2 loss: 4.7140\n","====== Epoch: 251\n","====> Validation loss: 4.6924,  X1 loss: 4.6738   X2 loss: 4.7110\n","====== Epoch: 252\n","====> Validation loss: 4.7172,  X1 loss: 4.7025   X2 loss: 4.7319\n","====== Epoch: 253\n","====> Validation loss: 4.6949,  X1 loss: 4.6732   X2 loss: 4.7165\n","====== Epoch: 254\n","====> Validation loss: 4.6610,  X1 loss: 4.6409   X2 loss: 4.6810\n","====== Epoch: 255\n","====> Validation loss: 4.7279,  X1 loss: 4.7094   X2 loss: 4.7463\n","====== Epoch: 256\n","====> Validation loss: 4.6814,  X1 loss: 4.6610   X2 loss: 4.7018\n","====== Epoch: 257\n","====> Validation loss: 4.6484,  X1 loss: 4.6327   X2 loss: 4.6642\n","====== Epoch: 258\n","====> Validation loss: 4.6282,  X1 loss: 4.6081   X2 loss: 4.6483\n","====== Epoch: 259\n","====> Validation loss: 4.6533,  X1 loss: 4.6325   X2 loss: 4.6740\n","====== Epoch: 260\n","====> Validation loss: 4.7035,  X1 loss: 4.6881   X2 loss: 4.7189\n","====== Epoch: 261\n","====> Validation loss: 4.6367,  X1 loss: 4.6177   X2 loss: 4.6557\n","====== Epoch: 262\n","====> Validation loss: 4.7347,  X1 loss: 4.7168   X2 loss: 4.7526\n","====== Epoch: 263\n","====> Validation loss: 4.6929,  X1 loss: 4.6699   X2 loss: 4.7159\n","====== Epoch: 264\n","====> Validation loss: 4.7199,  X1 loss: 4.6968   X2 loss: 4.7431\n","====== Epoch: 265\n","====> Validation loss: 4.6796,  X1 loss: 4.6619   X2 loss: 4.6973\n","====== Epoch: 266\n","====> Validation loss: 4.6936,  X1 loss: 4.6752   X2 loss: 4.7119\n","====== Epoch: 267\n","====> Validation loss: 4.7844,  X1 loss: 4.7655   X2 loss: 4.8034\n","====== Epoch: 268\n","====> Validation loss: 4.7174,  X1 loss: 4.6981   X2 loss: 4.7366\n","====== Epoch: 269\n","====> Validation loss: 4.6750,  X1 loss: 4.6516   X2 loss: 4.6984\n","====== Epoch: 270\n","====> Validation loss: 4.7152,  X1 loss: 4.6953   X2 loss: 4.7352\n","====== Epoch: 271\n","====> Validation loss: 4.6077,  X1 loss: 4.5884   X2 loss: 4.6269\n","====== Epoch: 272\n","====> Validation loss: 4.8135,  X1 loss: 4.7915   X2 loss: 4.8355\n","====== Epoch: 273\n","====> Validation loss: 4.7467,  X1 loss: 4.7264   X2 loss: 4.7669\n","====== Epoch: 274\n","====> Validation loss: 4.7428,  X1 loss: 4.7207   X2 loss: 4.7648\n","====== Epoch: 275\n","====> Validation loss: 4.6813,  X1 loss: 4.6600   X2 loss: 4.7026\n","====== Epoch: 276\n","====> Validation loss: 4.7019,  X1 loss: 4.6790   X2 loss: 4.7247\n","====== Epoch: 277\n","====> Validation loss: 4.7077,  X1 loss: 4.6887   X2 loss: 4.7267\n","====== Epoch: 278\n","====> Validation loss: 4.7433,  X1 loss: 4.7235   X2 loss: 4.7630\n","====== Epoch: 279\n","====> Validation loss: 4.7135,  X1 loss: 4.6911   X2 loss: 4.7359\n","====== Epoch: 280\n","====> Validation loss: 4.7342,  X1 loss: 4.7151   X2 loss: 4.7533\n","====== Epoch: 281\n","====> Validation loss: 4.6991,  X1 loss: 4.6781   X2 loss: 4.7201\n","====== Epoch: 282\n","====> Validation loss: 4.6701,  X1 loss: 4.6489   X2 loss: 4.6912\n","====== Epoch: 283\n","====> Validation loss: 4.6309,  X1 loss: 4.6132   X2 loss: 4.6487\n","====== Epoch: 284\n","====> Validation loss: 4.7404,  X1 loss: 4.7237   X2 loss: 4.7572\n","====== Epoch: 285\n","====> Validation loss: 4.6334,  X1 loss: 4.6100   X2 loss: 4.6568\n","====== Epoch: 286\n","====> Validation loss: 4.7072,  X1 loss: 4.6890   X2 loss: 4.7254\n","====== Epoch: 287\n","====> Validation loss: 4.6299,  X1 loss: 4.6120   X2 loss: 4.6478\n","====== Epoch: 288\n","====> Validation loss: 4.6830,  X1 loss: 4.6650   X2 loss: 4.7009\n","====== Epoch: 289\n","====> Validation loss: 4.6363,  X1 loss: 4.6178   X2 loss: 4.6547\n","====== Epoch: 290\n","====> Validation loss: 4.6553,  X1 loss: 4.6365   X2 loss: 4.6741\n","====== Epoch: 291\n","====> Validation loss: 4.7222,  X1 loss: 4.6989   X2 loss: 4.7455\n","====== Epoch: 292\n","====> Validation loss: 4.7517,  X1 loss: 4.7351   X2 loss: 4.7684\n","====== Epoch: 293\n","====> Validation loss: 4.7431,  X1 loss: 4.7208   X2 loss: 4.7655\n","====== Epoch: 294\n","====> Validation loss: 4.7192,  X1 loss: 4.6994   X2 loss: 4.7391\n","====== Epoch: 295\n","====> Validation loss: 4.7474,  X1 loss: 4.7303   X2 loss: 4.7645\n","====== Epoch: 296\n","====> Validation loss: 4.6984,  X1 loss: 4.6759   X2 loss: 4.7210\n","====== Epoch: 297\n","====> Validation loss: 4.7177,  X1 loss: 4.6982   X2 loss: 4.7372\n","====== Epoch: 298\n","====> Validation loss: 4.7594,  X1 loss: 4.7413   X2 loss: 4.7776\n","====== Epoch: 299\n","====> Validation loss: 4.5835,  X1 loss: 4.5662   X2 loss: 4.6008\n","====== Epoch: 300\n","====> Validation loss: 4.7004,  X1 loss: 4.6781   X2 loss: 4.7227\n","====== Epoch: 301\n","====> Validation loss: 4.6240,  X1 loss: 4.6065   X2 loss: 4.6415\n","====== Epoch: 302\n","====> Validation loss: 4.8110,  X1 loss: 4.7935   X2 loss: 4.8285\n","====== Epoch: 303\n","====> Validation loss: 4.7310,  X1 loss: 4.7124   X2 loss: 4.7495\n","====== Epoch: 304\n","====> Validation loss: 4.6908,  X1 loss: 4.6705   X2 loss: 4.7110\n","====== Epoch: 305\n","====> Validation loss: 4.7057,  X1 loss: 4.6858   X2 loss: 4.7255\n","====== Epoch: 306\n","====> Validation loss: 4.6969,  X1 loss: 4.6783   X2 loss: 4.7154\n","====== Epoch: 307\n","====> Validation loss: 4.7042,  X1 loss: 4.6811   X2 loss: 4.7274\n","====== Epoch: 308\n","====> Validation loss: 4.6866,  X1 loss: 4.6680   X2 loss: 4.7052\n","====== Epoch: 309\n","====> Validation loss: 4.6853,  X1 loss: 4.6672   X2 loss: 4.7034\n","====== Epoch: 310\n","====> Validation loss: 4.6998,  X1 loss: 4.6743   X2 loss: 4.7254\n","====== Epoch: 311\n","====> Validation loss: 4.6658,  X1 loss: 4.6513   X2 loss: 4.6803\n","====== Epoch: 312\n","====> Validation loss: 4.6601,  X1 loss: 4.6398   X2 loss: 4.6805\n","====== Epoch: 313\n","====> Validation loss: 4.7227,  X1 loss: 4.7020   X2 loss: 4.7434\n","====== Epoch: 314\n","====> Validation loss: 4.6777,  X1 loss: 4.6551   X2 loss: 4.7003\n","====== Epoch: 315\n","====> Validation loss: 4.7378,  X1 loss: 4.7169   X2 loss: 4.7586\n","====== Epoch: 316\n","====> Validation loss: 4.6521,  X1 loss: 4.6287   X2 loss: 4.6756\n","====== Epoch: 317\n","====> Validation loss: 4.6854,  X1 loss: 4.6661   X2 loss: 4.7048\n","====== Epoch: 318\n","====> Validation loss: 4.6489,  X1 loss: 4.6284   X2 loss: 4.6695\n","====== Epoch: 319\n","====> Validation loss: 4.6963,  X1 loss: 4.6752   X2 loss: 4.7174\n","====== Epoch: 320\n","====> Validation loss: 4.6443,  X1 loss: 4.6241   X2 loss: 4.6646\n","====== Epoch: 321\n","====> Validation loss: 4.6859,  X1 loss: 4.6650   X2 loss: 4.7068\n","====== Epoch: 322\n","====> Validation loss: 4.6162,  X1 loss: 4.5976   X2 loss: 4.6347\n","====== Epoch: 323\n","====> Validation loss: 4.7206,  X1 loss: 4.7000   X2 loss: 4.7411\n","====== Epoch: 324\n","====> Validation loss: 4.7241,  X1 loss: 4.7052   X2 loss: 4.7430\n","====== Epoch: 325\n","====> Validation loss: 4.6394,  X1 loss: 4.6218   X2 loss: 4.6571\n","====== Epoch: 326\n","====> Validation loss: 4.7400,  X1 loss: 4.7200   X2 loss: 4.7599\n","====== Epoch: 327\n","====> Validation loss: 4.7668,  X1 loss: 4.7484   X2 loss: 4.7852\n","====== Epoch: 328\n","====> Validation loss: 4.7041,  X1 loss: 4.6844   X2 loss: 4.7239\n","====== Epoch: 329\n","====> Validation loss: 4.6398,  X1 loss: 4.6156   X2 loss: 4.6639\n","====== Epoch: 330\n","====> Validation loss: 4.7087,  X1 loss: 4.6882   X2 loss: 4.7293\n","====== Epoch: 331\n","====> Validation loss: 4.6011,  X1 loss: 4.5826   X2 loss: 4.6196\n","====== Epoch: 332\n","====> Validation loss: 4.6970,  X1 loss: 4.6769   X2 loss: 4.7171\n","====== Epoch: 333\n","====> Validation loss: 4.7352,  X1 loss: 4.7152   X2 loss: 4.7552\n","====== Epoch: 334\n","====> Validation loss: 4.6391,  X1 loss: 4.6230   X2 loss: 4.6552\n","====== Epoch: 335\n","====> Validation loss: 4.6901,  X1 loss: 4.6704   X2 loss: 4.7099\n","====== Epoch: 336\n","====> Validation loss: 4.6636,  X1 loss: 4.6416   X2 loss: 4.6855\n","====== Epoch: 337\n","====> Validation loss: 4.6880,  X1 loss: 4.6682   X2 loss: 4.7078\n","====== Epoch: 338\n","====> Validation loss: 4.6259,  X1 loss: 4.6020   X2 loss: 4.6498\n","====== Epoch: 339\n","====> Validation loss: 4.7151,  X1 loss: 4.6940   X2 loss: 4.7361\n","====== Epoch: 340\n","====> Validation loss: 4.6537,  X1 loss: 4.6272   X2 loss: 4.6801\n","====== Epoch: 341\n","====> Validation loss: 4.7759,  X1 loss: 4.7558   X2 loss: 4.7959\n","====== Epoch: 342\n","====> Validation loss: 4.6063,  X1 loss: 4.5834   X2 loss: 4.6291\n","====== Epoch: 343\n","====> Validation loss: 4.6880,  X1 loss: 4.6658   X2 loss: 4.7102\n","====== Epoch: 344\n","====> Validation loss: 4.5875,  X1 loss: 4.5679   X2 loss: 4.6071\n","====== Epoch: 345\n","====> Validation loss: 4.7327,  X1 loss: 4.7133   X2 loss: 4.7522\n","====== Epoch: 346\n","====> Validation loss: 4.6135,  X1 loss: 4.5947   X2 loss: 4.6323\n","====== Epoch: 347\n","====> Validation loss: 4.7285,  X1 loss: 4.7043   X2 loss: 4.7527\n","====== Epoch: 348\n","====> Validation loss: 4.6426,  X1 loss: 4.6211   X2 loss: 4.6641\n","====== Epoch: 349\n","====> Validation loss: 4.6748,  X1 loss: 4.6527   X2 loss: 4.6969\n","====== Epoch: 350\n","====> Validation loss: 4.6858,  X1 loss: 4.6650   X2 loss: 4.7065\n","====== Epoch: 351\n","====> Validation loss: 4.6663,  X1 loss: 4.6509   X2 loss: 4.6818\n","====== Epoch: 352\n","====> Validation loss: 4.6358,  X1 loss: 4.6088   X2 loss: 4.6628\n","====== Epoch: 353\n","====> Validation loss: 4.6204,  X1 loss: 4.5964   X2 loss: 4.6445\n","====== Epoch: 354\n","====> Validation loss: 4.6295,  X1 loss: 4.6073   X2 loss: 4.6518\n","====== Epoch: 355\n","====> Validation loss: 4.7207,  X1 loss: 4.7006   X2 loss: 4.7408\n","====== Epoch: 356\n","====> Validation loss: 4.6558,  X1 loss: 4.6336   X2 loss: 4.6780\n","====== Epoch: 357\n","====> Validation loss: 4.6478,  X1 loss: 4.6259   X2 loss: 4.6698\n","====== Epoch: 358\n","====> Validation loss: 4.7138,  X1 loss: 4.6919   X2 loss: 4.7356\n","====== Epoch: 359\n","====> Validation loss: 4.6521,  X1 loss: 4.6332   X2 loss: 4.6711\n","====== Epoch: 360\n","====> Validation loss: 4.6674,  X1 loss: 4.6502   X2 loss: 4.6846\n","====== Epoch: 361\n","====> Validation loss: 4.7220,  X1 loss: 4.6964   X2 loss: 4.7476\n","====== Epoch: 362\n","====> Validation loss: 4.7483,  X1 loss: 4.7288   X2 loss: 4.7678\n","====== Epoch: 363\n","====> Validation loss: 4.6273,  X1 loss: 4.6026   X2 loss: 4.6521\n","====== Epoch: 364\n","====> Validation loss: 4.6825,  X1 loss: 4.6632   X2 loss: 4.7018\n","====== Epoch: 365\n","====> Validation loss: 4.6174,  X1 loss: 4.5980   X2 loss: 4.6368\n","====== Epoch: 366\n","====> Validation loss: 4.6206,  X1 loss: 4.6000   X2 loss: 4.6412\n","====== Epoch: 367\n","====> Validation loss: 4.6801,  X1 loss: 4.6591   X2 loss: 4.7011\n","====== Epoch: 368\n","====> Validation loss: 4.6757,  X1 loss: 4.6529   X2 loss: 4.6986\n","====== Epoch: 369\n","====> Validation loss: 4.7449,  X1 loss: 4.7206   X2 loss: 4.7693\n","====== Epoch: 370\n","====> Validation loss: 4.6477,  X1 loss: 4.6307   X2 loss: 4.6647\n","====== Epoch: 371\n","====> Validation loss: 4.6363,  X1 loss: 4.6149   X2 loss: 4.6576\n","====== Epoch: 372\n","====> Validation loss: 4.7176,  X1 loss: 4.7013   X2 loss: 4.7339\n","====== Epoch: 373\n","====> Validation loss: 4.6475,  X1 loss: 4.6254   X2 loss: 4.6697\n","====== Epoch: 374\n","====> Validation loss: 4.6722,  X1 loss: 4.6531   X2 loss: 4.6914\n","====== Epoch: 375\n","====> Validation loss: 4.7027,  X1 loss: 4.6828   X2 loss: 4.7227\n","====== Epoch: 376\n","====> Validation loss: 4.6564,  X1 loss: 4.6334   X2 loss: 4.6793\n","====== Epoch: 377\n","====> Validation loss: 4.6714,  X1 loss: 4.6474   X2 loss: 4.6955\n","====== Epoch: 378\n","====> Validation loss: 4.7363,  X1 loss: 4.7171   X2 loss: 4.7555\n","====== Epoch: 379\n","====> Validation loss: 4.6857,  X1 loss: 4.6691   X2 loss: 4.7022\n","====== Epoch: 380\n","====> Validation loss: 4.6854,  X1 loss: 4.6660   X2 loss: 4.7047\n","====== Epoch: 381\n","====> Validation loss: 4.6316,  X1 loss: 4.6149   X2 loss: 4.6484\n","====== Epoch: 382\n","====> Validation loss: 4.6231,  X1 loss: 4.5996   X2 loss: 4.6467\n","====== Epoch: 383\n","====> Validation loss: 4.6910,  X1 loss: 4.6705   X2 loss: 4.7114\n","====== Epoch: 384\n","====> Validation loss: 4.6315,  X1 loss: 4.6126   X2 loss: 4.6505\n","====== Epoch: 385\n","====> Validation loss: 4.7003,  X1 loss: 4.6828   X2 loss: 4.7178\n","====== Epoch: 386\n","====> Validation loss: 4.6926,  X1 loss: 4.6713   X2 loss: 4.7139\n","====== Epoch: 387\n","====> Validation loss: 4.7046,  X1 loss: 4.6825   X2 loss: 4.7267\n","====== Epoch: 388\n","====> Validation loss: 4.6624,  X1 loss: 4.6397   X2 loss: 4.6850\n","====== Epoch: 389\n","====> Validation loss: 4.6795,  X1 loss: 4.6596   X2 loss: 4.6995\n","====== Epoch: 390\n","====> Validation loss: 4.6647,  X1 loss: 4.6445   X2 loss: 4.6849\n","====== Epoch: 391\n","====> Validation loss: 4.6590,  X1 loss: 4.6420   X2 loss: 4.6760\n","====== Epoch: 392\n","====> Validation loss: 4.6350,  X1 loss: 4.6095   X2 loss: 4.6606\n","====== Epoch: 393\n","====> Validation loss: 4.6509,  X1 loss: 4.6290   X2 loss: 4.6729\n","====== Epoch: 394\n","====> Validation loss: 4.6942,  X1 loss: 4.6719   X2 loss: 4.7166\n","====== Epoch: 395\n","====> Validation loss: 4.6901,  X1 loss: 4.6697   X2 loss: 4.7105\n","====== Epoch: 396\n","====> Validation loss: 4.6998,  X1 loss: 4.6741   X2 loss: 4.7255\n","====== Epoch: 397\n","====> Validation loss: 4.6397,  X1 loss: 4.6208   X2 loss: 4.6585\n","====== Epoch: 398\n","====> Validation loss: 4.7158,  X1 loss: 4.6958   X2 loss: 4.7358\n","====== Epoch: 399\n","====> Validation loss: 4.6964,  X1 loss: 4.6710   X2 loss: 4.7218\n"]}],"source":["\n","\n","\n","\n","\n","\n","# Read the command-line argument passed to the interpreter when invoking the script\n","lr = 0.001#sys.argv[1]\n","batch_size = 256# sys.argv[2]\n","\n","\n","# Read data\n","subj_ids = list(range(1, 20))\n","fs = 128\n","window_size = int(5 * fs)\n","stride_size_train, stride_size_val, stride_size_test = int(2.5 * fs), int(5 * fs), int(5 * fs)\n","batch_size = int(batch_size)\n","lr = lr\n","\n","n_channs = 129 # 128 for eeg, 1 for env\n","print('-------------------------------------')\n","print(f'window_size: {window_size}  stride_size_test: {stride_size_test}')\n","\n","dataset_name = ['rochester_data', 'natural_speech']\n","outputs_path = f'../outputs/'\n","data_path = os.path.join(outputs_path, dataset_name[0], dataset_name[1])\n","after_ica_path = os.path.join(data_path, 'after_ica_raw')\n","print(f'data_path: {data_path}')\n","\n","\n","X = load_data(subj_ids, after_ica_path, window_size, \n","              stride_size_train, stride_size_val, stride_size_test, n_channs)\n","\n","\n","# Create dataloaders\n","class MyDataset(Dataset):\n","    def __init__(self, eeg, env):\n","        self.eeg = eeg\n","        self.env = env\n","    \n","    def __getitem__(self, index):\n","        return self.eeg[index], self.env[index]\n","    \n","    def __len__(self):\n","        return len(self.eeg)\n","    \n","\n","dataset_train = MyDataset(X['eegs_train'], X['envs_train'])\n","dl_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)\n","dl_val = DataLoader(MyDataset(X[\"eegs_val\"], X[\"envs_val\"]), \n","                    batch_size=batch_size, shuffle=True, drop_last=True)\n","\n","\n","\n","# Create model\n","eeg_encoder = EEGEncoder()\n","env_encoder = EnvelopeEncoder()\n","model = CLEE(eeg_encoder, env_encoder)\n","model.to(device)\n","\n","# Train model\n","models_dict = {f'lr_{lr}_bs_{batch_size}': model}\n","lossi = []\n","udri = [] # update / data ratio \n","ud = []\n","\n","\n","\n","for name, model in models_dict.items():\n","\n","    # Reset for the new model in the loop\n","    print(f\"+--------------New model: {name}----------------------+\")\n","    writer = SummaryWriter(log_dir=f\"runs/{name}_{time.strftime('%Y%m%d_%H%M%S')}\")\n","    model.to(device)\n","    optimizer = optim.NAdam(model.parameters(), lr=lr)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.7)\n","    cnt = 0\n","    loss_batches = []\n","\n","\n","    for epoch in range(1, 400):\n","\n","        print(f\"====== Epoch: {epoch}\")\n","\n","        model.train()\n","        for ix_batch, (Xb_eeg, Xb_env) in enumerate(dl_train):\n","\n","            # send to device\n","            Xb_eeg = Xb_eeg.to(device)\n","            Xb_env = Xb_env.to(device)\n","\n","            # Zero out gradients\n","            optimizer.zero_grad()\n","\n","            # forward pass\n","            eeg_features, env_features, logit_scale = model(Xb_eeg, Xb_env) \n","\n","\n","            # normalize features\n","            eeg_features_n = eeg_features / eeg_features.norm(dim=1, keepdim=True)\n","            env_features_n = env_features / env_features.norm(dim=1, keepdim=True)\n","\n","            # logits\n","            logits_per_eeg = logit_scale * eeg_features_n @ env_features_n.t()\n","            logits_per_env = logits_per_eeg.t()\n","\n","            #loss function\n","            labels = torch.arange(batch_size).to(device)\n","            loss_eeg = F.cross_entropy(logits_per_eeg, labels)\n","            loss_env = F.cross_entropy(logits_per_env, labels)\n","            loss   = (loss_eeg + loss_env)/2\n","\n","            # backward pass\n","            loss.backward()\n","            optimizer.step()\n","\n","            loss_batches.append(loss.item())\n","            cnt += 1\n","\n","            with torch.no_grad():\n","                #ud = {f\"p{ix}\":(lr*p.grad.std() / p.data.std()).log10().item() for ix, p in enumerate(model.parameters()) if p.ndim==4 }\n","                #writer.add_scalars('UpdateOData/ud', ud, cnt)\n","                writer.add_scalar('Loss/train_batch', loss.item(), cnt)\n","\n","            # normalize weights\n","            with torch.no_grad():\n","                model.eeg_encoder.normalize_weights()\n","            \n","            #break   \n","\n","        loss_epoch = loss_batches[-(ix_batch + 1):]  # mean loss across batches\n","        loss_epoch = sum(loss_epoch) / len(loss_epoch)\n","        writer.add_scalar('Loss/train_epoch', loss_epoch, epoch)\n","        #for pname, p in model.named_parameters():\n","        #writer.add_histogram(f'Params/{pname}', p, epoch)\n","        #writer.add_histogram(f'Grads/{pname}', p.grad, epoch)\n","\n","        loss_val, *_ = eval_model_cl(dl_val, model, device=device)\n","        writer.add_scalar('Loss/val_epoch', loss_val, epoch)\n","\n","        \n","\n","        model.train()\n","\n","        # Update learning rate based on epoch\n","        scheduler.step()\n","            \n","    #break   \n","\n","\n","\n","\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"colab":{"provenance":[],"machine_shape":"hm"},"accelerator":"GPU","gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}