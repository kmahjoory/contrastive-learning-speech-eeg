{"cells":[{"cell_type":"markdown","metadata":{"id":"VhCK252zNjUG"},"source":["## COLAB TOOLS"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1977,"status":"ok","timestamp":1677715273457,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"v-pxnK4OaI-b","outputId":"c96b22ff-d18e-4903-e554-68f794211e61"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1677715273458,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"ZEopbadxbJH0","outputId":"9c3327a6-267f-46dd-f638-14494e673086"},"outputs":[{"output_type":"stream","name":"stdout","text":["['train_cl_eeg2speech_rochester_v3_test_gridsearch.ipynb', '.git', '.DS_Store', '.gitignore', 'EEG', 'LICENSE', 'train_cl_eeg2speech_rochester_v1.ipynb', 'train_cl_eeg2speech_rochester_v2.ipynb', 'train_cl_eeg2speech_rochester_v3.ipynb', '.ipynb_checkpoints', 'train_cl_eeg2speech_rochester_v3_test_old.ipynb', 'runs', 'train_cl_eeg2speech_rochester_v3_test.ipynb', 'train_cl_eeg2speech_rochester_v4_gridseaerch.ipynb', 'train_cl_eeg2speech_2.ipynb', 'train_cl_eeg2speech_rochester_subj_2.ipynb', 'README.md', 'train_eeg2speech_rochester.ipynb']\n"]}],"source":["\n","import os\n","import sys\n","\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"Colab Notebooks/prj_neuroread_analysis/neuroread/\"\n","GOOGLE_DRIVE_PATH = os.path.join(\"/content\", \"drive\", \"MyDrive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))\n","\n","# Add to sys so we can import .py files.\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","os.chdir(GOOGLE_DRIVE_PATH)\n","\n","# Install unavailable packages\n","import pip\n","def import_or_install(package):\n","    try:\n","        __import__(package)\n","    except ImportError:\n","        pip.main(['install', package])\n","\n","import_or_install(\"mne\")\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677715273458,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"M0LuZowEbY29","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4ea7e367-3775-40db-bb28-dc6cb6c00b99"},"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":427,"status":"ok","timestamp":1677715273875,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"HTqIrJcOaQLu","outputId":"beabc709-8cc9-4c3f-e292-33bedc9c6c40"},"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 89.6 gigabytes of available RAM\n","\n","Thu Mar  2 00:01:13 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P0    46W / 400W |      3MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n"," print('Not connected to a GPU')\n","else:\n"," print(gpu_info)"]},{"cell_type":"markdown","metadata":{"id":"aXEXbz7ENjUM"},"source":["## Main code"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1677715273876,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"gGggzCoKZQeu","outputId":"96fd8392-0fc5-45b2-8506-a7ed7cc9b42b"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["import os, sys, glob\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","import numpy as np\n","\n","import mne\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import time\n","\n","from torchsummary import summary\n","from torch.utils.tensorboard import SummaryWriter\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1677715273877,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"GYgZe_3OQniG"},"outputs":[],"source":["def eval_model_cl(dl, model, device=torch.device('cpu'), verbose=True):\n","    \"\"\" \n","    This function calculates the loss on data, setting backward gradients and batchnorm\n","    off. This function is written for contrasting learning where the model takes in two\n","    inputs.\n","\n","    Args:\n","\n","    Returns:\n","      loss_test: Mean loss of all test samples (scalar)\n","\n","    \"\"\"\n","    losses, losses_X1, losses_X2 = [], [], []\n","    model.to(device)  # inplace for model\n","    # Set the model in evaluation mode\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for idx_batch, (X1b, X2b) in enumerate(dl):\n","\n","            X1b = X1b.to(device)\n","            X2b = X2b.to(device)\n","\n","            X1b_features, X2b_features, logit_sc = model(X1b, X2b)\n","\n","            # Normalize features\n","            X1b_f_n = X1b_features / X1b_features.norm(dim=1, keepdim=True)\n","            X2b_f_n = X2b_features / X2b_features.norm(dim=1, keepdim=True)\n","\n","            logits_per_X1 = logit_sc * X1b_f_n @ X2b_f_n.t()\n","            logits_per_X2 = logits_per_X1.t()\n","\n","            # Number of labels equals to the 1st dimension of X1b\n","            labels = torch.arange(X1b.shape[0], device=device)\n","\n","            # Batch Loss \n","            loss_X1 = F.cross_entropy(logits_per_X1, labels)\n","            loss_X2 = F.cross_entropy(logits_per_X2, labels)\n","            loss_batch   = (loss_X1 + loss_X2) / 2\n","            losses.append(loss_batch.item())\n","            losses_X1.append(loss_X1.item())\n","            losses_X2.append(loss_X2.item())\n","\n","        # Epoch loss (mean of batch losses)\n","        loss  = sum(losses) / len(losses)\n","        loss_X1 = sum(losses_X1) / len(losses_X1)\n","        loss_X2 = sum(losses_X2) / len(losses_X2)\n","\n","        if verbose:\n","          print(f\"====> Validation loss: {loss:.4f},  X1 loss: {loss_X1:.4f}   X2 loss: {loss_X2:.4f}\")\n","\n","        return loss, loss_X1, loss_X2\n"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1677715274190,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"EaAsZ-SLZQey"},"outputs":[],"source":["def unfold_raw(raw, window_size=None, stride=None):\n","    \"\"\"\n","    This function unfolds raw MNE object into a list of raw objects\n","    Args:\n","        raw: a raw MNE object cropped by rejecting bad segments.\n","    Returns:\n","        raw_unfolded: a raw MNE object unfolded by applying a sliding window.\n","    \"\"\"\n","    if window_size is None:\n","        window_size = int(5 * raw.info['sfreq'])\n","    if stride is None:\n","        stride = window_size\n","    nchans = len(raw.ch_names)\n","    sig = torch.tensor(raw.get_data(), dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n","    sig_unf = F.unfold(sig, (nchans, window_size), stride=stride , padding=0)\n","    sig_unf = sig_unf.permute(0, 2, 1).reshape(-1, sig_unf.shape[-1], nchans, window_size)\n","    return sig_unf"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1677715274192,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"1cf9EoF3ZQey"},"outputs":[],"source":["def rm_repeated_annotations(raw):\n","    \"\"\"This functions taskes in raw MNE obejct and removes repeated annotations\"\"\"\n","    annots = raw.annotations.copy()\n","    annots_drop = []\n","    for k in annots:\n","        annots_drop.extend([k for kk in annots if (k['onset'] > kk['onset']) and (k['onset']+k['duration'] < kk['onset']+kk['duration']) ])\n","\n","    annots_updated = [i for i in annots if i not in annots_drop]\n","    onsets = [i['onset'] for i in annots_updated]\n","    durations = [i['duration'] for i in annots_updated]\n","    descriptions = [i['description'] for i in annots_updated]\n","    print('Initial num of annots: %d  Num of removed annots: %d  Num of retained annots:  %d' % (len(annots), len(annots_drop), len(annots_updated)))\n","    print(f' New annots: {annots_updated}')\n","    raw.set_annotations(mne.Annotations(onsets, durations, descriptions) ) \n","    return raw"]},{"cell_type":"markdown","metadata":{"id":"q4sGJAdFZQez"},"source":["## Read Data"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677715274192,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"KdQFfHcJZQe0","outputId":"60aea4aa-d181-453a-a677-0d1a53437d88"},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------\n","window_size: 640  stride_size_test: 640\n","data_path: ../outputs/rochester_data/natural_speech\n"]}],"source":["subj_ids = list(range(1, 11))\n","fs = 128\n","window_size = int(5 * fs)\n","stride_size_train, stride_size_val, stride_size_test = int(2.5 * fs), int(5 * fs), int(5 * fs)\n","n_channs = 129 # 128 for eeg, 1 for env\n","batch_size = int(32)\n","print('-------------------------------------')\n","print(f'window_size: {window_size}  stride_size_test: {stride_size_test}')\n","\n","dataset_name = ['rochester_data', 'natural_speech']\n","outputs_path = f'../outputs/'\n","data_path = os.path.join(outputs_path, dataset_name[0], dataset_name[1])\n","after_ica_path = os.path.join(data_path, 'after_ica_raw')\n","print(f'data_path: {data_path}')"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":105642,"status":"ok","timestamp":1677715379824,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"fIhJi5IxZQe1","outputId":"e3fab5bf-556f-4ee2-fe9e-3b070e413a09"},"outputs":[{"output_type":"stream","name":"stdout","text":["Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_1_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 48  Num of removed annots: 19  Num of retained annots:  29\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.559097), ('duration', 2.240447998046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.084473), ('duration', 2.24041748046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.212158), ('duration', 2.0118408203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.890503), ('duration', 2.67486572265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 899.02771), ('duration', 2.05755615234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 983.746643), ('duration', 6.58416748046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1051.039551), ('duration', 1.8746337890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1100.719482), ('duration', 6.4012451171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1265.987061), ('duration', 3.269287109375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1437.989502), ('duration', 2.080322265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1611.904297), ('duration', 11.796630859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.480591), ('duration', 3.81787109375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1977.884644), ('duration', 8.5731201171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2058.135986), ('duration', 5.052490234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2161.450928), ('duration', 3.2236328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.70874), ('duration', 1.8291015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.632324), ('duration', 5.578369140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2656.933838), ('duration', 3.246337890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2699.549561), ('duration', 15.7060546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2892.681641), ('duration', 3.497802734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2977.071045), ('duration', 1.64599609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3079.503906), ('duration', 8.321533203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3220.774658), ('duration', 3.017822265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3257.32251), ('duration', 10.927978515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3327.002441), ('duration', 6.584228515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3438.776123), ('duration', 5.806884765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3480.157471), ('duration', 12.139404296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3607.381592), ('duration', 10.49365234375), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 26  N val: 1  N test: 1\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_2_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 65  Num of removed annots: 19  Num of retained annots:  46\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 177.009033), ('duration', 4.4808807373046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 233.547455), ('duration', 3.1549072265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 256.64325), ('duration', 4.549468994140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.877777), ('duration', 1.554595947265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 419.798157), ('duration', 4.18365478515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 521.238403), ('duration', 3.72650146484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 536.986145), ('duration', 8.36737060546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 591.58136), ('duration', 12.36810302734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 684.541626), ('duration', 37.07598876953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 731.651611), ('duration', 0.9830322265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 760.453308), ('duration', 5.80682373046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 895.355164), ('duration', 14.21990966796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 955.299988), ('duration', 3.9779052734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1010.325623), ('duration', 5.5782470703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1047.856567), ('duration', 3.0177001953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.390625), ('duration', 4.892333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1266.681152), ('duration', 8.984619140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1304.783569), ('duration', 3.2464599609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1385.044556), ('duration', 3.749267578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.464355), ('duration', 2.171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1613.947266), ('duration', 13.3740234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1636.351562), ('duration', 1.600341796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1650.508545), ('duration', 8.5731201171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1683.526245), ('duration', 6.5384521484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1726.90979), ('duration', 6.2640380859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1807.079224), ('duration', 13.076904296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.602295), ('duration', 1.348876953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1991.061035), ('duration', 2.126220703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2049.411133), ('duration', 4.503662109375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2136.050537), ('duration', 4.732421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2162.68042), ('duration', 11.659423828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2234.577881), ('duration', 7.315673828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.926758), ('duration', 14.63134765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.343262), ('duration', 9.076171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2546.635498), ('duration', 3.177734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2707.869873), ('duration', 15.68310546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.512939), ('duration', 8.481689453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3081.322021), ('duration', 2.468994140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3087.928955), ('duration', 6.835693359375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3116.119141), ('duration', 3.886474609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3257.709961), ('duration', 6.652587890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3280.65918), ('duration', 5.71533203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3440.005615), ('duration', 0.914306640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3452.968018), ('duration', 7.29296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3604.318115), ('duration', 3.817138671875), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 68  N val: 2  N test: 2\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_3_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 47  Num of removed annots: 19  Num of retained annots:  28\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 43.135948), ('duration', 4.115089416503906), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 121.796593), ('duration', 5.6010894775390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.748077), ('duration', 1.760345458984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 275.317291), ('duration', 2.994842529296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.205322), ('duration', 2.126129150390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.65863), ('duration', 1.89752197265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.599854), ('duration', 2.12615966796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 898.988525), ('duration', 1.6231689453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1033.410278), ('duration', 0.9830322265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.293579), ('duration', 1.7603759765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1266.313721), ('duration', 3.56640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1307.502563), ('duration', 0.86865234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.074097), ('duration', 2.4461669921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1583.876953), ('duration', 0.3658447265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1613.867554), ('duration', 1.9432373046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.839478), ('duration', 1.9661865234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.358643), ('duration', 2.1947021484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2162.061523), ('duration', 2.14892578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.888916), ('duration', 0.8916015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.831299), ('duration', 2.92626953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2707.159668), ('duration', 2.01171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2790.049561), ('duration', 9.510498046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.086426), ('duration', 2.400390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3081.215576), ('duration', 1.874755859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.645996), ('duration', 2.14892578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3439.272217), ('duration', 2.42333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3525.888916), ('duration', 1.87451171875), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 93  N val: 3  N test: 3\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_4_after_ica_raw.fif...\n","    Range : 0 ... 464394 =      0.000 ...  3628.078 secs\n","Ready.\n","Reading 0 ... 464394  =      0.000 ...  3628.078 secs...\n","Initial num of annots: 41  Num of removed annots: 19  Num of retained annots:  22\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.316391), ('duration', 2.4461822509765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.619507), ('duration', 1.074493408203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.230835), ('duration', 1.851806640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.968323), ('duration', 4.89239501953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 898.236755), ('duration', 2.56048583984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1100.372314), ('duration', 2.5833740234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1265.114136), ('duration', 4.6181640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1437.281372), ('duration', 1.874755859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1613.327759), ('duration', 1.783203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.417358), ('duration', 1.2344970703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1981.789795), ('duration', 2.720458984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2161.278076), ('duration', 4.8466796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2344.500732), ('duration', 1.80615234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.130127), ('duration', 2.194580078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2706.033447), ('duration', 2.743408203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2876.088623), ('duration', 1.6689453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2892.718506), ('duration', 0.822998046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3079.622803), ('duration', 2.217529296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.026855), ('duration', 6.172607421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3438.603516), ('duration', 0.822998046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3535.585693), ('duration', 2.720458984375), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 112  N val: 4  N test: 4\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_5_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 41  Num of removed annots: 19  Num of retained annots:  22\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 177.408173), ('duration', 0.270477294921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 358.121857), ('duration', 0.270477294921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.280273), ('duration', 1.9835205078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.369812), ('duration', 2.3441162109375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 899.271362), ('duration', 1.3974609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.870972), ('duration', 0.4508056640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1267.272949), ('duration', 1.4425048828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.279663), ('duration', 1.126953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1614.049805), ('duration', 1.8707275390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1701.652954), ('duration', 0.96923828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.822144), ('duration', 2.434326171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.066284), ('duration', 2.186279296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2161.976074), ('duration', 1.983642578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.484619), ('duration', 1.870849609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2524.411133), ('duration', 0.541015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2707.077393), ('duration', 2.569580078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.742432), ('duration', 0.8564453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3081.496094), ('duration', 0.653564453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3229.895752), ('duration', 1.84814453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.698975), ('duration', 2.727294921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3439.993408), ('duration', 0.901611328125), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 131  N val: 5  N test: 5\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_6_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 58  Num of removed annots: 19  Num of retained annots:  39\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.775345), ('duration', 1.6688995361328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 282.102112), ('duration', 2.56048583984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.346893), ('duration', 2.263275146484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 523.917664), ('duration', 2.1260986328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 536.834473), ('duration', 2.56048583984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.924316), ('duration', 5.0753173828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 757.242065), ('duration', 2.65191650390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 837.127747), ('duration', 3.177734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 879.596802), ('duration', 2.5147705078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 897.584961), ('duration', 4.526611328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.219971), ('duration', 2.0574951171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1218.668945), ('duration', 2.4461669921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1267.177368), ('duration', 1.7603759765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1405.632324), ('duration', 2.3089599609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.297607), ('duration', 1.8746337890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1469.58728), ('duration', 2.03466796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1497.78125), ('duration', 2.4461669921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1554.397705), ('duration', 10.470703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1608.48999), ('duration', 8.39013671875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1634.155762), ('duration', 2.468994140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1747.676392), ('duration', 4.3209228515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1807.026855), ('duration', 1.6002197265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.125), ('duration', 2.7890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2036.889648), ('duration', 4.7322998046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2122.203125), ('duration', 2.03466796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2162.363037), ('duration', 1.623291015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2253.597656), ('duration', 2.491943359375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.462646), ('duration', 2.034912109375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2487.927979), ('duration', 2.42333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2524.04541), ('duration', 1.92041015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2603.16333), ('duration', 1.8974609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2706.935303), ('duration', 2.743408203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.359375), ('duration', 2.0576171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3080.958984), ('duration', 1.783203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3111.987793), ('duration', 0.64013671875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.530518), ('duration', 2.8349609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3403.469971), ('duration', 1.1201171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3439.724609), ('duration', 1.028564453125), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 167  N val: 6  N test: 6\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_7_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 74  Num of removed annots: 18  Num of retained annots:  56\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.368912), ('duration', 2.6062164306640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 202.71106), ('duration', 3.5206756591796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 315.806854), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 356.574677), ('duration', 2.903411865234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 534.393311), ('duration', 6.35552978515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.769409), ('duration', 1.44024658203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 899.308594), ('duration', 2.5147705078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 981.924255), ('duration', 1.8289794921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 995.180115), ('duration', 2.53765869140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1054.425659), ('duration', 13.945556640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1091.636597), ('duration', 3.54345703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1100.598267), ('duration', 3.2235107421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1187.123291), ('duration', 1.851806640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1267.292847), ('duration', 1.2574462890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1323.053955), ('duration', 1.5089111328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.083496), ('duration', 1.8289794921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1504.320801), ('duration', 2.354736328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1536.858521), ('duration', 5.1666259765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1614.278198), ('duration', 1.9432373046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1668.210327), ('duration', 2.766357421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1713.217041), ('duration', 1.80615234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1729.60498), ('duration', 3.1319580078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1784.040039), ('duration', 8.756103515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1805.663208), ('duration', 3.7950439453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.498901), ('duration', 2.0574951171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2096.175781), ('duration', 2.37744140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2157.560791), ('duration', 10.72216796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2288.804688), ('duration', 3.955078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2330.930908), ('duration', 2.468994140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.640381), ('duration', 1.600341796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2363.354248), ('duration', 2.080322265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2398.324707), ('duration', 2.354736328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.527588), ('duration', 2.53759765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2550.565186), ('duration', 6.149658203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2706.920654), ('duration', 2.857666015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.047607), ('duration', 2.26318359375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2912.270264), ('duration', 3.2919921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2929.330566), ('duration', 3.08642578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2943.047607), ('duration', 4.09228515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2969.869873), ('duration', 3.497802734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2992.805664), ('duration', 3.246337890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3006.655762), ('duration', 2.743408203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3080.669922), ('duration', 3.04052734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3104.24585), ('duration', 2.171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3139.366943), ('duration', 1.463134765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3160.199219), ('duration', 6.67578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3224.72583), ('duration', 9.05322265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.845459), ('duration', 2.6748046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3312.975098), ('duration', 3.132080078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3358.553223), ('duration', 2.8349609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3393.308594), ('duration', 2.080322265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3432.110352), ('duration', 2.583251953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3440.40625), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3503.788574), ('duration', 3.360595703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3555.731934), ('duration', 7.33837890625), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 220  N val: 7  N test: 7\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_8_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 42  Num of removed annots: 19  Num of retained annots:  23\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.14798), ('duration', 2.926300048828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.06424), ('duration', 2.2633056640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 371.421326), ('duration', 5.235321044921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 536.955017), ('duration', 4.4808349609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.692444), ('duration', 2.37762451171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 898.715332), ('duration', 2.65191650390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.184326), ('duration', 2.19482421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1266.598633), ('duration', 2.743408203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1437.696045), ('duration', 5.6468505859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1613.613281), ('duration', 11.3165283203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.690186), ('duration', 2.53759765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.835938), ('duration', 1.348876953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2162.807617), ('duration', 0.8916015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2344.188721), ('duration', 3.269287109375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2377.503662), ('duration', 2.92626953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.80957), ('duration', 1.874755859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2707.339844), ('duration', 1.87451171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2716.461426), ('duration', 2.240478515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2888.094238), ('duration', 7.0185546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3080.641357), ('duration', 2.2861328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.550049), ('duration', 2.652099609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3439.231689), ('duration', 2.19482421875), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 240  N val: 8  N test: 8\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_9_after_ica_raw.fif...\n","    Range : 0 ... 464396 =      0.000 ...  3628.094 secs\n","Ready.\n","Reading 0 ... 464396  =      0.000 ...  3628.094 secs...\n","Initial num of annots: 50  Num of removed annots: 19  Num of retained annots:  31\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 130.882126), ('duration', 1.5317230224609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 175.55928), ('duration', 3.543548583984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 264.037018), ('duration', 2.994873046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 356.953827), ('duration', 2.42333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.136719), ('duration', 2.60626220703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.279785), ('duration', 2.19476318359375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 805.422119), ('duration', 2.03466796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 863.707458), ('duration', 1.5089111328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 898.531311), ('duration', 2.1947021484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 942.221497), ('duration', 1.16595458984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1100.621216), ('duration', 2.103271484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1266.725952), ('duration', 1.2344970703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1361.294312), ('duration', 1.851806640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1436.863159), ('duration', 3.3377685546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1612.4375), ('duration', 5.2352294921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1627.293579), ('duration', 2.6519775390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1664.12915), ('duration', 145.87548828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.199341), ('duration', 14.970458984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2160.909668), ('duration', 2.720458984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2343.753906), ('duration', 3.955078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2376.438232), ('duration', 2.42333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2522.852783), ('duration', 2.08056640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2621.444824), ('duration', 2.2861328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2704.782715), ('duration', 5.006591796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2779.488281), ('duration', 3.4521484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2890.36084), ('duration', 4.572509765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3018.792969), ('duration', 2.2861328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3073.694824), ('duration', 8.207275390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3260.755127), ('duration', 1.966064453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3437.777588), ('duration', 2.468994140625), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 268  N val: 9  N test: 9\n","Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_10_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 69  Num of removed annots: 18  Num of retained annots:  51\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 149.729568), ('duration', 2.286163330078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.793808), ('duration', 1.5088653564453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 211.101257), ('duration', 1.3259735107421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 304.370483), ('duration', 2.6748046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.593964), ('duration', 3.429229736328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 423.59317), ('duration', 4.526611328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 492.564392), ('duration', 6.309814453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.557739), ('duration', 1.6231689453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 572.701599), ('duration', 2.994873046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 612.724426), ('duration', 2.5833740234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 719.066589), ('duration', 4.8466796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 811.009827), ('duration', 2.1260986328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 880.173401), ('duration', 2.42333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 887.100464), ('duration', 2.21759033203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 900.03125), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1025.614502), ('duration', 2.81201171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.146973), ('duration', 3.8636474609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1150.301147), ('duration', 2.14892578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1233.59314), ('duration', 1.7603759765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1267.456787), ('duration', 3.0406494140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1362.038574), ('duration', 3.6578369140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1437.113892), ('duration', 3.4520263671875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1486.748047), ('duration', 2.3319091796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1505.262085), ('duration', 3.4063720703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1614.439087), ('duration', 1.0516357421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1756.245117), ('duration', 4.2523193359375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1807.182495), ('duration', 1.828857421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1922.290039), ('duration', 2.1490478515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.267212), ('duration', 5.3724365234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2112.381592), ('duration', 7.91015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2162.427246), ('duration', 3.93212890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2216.300293), ('duration', 7.27001953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2331.169922), ('duration', 2.560546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2344.951416), ('duration', 2.69775390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2499.67041), ('duration', 1.486083984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.835205), ('duration', 3.2919921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2550.506836), ('duration', 3.54345703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2649.912354), ('duration', 2.994873046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2706.953857), ('duration', 4.09228515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.044434), ('duration', 3.0634765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2913.181641), ('duration', 3.360595703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2961.087891), ('duration', 3.223388671875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2998.279785), ('duration', 3.360595703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3080.013428), ('duration', 6.88134765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3253.579834), ('duration', 17.740478515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3347.544434), ('duration', 2.62890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3420.438232), ('duration', 2.53759765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3438.906494), ('duration', 3.109130859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3501.901367), ('duration', 2.03466796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3572.518799), ('duration', 1.828857421875), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 315  N val: 10  N test: 10\n","Shape Trian: torch.Size([11558, 1, 129, 640])  Shape Val: torch.Size([374, 1, 129, 640])  Shape Test: torch.Size([360, 1, 129, 640])\n","-------------------------------------\n","Shape EEG Train: torch.Size([11558, 1, 128, 640])  Val: torch.Size([374, 1, 128, 640])  Test: torch.Size([360, 1, 128, 640])\n","Mean: 4.717854579228131e-11  Std: 5.596546088781906e-06\n","Shape Env Train: torch.Size([11558, 1, 1, 640])  Val: torch.Size([374, 1, 1, 640])  Test: torch.Size([360, 1, 1, 640])\n","Mean Env: 2.3700499534606934  Std Env: 2.6003262996673584\n"]}],"source":["raws_train_windowed, raws_val_windowed, raws_test_windowed = [], [], []\n","\n","for subj_id in subj_ids:\n","    \n","\n","    # load subject raw MNE object\n","    raw = mne.io.read_raw(os.path.join(after_ica_path, f'subj_{subj_id}_after_ica_raw.fif'), preload=True)\n","    # drop M1 and M2 channels\n","    raw.drop_channels(['M1', 'M2'])\n","    assert raw.info['nchan'] == n_channs\n","\n","    raw = rm_repeated_annotations(raw)\n","    annots = raw.annotations.copy()\n","    raw_split = [raw.copy().crop(t1, t2) for t1, t2 in zip(annots.onset[:-1]+annots.duration[:-1], annots.onset[1:])]\n","\n","    # Pick the split with the longest duration for validation, supposedly less noisy\n","    ix_val = np.argmax([i.get_data().shape[1] for i in raw_split])\n","    raw_val = [raw_split.pop(ix_val)] # create a list to make it iterable. later may be used for multiple splits\n","\n","    # Pick the next split with the longest duration for testing, supposedly less noisy\n","    ix_test = np.argmax([i.get_data().shape[1] for i in raw_split])\n","    raw_test = [raw_split.pop(ix_test)]\n","    \n","    # creat list of unfolded tensor raw objects\n","    fs = raw.info['sfreq']\n","    raws_train_windowed.extend([unfold_raw(i, window_size=window_size, stride=stride_size_train) for i in raw_split if i.get_data().shape[1] > window_size])\n","    raws_val_windowed.extend([unfold_raw(i, window_size=window_size, stride=stride_size_val) for i in raw_val if i.get_data().shape[1] > window_size])\n","    raws_test_windowed.extend([unfold_raw(i, window_size=window_size, stride=stride_size_test) for i in raw_test if i.get_data().shape[1] > window_size])\n","    print(\"-------------------------------------\")\n","    print('N train: %d  N val: %d  N test: %d' % (len(raws_train_windowed), len(raws_val_windowed), len(raws_test_windowed)))\n","\n","# concatenate all in second dimension\n","sigs_train = torch.cat(raws_train_windowed, dim=1).permute(1, 0, 2, 3)\n","sigs_val = torch.cat(raws_val_windowed, dim=1).permute(1, 0, 2, 3)\n","sigs_test = torch.cat(raws_test_windowed, dim=1).permute(1, 0, 2, 3)\n","print(f\"Shape Trian: {sigs_train.shape}  Shape Val: {sigs_val.shape}  Shape Test: {sigs_test.shape}\")\n","\n","eegs_train = sigs_train[:, :, :-1, :]\n","eegs_val = sigs_val[:, :, :-1, :]\n","eegs_test = sigs_test[:, :, :-1, :]\n","print(\"-------------------------------------\")\n","print(f\"Shape EEG Train: {eegs_train.shape}  Val: {eegs_val.shape}  Test: {eegs_test.shape}\")\n","\n","# To avoid information leakage, we estimate the mean and std from the training set only.\n","mean_eeg_train =  eegs_train.mean()\n","std_eeg_train = eegs_train.std()\n","print(f\"Mean: {mean_eeg_train}  Std: {std_eeg_train}\")\n","\n","envs_train = sigs_train[:, :, [-1], :]\n","envs_val = sigs_val[:, :, [-1], :]\n","envs_test = sigs_test[:, :, [-1], :]\n","print(f\"Shape Env Train: {envs_train.shape}  Val: {envs_val.shape}  Test: {envs_test.shape}\")\n","\n","# Estimate mean and std of the Envelope data set\n","mean_env_train =  envs_train.mean()\n","std_env_train = envs_train.std()\n","print(f\"Mean Env: {mean_env_train}  Std Env: {std_env_train}\")\n","\n","# Normalize the data\n","eegs_train = (eegs_train - mean_eeg_train) / std_eeg_train\n","eegs_val = (eegs_val - mean_eeg_train) / std_eeg_train\n","eegs_test = (eegs_test - mean_eeg_train) / std_eeg_train\n","\n","envs_train = (envs_train - mean_env_train) / std_env_train\n","envs_val = (envs_val - mean_env_train) / std_env_train\n","envs_test = (envs_test - mean_env_train) / std_env_train\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tesRTfOdZQe2"},"source":["### Pytorch dataloader"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1677715379825,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"HviGOmH1ZQe2"},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, eeg, env):\n","        self.eeg = eeg\n","        self.env = env\n","    \n","    def __getitem__(self, index):\n","        return self.eeg[index], self.env[index]\n","    \n","    def __len__(self):\n","        return len(self.eeg)\n","    \n","dataset_train = MyDataset(eegs_train, envs_train)\n","dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)\n","\n","dl_val = DataLoader(MyDataset(eegs_val, envs_val), batch_size=batch_size, shuffle=True, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"XMjI2NeFZQe3"},"source":["## Model"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1677715379827,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"57o2oV6VZQe3"},"outputs":[],"source":["class Conv2d(nn.Conv2d):\n","    def __init__(self, in_channels, out_channels, kernel_size, **kargs):\n","        super().__init__(in_channels, out_channels, kernel_size, **kargs)\n","\n","    def __call__(self, inp):\n","        self.out = super().__call__(inp)\n","\n","        if self.out.requires_grad:\n","            self.out.retain_grad()\n","\n","        return self.out\n","    \n","    # -----------------------------------------------------------------------------------------------\n","class Flatten:\n","    \n","  def __call__(self, x):\n","    self.out = x.view(x.shape[0], -1)\n","    return self.out\n","  \n","  def parameters(self):\n","    return []\n","  \n","  # -----------------------------------------------------------------------------------------------\n","class Linear(nn.Linear):\n","    def __init__(self, x, y, **kargs):\n","        super().__init__(x, y, **kargs)\n","\n","    def __call__(self, inp):\n","        self.out = super().__call__(inp)\n","        return self.out\n","  # -----------------------------------------------------------------------------------------------\n","   \n","class ELU(nn.ELU):\n","    def __init__(self, alpha=1.0, inplace=False):\n","        super().__init__(alpha=1.0, inplace=False)\n","\n","    def __call__(self, inp):\n","        self.out = super().__call__(inp)\n","        if self.out.requires_grad:\n","            self.out.retain_grad()\n","        return self.out\n","\n","  # -----------------------------------------------------------------------------------------------\n","class Sequential:\n","  \n","    def __init__(self, layers):\n","        self.layers = layers\n","\n","    def __call__(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        self.out = x\n","        return self.out\n","\n","    def parameters(self):\n","        # get parameters of all layers and stretch them out into one list\n","        return [p for layer in self.layers for p in layer.parameters()]\n","\n","    def named_parameters(self):\n","        # get parameters of all layers and stretch them out into one list\n","        return ((n, p) for layer in self.layers for n, p in layer.named_parameters())"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1677715379828,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"IffWKmD6ZQe3"},"outputs":[],"source":["# My implementation of the shallow convnet\n","\n","fs = 64 # sampling rate\n","T = 5 * fs # number of time points in each trial\n","C = 64 # number of EEG channels\n","F1 = 8 # number of channels (depth) in the first conv layer\n","D = 2 # number of spatial filters in the second conv layer\n","F2 = D * F1 # number of channels (depth) in the pont-wise conv layer\n","num_classes = 4 # number of classes\n","\n","shallow_covnet = Sequential([\n","    Conv2d(1, 40, (1, int(fs//2)), padding='same', bias=True),\n","    Conv2d(40, 40, (C, 1), padding=(0, 0), bias=False), nn.BatchNorm2d(40, affine=True), \n","    nn.AvgPool2d((1, 75), (1, 15)), nn.Dropout(0.5),\n","    Conv2d(40, 4, kernel_size=(1, 30), padding='same', stride=(1, 1), bias=True),\n","    nn.Flatten(1, -1), # Flatten start_dim=1, end_dim=-1\n","    Linear(62*4, 4, bias=True),\n","])\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":306,"status":"ok","timestamp":1677715380122,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"bT-TwE-oTAHE","outputId":"0c416d0e-079f-4385-e28e-f4f05422b455"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 32])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)\n","  return F.conv2d(input, weight, bias, self.stride,\n"]}],"source":["## EEG Encoder with LINEAR\n","\n","class EEGEncoderWithLinear(nn.Module):\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            C = 128, # number of EEG channels\n","            F1 = 8, # 8 or 4 number of channels (depth) in the first conv layer\n","            D = 2, # number of spatial filters in the second conv layer\n","            F2 = None # number of channels (depth) in the pont-wise conv layer\n","        ):\n","        super(EEGEncoderWithLinear, self).__init__()\n","\n","        if F2 is None:\n","            F2 = D * F1\n","\n","        self.eeg_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs/2)), padding='same', bias=True, groups=1),\n","            nn.BatchNorm2d(F1, affine=True),\n","            Conv2d(F1, out_channels=D*F1, kernel_size=(C, 1), padding=(0, 0), bias=False, groups=F1),\n","            nn.BatchNorm2d(D*F1, affine=True), ELU(), nn.AvgPool2d(1, 4), nn.Dropout(0.25),\n","                    \n","            Conv2d(F2, F2, (1, int(fs/(2*4))), padding='same', bias=False, groups=D*F1),\n","            Conv2d(D*F1, F2, kernel_size=(1, 1), padding=(0, 0), groups=1, bias=False),\n","            nn.BatchNorm2d(F2, affine=True), ELU(), nn.AvgPool2d(1, 8), nn.Dropout(0.25),\n","\n","            nn.Flatten(),\n","            nn.Linear(F2*int((T*fs)//(8*4)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.eeg_encoder(x)\n","        return x\n","\n","\n","def normalize_weights_eegnet(eeg_encoder):\n","\n","    for ix, (name, param) in enumerate(eeg_encoder.named_parameters()):\n","        if  name == 'weight' and param.ndim==4 and ix==1: # normalize conv weights to max norm 1\n","            param.data = torch.renorm(param.data, 2, 0, maxnorm=1)\n","        elif name == 'weight' and param.ndim==2: # normalize fc weights to max norm 0.25\n","            param.data = torch.renorm(param.data, 2, 0, maxnorm=0.25)\n","\n","\n","eeg_encoder_with_linear = EEGEncoderWithLinear()\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(eeg_encoder_with_linear(eegs_train[:32, :, :, :]).shape)\n","\n","#summary(eeg_encoder_with_linear, (1, 128, 640))"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":208,"status":"ok","timestamp":1677715380326,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"sQjEWHA4ZQe4","outputId":"3fa006b0-fb9a-4bb0-8b6a-24e6d4aa28d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 320])\n"]}],"source":["## EEG Encoder NO LINEAR\n","\n","class EEGEncoderNoLinear(nn.Module):\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            C = 128, # number of EEG channels\n","            F1 = 8, # 8 or 4 number of channels (depth) in the first conv layer\n","            D = 2, # number of spatial filters in the second conv layer\n","            F2 = None # number of channels (depth) in the pont-wise conv layer\n","        ):\n","        super(EEGEncoderNoLinear, self).__init__()\n","\n","        if F2 is None:\n","            F2 = D * F1\n","\n","        self.eeg_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs/2)), padding='same', bias=True, groups=1),\n","            nn.BatchNorm2d(F1, affine=True),\n","            Conv2d(F1, out_channels=D*F1, kernel_size=(C, 1), padding=(0, 0), bias=False, groups=F1),\n","            nn.BatchNorm2d(D*F1, affine=True), ELU(), nn.AvgPool2d(1, 4), nn.Dropout(0.25),\n","                    \n","            Conv2d(F2, F2, (1, int(fs/(2*4))), padding='same', bias=False, groups=D*F1),\n","            Conv2d(D*F1, F2, kernel_size=(1, 1), padding=(0, 0), groups=1, bias=False),\n","            nn.BatchNorm2d(F2, affine=True), ELU(), nn.AvgPool2d(1, 8), nn.Dropout(0.25),\n","\n","            nn.Flatten(),\n","            #nn.Linear(F2*int((T*fs)//(8*4)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.eeg_encoder(x)\n","        return x\n","\n","\n","def normalize_weights_eegnet(eeg_encoder):\n","\n","    for ix, (name, param) in enumerate(eeg_encoder.named_parameters()):\n","        if  name == 'weight' and param.ndim==4 and ix==1: # normalize conv weights to max norm 1\n","            param.data = torch.renorm(param.data, 2, 0, maxnorm=1)\n","        elif name == 'weight' and param.ndim==2: # normalize fc weights to max norm 0.25\n","            param.data = torch.renorm(param.data, 2, 0, maxnorm=0.25)\n","\n","\n","eeg_encoder_no_linear = EEGEncoderNoLinear()\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(eeg_encoder_no_linear(eegs_train[:32, :, :, :]).shape)\n","\n","#summary(eeg_encoder_no_linear, (1, 128, 640))"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":224,"status":"ok","timestamp":1677715380546,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"SfO-UwlzZQe4","outputId":"77759cd9-bcdd-4ddb-b5d1-273bff17cdbf"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 320])\n"]}],"source":["class EnvEncoder3ConvNoLinear(nn.Module):\n","\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            F1 = 4\n","        ):\n","        super(EnvEncoder3ConvNoLinear, self).__init__()\n","\n","        self.env_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs//2)), padding='same', bias=True),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1, (1, int(fs//4)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1*4, (1, int(fs//8)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1*4, affine=True), ELU(), nn.AvgPool2d(1, 8), nn.Dropout(0.5),\n","            nn.Flatten(),\n","            #nn.Linear(F1*int((T*fs)//(2*8)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.env_encoder(x)\n","        return x\n","\n","env_encoder3conv_no_linear = EnvEncoder3ConvNoLinear()\n","\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(env_encoder3conv_no_linear(envs_train[:32, :, :, :]).shape)\n","#summary(env_encoder3conv_no_linear, (1, 1, 640))"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1677715380548,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"EHbSd3XITAHF","outputId":"f6a51907-337e-4daa-b370-1b0eea414e19"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 320])\n"]}],"source":["class EnvEncoder2ConvNoLinear(nn.Module):\n","\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            F1 = 4\n","        ):\n","        super(EnvEncoder2ConvNoLinear, self).__init__()\n","\n","        self.env_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs//2)), padding='same', bias=True),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1*4, (1, int(fs//4)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1*4, affine=True), ELU(), nn.AvgPool2d(1, 16), nn.Dropout(0.5),\n","            nn.Flatten(),\n","            #nn.Linear(F1*int((T*fs)//(2*8)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.env_encoder(x)\n","        return x\n","\n","env_encoder2conv_no_linear = EnvEncoder2ConvNoLinear()\n","\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(env_encoder2conv_no_linear(envs_train[:32, :, :, :]).shape)\n","#summary(env_encoder2conv_no_linear, (1, 1, 640))"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1677715380550,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"2914ppmhTAHG","outputId":"78b61d57-1646-4e2a-e847-38d5312cc42d"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 32])\n"]}],"source":["class EnvEncoder2ConvWithLinear(nn.Module):\n","\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            F1 = 4\n","        ):\n","        super(EnvEncoder2ConvWithLinear, self).__init__()\n","\n","        self.env_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs//2)), padding='same', bias=True),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1*4, (1, int(fs//4)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1*4, affine=True), ELU(), nn.AvgPool2d(1, 16), nn.Dropout(0.5),\n","            nn.Flatten(),\n","            nn.Linear(F1*4*int((T*fs)//(8*4)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.env_encoder(x)\n","        return x\n","\n","env_encoder2conv_with_linear = EnvEncoder2ConvWithLinear()\n","\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(env_encoder2conv_with_linear(envs_train[:32, :, :, :]).shape)\n","#summary(env_encoder2conv_with_linear, (1, 1, 640))"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1677715380550,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"ojZIx-6eTAHG","outputId":"26bfc300-b31e-4239-89f8-c54590a43bf5"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 32])\n"]}],"source":["class EnvEncoder3ConvWithLinear(nn.Module):\n","\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            F1 = 4\n","        ):\n","        super(EnvEncoder3ConvWithLinear, self).__init__()\n","\n","        self.env_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs//2)), padding='same', bias=True),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1, (1, int(fs//4)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1*4, (1, int(fs//8)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1*4, affine=True), ELU(), nn.AvgPool2d(1, 8), nn.Dropout(0.5),\n","            nn.Flatten(),\n","            nn.Linear(F1*4*int((T*fs)//(4*8)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.env_encoder(x)\n","        return x\n","\n","env_encoder3conv_with_linear = EnvEncoder3ConvWithLinear()\n","\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(env_encoder3conv_with_linear(envs_train[:32, :, :, :]).shape)\n","#summary(env_encoder3conv_with_linear, (1, 1, 640))"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3968,"status":"ok","timestamp":1677715384509,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"Bp8HVS-aZQe4","outputId":"f28bcea2-e7d7-48f1-c730-79f498031ef2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CES()"]},"metadata":{},"execution_count":31}],"source":["class CES(nn.Module):\n","    def __init__(self, \n","                 eeg_encoder= None,\n","                 env_encoder = None): \n","        super().__init__()\n","\n","        self.eeg_encoder = eeg_encoder\n","        self.env_encoder = env_encoder\n","        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n","\n","    def encode_eeg(self, x):\n","        return self.eeg_encoder(x)\n","    \n","    def encode_env(self, x):\n","        return self.env_encoder(x)\n","    \n","    def forward(self, eeg, env):\n","        eeg_features = self.encode_eeg(eeg)\n","        env_features = self.encode_env(env)\n","        return eeg_features, env_features, self.logit_scale.exp()\n","  \n","\n","model = CES();\n","model.to(device)\n","#for n,p in model.named_parameters():\n","    #print(n, p.shape)\n"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1677715384511,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"kRK-Pq1VTAHG","outputId":"c2386dea-44bd-4f40-8288-9f8cd65dde2e"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Models with no Linear Layers\n"," Models with Linear Layers\n"]}],"source":["print(\" Models with no Linear Layers\")\n","eeg_encoder_no_linear = EEGEncoderNoLinear()\n","env_encoder3conv_no_linear = EnvEncoder2ConvNoLinear()\n","ces_eeg_0lin_env_3conv_0lin = CES(eeg_encoder=eeg_encoder_no_linear.eeg_encoder, env_encoder=env_encoder2conv_no_linear.env_encoder)\n","#summary(ces_eeg_0lin_env_3conv_0lin, [(1, 128, 640), (1, 1, 640)])\n","\n","eeg_encoder_no_linear = EEGEncoderNoLinear()\n","env_encoder2conv_no_linear = EnvEncoder2ConvNoLinear()\n","ces_eeg_0lin_env_2conv_0lin = CES(eeg_encoder=eeg_encoder_no_linear.eeg_encoder, env_encoder=env_encoder2conv_no_linear.env_encoder)\n","#summary(ces_eeg_0lin_env_2conv_0lin, [(1, 128, 640), (1, 1, 640)])\n","\n","\n","print(\" Models with Linear Layers\")\n","eeg_encoder_with_linear = EEGEncoderWithLinear()\n","env_encoder3conv_with_linear = EnvEncoder2ConvWithLinear()\n","ces_eeg_1lin_env_3conv_1lin = CES(eeg_encoder=eeg_encoder_with_linear.eeg_encoder, env_encoder=env_encoder3conv_with_linear.env_encoder)\n","#summary(ces_eeg_1lin_env_3conv_1lin, [(1, 128, 640), (1, 1, 640)])\n","\n","eeg_encoder_with_linear = EEGEncoderWithLinear()\n","env_encoder2conv_with_linear = EnvEncoder2ConvWithLinear()\n","ces_eeg_1lin_env_2conv_1lin = CES(eeg_encoder=eeg_encoder_with_linear.eeg_encoder, env_encoder=env_encoder2conv_with_linear.env_encoder)\n","#summary(ces_eeg_1lin_env_2conv_1lin, [(1, 128, 640), (1, 1, 640)])\n","\n","models_name = [\"eeg0lin_env3conv0lin\", \"eeg0lin_env2conv0lin\", \"eeg1lin_env3conv1lin\", \"eeg1lin_env2conv1lin\"]\n","models_dict = {\"eeg0lin_env3conv0lin\": ces_eeg_0lin_env_3conv_0lin, \"eeg0lin_env2conv0lin\": ces_eeg_0lin_env_2conv_0lin, \n","               \"eeg1lin_env3conv1lin\": ces_eeg_1lin_env_3conv_1lin, \"eeg1lin_env2conv1lin\": ces_eeg_1lin_env_2conv_1lin}\n"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":3732333,"status":"ok","timestamp":1677719116837,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"d4iJiosNZQe7","outputId":"9160515d-7a24-4a1c-94e8-18675ac91f91"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------New model: eeg0lin_env3conv0lin----------------------+\n"]},{"output_type":"display_data","data":{"text/plain":["Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["NumExpr defaulting to 8 threads.\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">NumExpr defaulting to 8 threads.\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["====== Epoch: 1\n","====> Validation loss: 3.3378,  X1 loss: 3.3263   X2 loss: 3.3493\n","====== Epoch: 2\n","====> Validation loss: 3.1654,  X1 loss: 3.1622   X2 loss: 3.1686\n","====== Epoch: 3\n","====> Validation loss: 3.0379,  X1 loss: 3.0389   X2 loss: 3.0369\n","====== Epoch: 4\n","====> Validation loss: 3.0184,  X1 loss: 3.0228   X2 loss: 3.0139\n","====== Epoch: 5\n","====> Validation loss: 2.9508,  X1 loss: 2.9489   X2 loss: 2.9526\n","====== Epoch: 6\n","====> Validation loss: 2.8869,  X1 loss: 2.8833   X2 loss: 2.8906\n","====== Epoch: 7\n","====> Validation loss: 2.8744,  X1 loss: 2.8652   X2 loss: 2.8836\n","====== Epoch: 8\n","====> Validation loss: 2.8767,  X1 loss: 2.8661   X2 loss: 2.8874\n","====== Epoch: 9\n","====> Validation loss: 2.8223,  X1 loss: 2.8210   X2 loss: 2.8235\n","====== Epoch: 10\n","====> Validation loss: 2.8715,  X1 loss: 2.8676   X2 loss: 2.8754\n","====== Epoch: 11\n","====> Validation loss: 2.8148,  X1 loss: 2.8006   X2 loss: 2.8290\n","====== Epoch: 12\n","====> Validation loss: 2.8716,  X1 loss: 2.8598   X2 loss: 2.8835\n","====== Epoch: 13\n","====> Validation loss: 2.8472,  X1 loss: 2.8406   X2 loss: 2.8539\n","====== Epoch: 14\n","====> Validation loss: 2.8247,  X1 loss: 2.8124   X2 loss: 2.8369\n","====== Epoch: 15\n","====> Validation loss: 2.7724,  X1 loss: 2.7726   X2 loss: 2.7722\n","====== Epoch: 16\n","====> Validation loss: 2.7569,  X1 loss: 2.7427   X2 loss: 2.7712\n","====== Epoch: 17\n","====> Validation loss: 2.8100,  X1 loss: 2.8010   X2 loss: 2.8190\n","====== Epoch: 18\n","====> Validation loss: 2.7745,  X1 loss: 2.7728   X2 loss: 2.7761\n","====== Epoch: 19\n","====> Validation loss: 2.7985,  X1 loss: 2.7837   X2 loss: 2.8133\n","====== Epoch: 20\n","====> Validation loss: 2.7736,  X1 loss: 2.7611   X2 loss: 2.7862\n","====== Epoch: 21\n","====> Validation loss: 2.7926,  X1 loss: 2.7913   X2 loss: 2.7939\n","====== Epoch: 22\n","====> Validation loss: 2.8388,  X1 loss: 2.8331   X2 loss: 2.8445\n","====== Epoch: 23\n","====> Validation loss: 2.7922,  X1 loss: 2.7824   X2 loss: 2.8020\n","====== Epoch: 24\n","====> Validation loss: 2.7899,  X1 loss: 2.7886   X2 loss: 2.7912\n","====== Epoch: 25\n","====> Validation loss: 2.7054,  X1 loss: 2.7014   X2 loss: 2.7094\n","====== Epoch: 26\n","====> Validation loss: 2.7503,  X1 loss: 2.7378   X2 loss: 2.7627\n","====== Epoch: 27\n","====> Validation loss: 2.8103,  X1 loss: 2.7999   X2 loss: 2.8207\n","====== Epoch: 28\n","====> Validation loss: 2.7077,  X1 loss: 2.6994   X2 loss: 2.7160\n","====== Epoch: 29\n","====> Validation loss: 2.7956,  X1 loss: 2.7790   X2 loss: 2.8122\n","====== Epoch: 30\n","====> Validation loss: 2.7300,  X1 loss: 2.7216   X2 loss: 2.7384\n","====== Epoch: 31\n","====> Validation loss: 2.8120,  X1 loss: 2.7919   X2 loss: 2.8322\n","====== Epoch: 32\n","====> Validation loss: 2.7759,  X1 loss: 2.7678   X2 loss: 2.7841\n","====== Epoch: 33\n","====> Validation loss: 2.7911,  X1 loss: 2.7695   X2 loss: 2.8126\n","====== Epoch: 34\n","====> Validation loss: 2.7867,  X1 loss: 2.7758   X2 loss: 2.7976\n","====== Epoch: 35\n","====> Validation loss: 2.7085,  X1 loss: 2.7036   X2 loss: 2.7133\n","====== Epoch: 36\n","====> Validation loss: 2.7758,  X1 loss: 2.7721   X2 loss: 2.7795\n","====== Epoch: 37\n","====> Validation loss: 2.7760,  X1 loss: 2.7588   X2 loss: 2.7932\n","====== Epoch: 38\n","====> Validation loss: 2.7165,  X1 loss: 2.7131   X2 loss: 2.7200\n","====== Epoch: 39\n","====> Validation loss: 2.7471,  X1 loss: 2.7377   X2 loss: 2.7566\n","====== Epoch: 40\n","====> Validation loss: 2.7879,  X1 loss: 2.7755   X2 loss: 2.8003\n","====== Epoch: 41\n","====> Validation loss: 2.7036,  X1 loss: 2.6957   X2 loss: 2.7116\n","====== Epoch: 42\n","====> Validation loss: 2.8034,  X1 loss: 2.7980   X2 loss: 2.8088\n","====== Epoch: 43\n","====> Validation loss: 2.7223,  X1 loss: 2.7166   X2 loss: 2.7279\n","====== Epoch: 44\n","====> Validation loss: 2.7387,  X1 loss: 2.7249   X2 loss: 2.7524\n","====== Epoch: 45\n","====> Validation loss: 2.8123,  X1 loss: 2.7944   X2 loss: 2.8303\n","====== Epoch: 46\n","====> Validation loss: 2.7171,  X1 loss: 2.7055   X2 loss: 2.7287\n","====== Epoch: 47\n","====> Validation loss: 2.7751,  X1 loss: 2.7612   X2 loss: 2.7890\n","====== Epoch: 48\n","====> Validation loss: 2.7597,  X1 loss: 2.7488   X2 loss: 2.7705\n","====== Epoch: 49\n","====> Validation loss: 2.7751,  X1 loss: 2.7657   X2 loss: 2.7845\n","====== Epoch: 50\n","====> Validation loss: 2.7154,  X1 loss: 2.6973   X2 loss: 2.7334\n","====== Epoch: 51\n","====> Validation loss: 2.8452,  X1 loss: 2.7893   X2 loss: 2.9011\n","====== Epoch: 52\n","====> Validation loss: 2.7592,  X1 loss: 2.7332   X2 loss: 2.7852\n","====== Epoch: 53\n","====> Validation loss: 2.7298,  X1 loss: 2.7241   X2 loss: 2.7355\n","====== Epoch: 54\n","====> Validation loss: 2.7601,  X1 loss: 2.7546   X2 loss: 2.7657\n","====== Epoch: 55\n","====> Validation loss: 2.6913,  X1 loss: 2.6815   X2 loss: 2.7012\n","====== Epoch: 56\n","====> Validation loss: 2.7381,  X1 loss: 2.7346   X2 loss: 2.7416\n","====== Epoch: 57\n","====> Validation loss: 2.7785,  X1 loss: 2.7599   X2 loss: 2.7971\n","====== Epoch: 58\n","====> Validation loss: 2.7947,  X1 loss: 2.7796   X2 loss: 2.8099\n","====== Epoch: 59\n","====> Validation loss: 2.7160,  X1 loss: 2.7090   X2 loss: 2.7231\n","====== Epoch: 60\n","====> Validation loss: 2.6925,  X1 loss: 2.6788   X2 loss: 2.7061\n","====== Epoch: 61\n","====> Validation loss: 2.7425,  X1 loss: 2.7239   X2 loss: 2.7612\n","====== Epoch: 62\n","====> Validation loss: 2.7638,  X1 loss: 2.7365   X2 loss: 2.7910\n","====== Epoch: 63\n","====> Validation loss: 2.7504,  X1 loss: 2.7356   X2 loss: 2.7652\n","====== Epoch: 64\n","====> Validation loss: 2.7609,  X1 loss: 2.7619   X2 loss: 2.7600\n","====== Epoch: 65\n","====> Validation loss: 2.7319,  X1 loss: 2.7321   X2 loss: 2.7317\n","====== Epoch: 66\n","====> Validation loss: 2.7491,  X1 loss: 2.7284   X2 loss: 2.7697\n","====== Epoch: 67\n","====> Validation loss: 2.7689,  X1 loss: 2.7540   X2 loss: 2.7838\n","====== Epoch: 68\n","====> Validation loss: 2.7227,  X1 loss: 2.7012   X2 loss: 2.7441\n","====== Epoch: 69\n","====> Validation loss: 2.7461,  X1 loss: 2.7343   X2 loss: 2.7579\n","====== Epoch: 70\n","====> Validation loss: 2.7110,  X1 loss: 2.6992   X2 loss: 2.7229\n","====== Epoch: 71\n","====> Validation loss: 2.7382,  X1 loss: 2.7279   X2 loss: 2.7485\n","====== Epoch: 72\n","====> Validation loss: 2.6854,  X1 loss: 2.6720   X2 loss: 2.6989\n","====== Epoch: 73\n","====> Validation loss: 2.7514,  X1 loss: 2.7439   X2 loss: 2.7588\n","====== Epoch: 74\n","====> Validation loss: 2.7833,  X1 loss: 2.7673   X2 loss: 2.7993\n","====== Epoch: 75\n","====> Validation loss: 2.6640,  X1 loss: 2.6595   X2 loss: 2.6684\n","====== Epoch: 76\n","====> Validation loss: 2.7688,  X1 loss: 2.7671   X2 loss: 2.7706\n","====== Epoch: 77\n","====> Validation loss: 2.7305,  X1 loss: 2.7158   X2 loss: 2.7451\n","====== Epoch: 78\n","====> Validation loss: 2.7538,  X1 loss: 2.7416   X2 loss: 2.7660\n","====== Epoch: 79\n","====> Validation loss: 2.7676,  X1 loss: 2.7511   X2 loss: 2.7842\n","====== Epoch: 80\n","====> Validation loss: 2.6774,  X1 loss: 2.6722   X2 loss: 2.6826\n","====== Epoch: 81\n","====> Validation loss: 2.7335,  X1 loss: 2.7228   X2 loss: 2.7441\n","====== Epoch: 82\n","====> Validation loss: 2.6377,  X1 loss: 2.6294   X2 loss: 2.6461\n","====== Epoch: 83\n","====> Validation loss: 2.7121,  X1 loss: 2.7009   X2 loss: 2.7232\n","====== Epoch: 84\n","====> Validation loss: 2.7689,  X1 loss: 2.7535   X2 loss: 2.7843\n","====== Epoch: 85\n","====> Validation loss: 2.7050,  X1 loss: 2.6898   X2 loss: 2.7201\n","====== Epoch: 86\n","====> Validation loss: 2.6918,  X1 loss: 2.6899   X2 loss: 2.6937\n","====== Epoch: 87\n","====> Validation loss: 2.7635,  X1 loss: 2.7466   X2 loss: 2.7805\n","====== Epoch: 88\n","====> Validation loss: 2.7418,  X1 loss: 2.7394   X2 loss: 2.7442\n","====== Epoch: 89\n","====> Validation loss: 2.7690,  X1 loss: 2.7592   X2 loss: 2.7788\n","====== Epoch: 90\n","====> Validation loss: 2.8106,  X1 loss: 2.7898   X2 loss: 2.8314\n","====== Epoch: 91\n","====> Validation loss: 2.7105,  X1 loss: 2.7059   X2 loss: 2.7151\n","====== Epoch: 92\n","====> Validation loss: 2.7660,  X1 loss: 2.7475   X2 loss: 2.7845\n","====== Epoch: 93\n","====> Validation loss: 2.7607,  X1 loss: 2.7516   X2 loss: 2.7699\n","====== Epoch: 94\n","====> Validation loss: 2.7836,  X1 loss: 2.7689   X2 loss: 2.7983\n","====== Epoch: 95\n","====> Validation loss: 2.7196,  X1 loss: 2.7068   X2 loss: 2.7325\n","====== Epoch: 96\n","====> Validation loss: 2.6822,  X1 loss: 2.6759   X2 loss: 2.6886\n","====== Epoch: 97\n","====> Validation loss: 2.6289,  X1 loss: 2.6212   X2 loss: 2.6365\n","====== Epoch: 98\n","====> Validation loss: 2.7342,  X1 loss: 2.7242   X2 loss: 2.7442\n","====== Epoch: 99\n","====> Validation loss: 2.7072,  X1 loss: 2.6979   X2 loss: 2.7166\n","+--------------New model: eeg0lin_env2conv0lin----------------------+\n","====== Epoch: 1\n","====> Validation loss: 3.1624,  X1 loss: 3.1527   X2 loss: 3.1722\n","====== Epoch: 2\n","====> Validation loss: 3.0956,  X1 loss: 3.0943   X2 loss: 3.0969\n","====== Epoch: 3\n","====> Validation loss: 2.9732,  X1 loss: 2.9609   X2 loss: 2.9854\n","====== Epoch: 4\n","====> Validation loss: 2.9561,  X1 loss: 2.9575   X2 loss: 2.9546\n","====== Epoch: 5\n","====> Validation loss: 2.9731,  X1 loss: 2.9739   X2 loss: 2.9723\n","====== Epoch: 6\n","====> Validation loss: 2.9225,  X1 loss: 2.9342   X2 loss: 2.9108\n","====== Epoch: 7\n","====> Validation loss: 2.8860,  X1 loss: 2.8883   X2 loss: 2.8837\n","====== Epoch: 8\n","====> Validation loss: 2.9207,  X1 loss: 2.9185   X2 loss: 2.9229\n","====== Epoch: 9\n","====> Validation loss: 2.8938,  X1 loss: 2.8848   X2 loss: 2.9028\n","====== Epoch: 10\n","====> Validation loss: 2.8700,  X1 loss: 2.8668   X2 loss: 2.8732\n","====== Epoch: 11\n","====> Validation loss: 2.8909,  X1 loss: 2.8831   X2 loss: 2.8987\n","====== Epoch: 12\n","====> Validation loss: 2.8563,  X1 loss: 2.8553   X2 loss: 2.8572\n","====== Epoch: 13\n","====> Validation loss: 2.8440,  X1 loss: 2.8401   X2 loss: 2.8480\n","====== Epoch: 14\n","====> Validation loss: 2.7952,  X1 loss: 2.7865   X2 loss: 2.8040\n","====== Epoch: 15\n","====> Validation loss: 2.7842,  X1 loss: 2.7786   X2 loss: 2.7898\n","====== Epoch: 16\n","====> Validation loss: 2.8377,  X1 loss: 2.8307   X2 loss: 2.8447\n","====== Epoch: 17\n","====> Validation loss: 2.8989,  X1 loss: 2.8872   X2 loss: 2.9105\n","====== Epoch: 18\n","====> Validation loss: 2.8443,  X1 loss: 2.8407   X2 loss: 2.8480\n","====== Epoch: 19\n","====> Validation loss: 2.8179,  X1 loss: 2.8166   X2 loss: 2.8192\n","====== Epoch: 20\n","====> Validation loss: 2.7859,  X1 loss: 2.7723   X2 loss: 2.7995\n","====== Epoch: 21\n","====> Validation loss: 2.7550,  X1 loss: 2.7479   X2 loss: 2.7621\n","====== Epoch: 22\n","====> Validation loss: 2.8285,  X1 loss: 2.8184   X2 loss: 2.8385\n","====== Epoch: 23\n","====> Validation loss: 2.8232,  X1 loss: 2.8129   X2 loss: 2.8335\n","====== Epoch: 24\n","====> Validation loss: 2.8230,  X1 loss: 2.8180   X2 loss: 2.8281\n","====== Epoch: 25\n","====> Validation loss: 2.8201,  X1 loss: 2.8087   X2 loss: 2.8315\n","====== Epoch: 26\n","====> Validation loss: 2.8915,  X1 loss: 2.8737   X2 loss: 2.9092\n","====== Epoch: 27\n","====> Validation loss: 2.8033,  X1 loss: 2.7935   X2 loss: 2.8131\n","====== Epoch: 28\n","====> Validation loss: 2.7877,  X1 loss: 2.7699   X2 loss: 2.8056\n","====== Epoch: 29\n","====> Validation loss: 2.7870,  X1 loss: 2.7705   X2 loss: 2.8035\n","====== Epoch: 30\n","====> Validation loss: 2.8503,  X1 loss: 2.8400   X2 loss: 2.8606\n","====== Epoch: 31\n","====> Validation loss: 2.8464,  X1 loss: 2.8295   X2 loss: 2.8632\n","====== Epoch: 32\n","====> Validation loss: 2.8047,  X1 loss: 2.7830   X2 loss: 2.8264\n","====== Epoch: 33\n","====> Validation loss: 2.8318,  X1 loss: 2.8236   X2 loss: 2.8400\n","====== Epoch: 34\n","====> Validation loss: 2.7537,  X1 loss: 2.7385   X2 loss: 2.7688\n","====== Epoch: 35\n","====> Validation loss: 2.7829,  X1 loss: 2.7844   X2 loss: 2.7815\n","====== Epoch: 36\n","====> Validation loss: 2.7852,  X1 loss: 2.7742   X2 loss: 2.7961\n","====== Epoch: 37\n","====> Validation loss: 2.8100,  X1 loss: 2.7873   X2 loss: 2.8328\n","====== Epoch: 38\n","====> Validation loss: 2.7774,  X1 loss: 2.7761   X2 loss: 2.7787\n","====== Epoch: 39\n","====> Validation loss: 2.7991,  X1 loss: 2.7890   X2 loss: 2.8091\n","====== Epoch: 40\n","====> Validation loss: 2.7884,  X1 loss: 2.7801   X2 loss: 2.7967\n","====== Epoch: 41\n","====> Validation loss: 2.7681,  X1 loss: 2.7633   X2 loss: 2.7729\n","====== Epoch: 42\n","====> Validation loss: 2.7421,  X1 loss: 2.7233   X2 loss: 2.7610\n","====== Epoch: 43\n","====> Validation loss: 2.8405,  X1 loss: 2.8264   X2 loss: 2.8545\n","====== Epoch: 44\n","====> Validation loss: 2.8240,  X1 loss: 2.8169   X2 loss: 2.8310\n","====== Epoch: 45\n","====> Validation loss: 2.7709,  X1 loss: 2.7475   X2 loss: 2.7943\n","====== Epoch: 46\n","====> Validation loss: 2.7900,  X1 loss: 2.7812   X2 loss: 2.7988\n","====== Epoch: 47\n","====> Validation loss: 2.8102,  X1 loss: 2.8085   X2 loss: 2.8119\n","====== Epoch: 48\n","====> Validation loss: 2.7543,  X1 loss: 2.7590   X2 loss: 2.7496\n","====== Epoch: 49\n","====> Validation loss: 2.7815,  X1 loss: 2.7710   X2 loss: 2.7920\n","====== Epoch: 50\n","====> Validation loss: 2.7505,  X1 loss: 2.7422   X2 loss: 2.7588\n","====== Epoch: 51\n","====> Validation loss: 2.7349,  X1 loss: 2.7264   X2 loss: 2.7434\n","====== Epoch: 52\n","====> Validation loss: 2.7918,  X1 loss: 2.7809   X2 loss: 2.8026\n","====== Epoch: 53\n","====> Validation loss: 2.7895,  X1 loss: 2.7860   X2 loss: 2.7931\n","====== Epoch: 54\n","====> Validation loss: 2.7414,  X1 loss: 2.7337   X2 loss: 2.7490\n","====== Epoch: 55\n","====> Validation loss: 2.7226,  X1 loss: 2.7123   X2 loss: 2.7328\n","====== Epoch: 56\n","====> Validation loss: 2.7339,  X1 loss: 2.7322   X2 loss: 2.7356\n","====== Epoch: 57\n","====> Validation loss: 2.8153,  X1 loss: 2.8194   X2 loss: 2.8111\n","====== Epoch: 58\n","====> Validation loss: 2.7121,  X1 loss: 2.7005   X2 loss: 2.7237\n","====== Epoch: 59\n","====> Validation loss: 2.7214,  X1 loss: 2.7120   X2 loss: 2.7307\n","====== Epoch: 60\n","====> Validation loss: 2.8244,  X1 loss: 2.8139   X2 loss: 2.8349\n","====== Epoch: 61\n","====> Validation loss: 2.7149,  X1 loss: 2.7061   X2 loss: 2.7238\n","====== Epoch: 62\n","====> Validation loss: 2.6615,  X1 loss: 2.6495   X2 loss: 2.6734\n","====== Epoch: 63\n","====> Validation loss: 2.7659,  X1 loss: 2.7403   X2 loss: 2.7915\n","====== Epoch: 64\n","====> Validation loss: 2.7705,  X1 loss: 2.7629   X2 loss: 2.7780\n","====== Epoch: 65\n","====> Validation loss: 2.7323,  X1 loss: 2.7245   X2 loss: 2.7402\n","====== Epoch: 66\n","====> Validation loss: 2.7255,  X1 loss: 2.7236   X2 loss: 2.7274\n","====== Epoch: 67\n","====> Validation loss: 2.6706,  X1 loss: 2.6672   X2 loss: 2.6740\n","====== Epoch: 68\n","====> Validation loss: 2.7887,  X1 loss: 2.7805   X2 loss: 2.7969\n","====== Epoch: 69\n","====> Validation loss: 2.7540,  X1 loss: 2.7532   X2 loss: 2.7548\n","====== Epoch: 70\n","====> Validation loss: 2.8060,  X1 loss: 2.7934   X2 loss: 2.8186\n","====== Epoch: 71\n","====> Validation loss: 2.7611,  X1 loss: 2.7609   X2 loss: 2.7613\n","====== Epoch: 72\n","====> Validation loss: 2.6992,  X1 loss: 2.6908   X2 loss: 2.7076\n","====== Epoch: 73\n","====> Validation loss: 2.8075,  X1 loss: 2.8038   X2 loss: 2.8112\n","====== Epoch: 74\n","====> Validation loss: 2.7969,  X1 loss: 2.7845   X2 loss: 2.8093\n","====== Epoch: 75\n","====> Validation loss: 2.7992,  X1 loss: 2.7880   X2 loss: 2.8103\n","====== Epoch: 76\n","====> Validation loss: 2.7060,  X1 loss: 2.6977   X2 loss: 2.7143\n","====== Epoch: 77\n","====> Validation loss: 2.7126,  X1 loss: 2.7016   X2 loss: 2.7236\n","====== Epoch: 78\n","====> Validation loss: 2.7709,  X1 loss: 2.7590   X2 loss: 2.7828\n","====== Epoch: 79\n","====> Validation loss: 2.6901,  X1 loss: 2.6776   X2 loss: 2.7026\n","====== Epoch: 80\n","====> Validation loss: 2.7384,  X1 loss: 2.7148   X2 loss: 2.7619\n","====== Epoch: 81\n","====> Validation loss: 2.7298,  X1 loss: 2.7230   X2 loss: 2.7365\n","====== Epoch: 82\n","====> Validation loss: 2.7967,  X1 loss: 2.7868   X2 loss: 2.8066\n","====== Epoch: 83\n","====> Validation loss: 2.7290,  X1 loss: 2.7308   X2 loss: 2.7272\n","====== Epoch: 84\n","====> Validation loss: 2.7676,  X1 loss: 2.7522   X2 loss: 2.7830\n","====== Epoch: 85\n","====> Validation loss: 2.7228,  X1 loss: 2.7096   X2 loss: 2.7360\n","====== Epoch: 86\n","====> Validation loss: 2.7571,  X1 loss: 2.7510   X2 loss: 2.7633\n","====== Epoch: 87\n","====> Validation loss: 2.7434,  X1 loss: 2.7337   X2 loss: 2.7531\n","====== Epoch: 88\n","====> Validation loss: 2.7001,  X1 loss: 2.6898   X2 loss: 2.7105\n","====== Epoch: 89\n","====> Validation loss: 2.6844,  X1 loss: 2.6823   X2 loss: 2.6865\n","====== Epoch: 90\n","====> Validation loss: 2.6707,  X1 loss: 2.6475   X2 loss: 2.6939\n","====== Epoch: 91\n","====> Validation loss: 2.7246,  X1 loss: 2.7101   X2 loss: 2.7392\n","====== Epoch: 92\n","====> Validation loss: 2.7346,  X1 loss: 2.7211   X2 loss: 2.7481\n","====== Epoch: 93\n","====> Validation loss: 2.7409,  X1 loss: 2.7307   X2 loss: 2.7512\n","====== Epoch: 94\n","====> Validation loss: 2.7785,  X1 loss: 2.7645   X2 loss: 2.7926\n","====== Epoch: 95\n","====> Validation loss: 2.7475,  X1 loss: 2.7300   X2 loss: 2.7650\n","====== Epoch: 96\n","====> Validation loss: 2.7089,  X1 loss: 2.6978   X2 loss: 2.7200\n","====== Epoch: 97\n","====> Validation loss: 2.8182,  X1 loss: 2.8162   X2 loss: 2.8202\n","====== Epoch: 98\n","====> Validation loss: 2.7599,  X1 loss: 2.7493   X2 loss: 2.7705\n","====== Epoch: 99\n","====> Validation loss: 2.7298,  X1 loss: 2.7150   X2 loss: 2.7447\n","+--------------New model: eeg1lin_env3conv1lin----------------------+\n","====== Epoch: 1\n","====> Validation loss: 3.4671,  X1 loss: 3.4707   X2 loss: 3.4635\n","====== Epoch: 2\n","====> Validation loss: 3.3620,  X1 loss: 3.3623   X2 loss: 3.3617\n","====== Epoch: 3\n","====> Validation loss: 3.2659,  X1 loss: 3.2642   X2 loss: 3.2677\n","====== Epoch: 4\n","====> Validation loss: 3.1829,  X1 loss: 3.1804   X2 loss: 3.1855\n","====== Epoch: 5\n","====> Validation loss: 3.1342,  X1 loss: 3.1343   X2 loss: 3.1341\n","====== Epoch: 6\n","====> Validation loss: 3.1083,  X1 loss: 3.1057   X2 loss: 3.1109\n","====== Epoch: 7\n","====> Validation loss: 3.0835,  X1 loss: 3.0819   X2 loss: 3.0850\n","====== Epoch: 8\n","====> Validation loss: 3.1239,  X1 loss: 3.1196   X2 loss: 3.1282\n","====== Epoch: 9\n","====> Validation loss: 3.0685,  X1 loss: 3.0627   X2 loss: 3.0744\n","====== Epoch: 10\n","====> Validation loss: 3.0349,  X1 loss: 3.0336   X2 loss: 3.0361\n","====== Epoch: 11\n","====> Validation loss: 3.0731,  X1 loss: 3.0689   X2 loss: 3.0774\n","====== Epoch: 12\n","====> Validation loss: 3.0566,  X1 loss: 3.0547   X2 loss: 3.0584\n","====== Epoch: 13\n","====> Validation loss: 3.0852,  X1 loss: 3.0862   X2 loss: 3.0842\n","====== Epoch: 14\n","====> Validation loss: 3.0705,  X1 loss: 3.0702   X2 loss: 3.0708\n","====== Epoch: 15\n","====> Validation loss: 3.0626,  X1 loss: 3.0643   X2 loss: 3.0609\n","====== Epoch: 16\n","====> Validation loss: 3.0855,  X1 loss: 3.0836   X2 loss: 3.0874\n","====== Epoch: 17\n","====> Validation loss: 3.0560,  X1 loss: 3.0577   X2 loss: 3.0543\n","====== Epoch: 18\n","====> Validation loss: 3.0585,  X1 loss: 3.0582   X2 loss: 3.0588\n","====== Epoch: 19\n","====> Validation loss: 3.0440,  X1 loss: 3.0368   X2 loss: 3.0512\n","====== Epoch: 20\n","====> Validation loss: 3.0290,  X1 loss: 3.0256   X2 loss: 3.0325\n","====== Epoch: 21\n","====> Validation loss: 3.0575,  X1 loss: 3.0552   X2 loss: 3.0597\n","====== Epoch: 22\n","====> Validation loss: 3.0703,  X1 loss: 3.0707   X2 loss: 3.0700\n","====== Epoch: 23\n","====> Validation loss: 3.0819,  X1 loss: 3.0810   X2 loss: 3.0829\n","====== Epoch: 24\n","====> Validation loss: 3.0732,  X1 loss: 3.0675   X2 loss: 3.0789\n","====== Epoch: 25\n","====> Validation loss: 3.0214,  X1 loss: 3.0233   X2 loss: 3.0196\n","====== Epoch: 26\n","====> Validation loss: 3.0442,  X1 loss: 3.0427   X2 loss: 3.0457\n","====== Epoch: 27\n","====> Validation loss: 3.0725,  X1 loss: 3.0734   X2 loss: 3.0715\n","====== Epoch: 28\n","====> Validation loss: 3.0570,  X1 loss: 3.0535   X2 loss: 3.0604\n","====== Epoch: 29\n","====> Validation loss: 3.0886,  X1 loss: 3.0832   X2 loss: 3.0940\n","====== Epoch: 30\n","====> Validation loss: 3.0467,  X1 loss: 3.0468   X2 loss: 3.0465\n","====== Epoch: 31\n","====> Validation loss: 3.0891,  X1 loss: 3.0864   X2 loss: 3.0918\n","====== Epoch: 32\n","====> Validation loss: 3.0850,  X1 loss: 3.0868   X2 loss: 3.0832\n","====== Epoch: 33\n","====> Validation loss: 3.0572,  X1 loss: 3.0562   X2 loss: 3.0583\n","====== Epoch: 34\n","====> Validation loss: 3.0717,  X1 loss: 3.0718   X2 loss: 3.0716\n","====== Epoch: 35\n","====> Validation loss: 3.1254,  X1 loss: 3.1213   X2 loss: 3.1295\n","====== Epoch: 36\n","====> Validation loss: 3.0501,  X1 loss: 3.0467   X2 loss: 3.0535\n","====== Epoch: 37\n","====> Validation loss: 3.0608,  X1 loss: 3.0611   X2 loss: 3.0606\n","====== Epoch: 38\n","====> Validation loss: 3.0608,  X1 loss: 3.0595   X2 loss: 3.0622\n","====== Epoch: 39\n","====> Validation loss: 3.0871,  X1 loss: 3.0868   X2 loss: 3.0874\n","====== Epoch: 40\n","====> Validation loss: 3.1162,  X1 loss: 3.1169   X2 loss: 3.1155\n","====== Epoch: 41\n","====> Validation loss: 3.0947,  X1 loss: 3.0926   X2 loss: 3.0968\n","====== Epoch: 42\n","====> Validation loss: 3.0598,  X1 loss: 3.0593   X2 loss: 3.0603\n","====== Epoch: 43\n","====> Validation loss: 3.0629,  X1 loss: 3.0608   X2 loss: 3.0650\n","====== Epoch: 44\n","====> Validation loss: 3.0867,  X1 loss: 3.0866   X2 loss: 3.0867\n","====== Epoch: 45\n","====> Validation loss: 3.0831,  X1 loss: 3.0859   X2 loss: 3.0802\n","====== Epoch: 46\n","====> Validation loss: 3.0515,  X1 loss: 3.0514   X2 loss: 3.0515\n","====== Epoch: 47\n","====> Validation loss: 3.0778,  X1 loss: 3.0751   X2 loss: 3.0805\n","====== Epoch: 48\n","====> Validation loss: 3.1134,  X1 loss: 3.1118   X2 loss: 3.1149\n","====== Epoch: 49\n","====> Validation loss: 3.0737,  X1 loss: 3.0781   X2 loss: 3.0694\n","====== Epoch: 50\n","====> Validation loss: 3.1024,  X1 loss: 3.1059   X2 loss: 3.0989\n","====== Epoch: 51\n","====> Validation loss: 3.0246,  X1 loss: 3.0207   X2 loss: 3.0285\n","====== Epoch: 52\n","====> Validation loss: 3.1289,  X1 loss: 3.1268   X2 loss: 3.1311\n","====== Epoch: 53\n","====> Validation loss: 3.1536,  X1 loss: 3.1494   X2 loss: 3.1578\n","====== Epoch: 54\n","====> Validation loss: 3.1894,  X1 loss: 3.1874   X2 loss: 3.1914\n","====== Epoch: 55\n","====> Validation loss: 3.0875,  X1 loss: 3.0849   X2 loss: 3.0901\n","====== Epoch: 56\n","====> Validation loss: 3.1494,  X1 loss: 3.1425   X2 loss: 3.1563\n","====== Epoch: 57\n","====> Validation loss: 3.1009,  X1 loss: 3.0981   X2 loss: 3.1037\n","====== Epoch: 58\n","====> Validation loss: 3.0652,  X1 loss: 3.0637   X2 loss: 3.0667\n","====== Epoch: 59\n","====> Validation loss: 3.0982,  X1 loss: 3.0932   X2 loss: 3.1032\n","====== Epoch: 60\n","====> Validation loss: 3.1098,  X1 loss: 3.1036   X2 loss: 3.1160\n","====== Epoch: 61\n","====> Validation loss: 3.1506,  X1 loss: 3.1453   X2 loss: 3.1560\n","====== Epoch: 62\n","====> Validation loss: 3.1447,  X1 loss: 3.1442   X2 loss: 3.1453\n","====== Epoch: 63\n","====> Validation loss: 3.1508,  X1 loss: 3.1398   X2 loss: 3.1618\n","====== Epoch: 64\n","====> Validation loss: 3.0968,  X1 loss: 3.0955   X2 loss: 3.0981\n","====== Epoch: 65\n","====> Validation loss: 3.1417,  X1 loss: 3.1383   X2 loss: 3.1451\n","====== Epoch: 66\n","====> Validation loss: 3.1323,  X1 loss: 3.1331   X2 loss: 3.1316\n","====== Epoch: 67\n","====> Validation loss: 3.1170,  X1 loss: 3.1157   X2 loss: 3.1183\n","====== Epoch: 68\n","====> Validation loss: 3.1029,  X1 loss: 3.1021   X2 loss: 3.1038\n","====== Epoch: 69\n","====> Validation loss: 3.0672,  X1 loss: 3.0695   X2 loss: 3.0650\n","====== Epoch: 70\n","====> Validation loss: 3.0931,  X1 loss: 3.0909   X2 loss: 3.0953\n","====== Epoch: 71\n","====> Validation loss: 3.1412,  X1 loss: 3.1436   X2 loss: 3.1389\n","====== Epoch: 72\n","====> Validation loss: 3.1278,  X1 loss: 3.1287   X2 loss: 3.1268\n","====== Epoch: 73\n","====> Validation loss: 3.1503,  X1 loss: 3.1535   X2 loss: 3.1472\n","====== Epoch: 74\n","====> Validation loss: 3.1837,  X1 loss: 3.1825   X2 loss: 3.1849\n","====== Epoch: 75\n","====> Validation loss: 3.1175,  X1 loss: 3.1105   X2 loss: 3.1246\n","====== Epoch: 76\n","====> Validation loss: 3.1081,  X1 loss: 3.1039   X2 loss: 3.1122\n","====== Epoch: 77\n","====> Validation loss: 3.0951,  X1 loss: 3.0963   X2 loss: 3.0938\n","====== Epoch: 78\n","====> Validation loss: 3.1552,  X1 loss: 3.1518   X2 loss: 3.1586\n","====== Epoch: 79\n","====> Validation loss: 3.1685,  X1 loss: 3.1636   X2 loss: 3.1734\n","====== Epoch: 80\n","====> Validation loss: 3.1361,  X1 loss: 3.1276   X2 loss: 3.1446\n","====== Epoch: 81\n","====> Validation loss: 3.1471,  X1 loss: 3.1408   X2 loss: 3.1534\n","====== Epoch: 82\n","====> Validation loss: 3.1655,  X1 loss: 3.1627   X2 loss: 3.1683\n","====== Epoch: 83\n","====> Validation loss: 3.1445,  X1 loss: 3.1357   X2 loss: 3.1534\n","====== Epoch: 84\n","====> Validation loss: 3.1617,  X1 loss: 3.1605   X2 loss: 3.1629\n","====== Epoch: 85\n","====> Validation loss: 3.1161,  X1 loss: 3.1107   X2 loss: 3.1214\n","====== Epoch: 86\n","====> Validation loss: 3.1230,  X1 loss: 3.1198   X2 loss: 3.1263\n","====== Epoch: 87\n","====> Validation loss: 3.1735,  X1 loss: 3.1689   X2 loss: 3.1780\n","====== Epoch: 88\n","====> Validation loss: 3.1399,  X1 loss: 3.1502   X2 loss: 3.1295\n","====== Epoch: 89\n","====> Validation loss: 3.1625,  X1 loss: 3.1605   X2 loss: 3.1644\n","====== Epoch: 90\n","====> Validation loss: 3.1692,  X1 loss: 3.1654   X2 loss: 3.1731\n","====== Epoch: 91\n","====> Validation loss: 3.1342,  X1 loss: 3.1305   X2 loss: 3.1379\n","====== Epoch: 92\n","====> Validation loss: 3.1728,  X1 loss: 3.1729   X2 loss: 3.1727\n","====== Epoch: 93\n","====> Validation loss: 3.1100,  X1 loss: 3.1119   X2 loss: 3.1081\n","====== Epoch: 94\n","====> Validation loss: 3.1117,  X1 loss: 3.1127   X2 loss: 3.1107\n","====== Epoch: 95\n","====> Validation loss: 3.1670,  X1 loss: 3.1651   X2 loss: 3.1689\n","====== Epoch: 96\n","====> Validation loss: 3.2017,  X1 loss: 3.1972   X2 loss: 3.2063\n","====== Epoch: 97\n","====> Validation loss: 3.1358,  X1 loss: 3.1279   X2 loss: 3.1437\n","====== Epoch: 98\n","====> Validation loss: 3.1204,  X1 loss: 3.1240   X2 loss: 3.1167\n","====== Epoch: 99\n","====> Validation loss: 3.1557,  X1 loss: 3.1455   X2 loss: 3.1659\n","+--------------New model: eeg1lin_env2conv1lin----------------------+\n","====== Epoch: 1\n","====> Validation loss: 3.4706,  X1 loss: 3.4721   X2 loss: 3.4692\n","====== Epoch: 2\n","====> Validation loss: 3.4461,  X1 loss: 3.4465   X2 loss: 3.4456\n","====== Epoch: 3\n","====> Validation loss: 3.3642,  X1 loss: 3.3649   X2 loss: 3.3635\n","====== Epoch: 4\n","====> Validation loss: 3.2473,  X1 loss: 3.2465   X2 loss: 3.2481\n","====== Epoch: 5\n","====> Validation loss: 3.1781,  X1 loss: 3.1772   X2 loss: 3.1789\n","====== Epoch: 6\n","====> Validation loss: 3.1413,  X1 loss: 3.1393   X2 loss: 3.1434\n","====== Epoch: 7\n","====> Validation loss: 3.0831,  X1 loss: 3.0836   X2 loss: 3.0826\n","====== Epoch: 8\n","====> Validation loss: 3.0876,  X1 loss: 3.0857   X2 loss: 3.0895\n","====== Epoch: 9\n","====> Validation loss: 3.0760,  X1 loss: 3.0744   X2 loss: 3.0775\n","====== Epoch: 10\n","====> Validation loss: 3.0444,  X1 loss: 3.0436   X2 loss: 3.0451\n","====== Epoch: 11\n","====> Validation loss: 3.0237,  X1 loss: 3.0197   X2 loss: 3.0277\n","====== Epoch: 12\n","====> Validation loss: 3.0339,  X1 loss: 3.0352   X2 loss: 3.0326\n","====== Epoch: 13\n","====> Validation loss: 3.0329,  X1 loss: 3.0302   X2 loss: 3.0356\n","====== Epoch: 14\n","====> Validation loss: 3.0339,  X1 loss: 3.0315   X2 loss: 3.0364\n","====== Epoch: 15\n","====> Validation loss: 3.0634,  X1 loss: 3.0637   X2 loss: 3.0631\n","====== Epoch: 16\n","====> Validation loss: 3.0286,  X1 loss: 3.0305   X2 loss: 3.0267\n","====== Epoch: 17\n","====> Validation loss: 3.0045,  X1 loss: 3.0054   X2 loss: 3.0035\n","====== Epoch: 18\n","====> Validation loss: 3.0786,  X1 loss: 3.0780   X2 loss: 3.0791\n","====== Epoch: 19\n","====> Validation loss: 3.0267,  X1 loss: 3.0296   X2 loss: 3.0238\n","====== Epoch: 20\n","====> Validation loss: 3.0563,  X1 loss: 3.0563   X2 loss: 3.0563\n","====== Epoch: 21\n","====> Validation loss: 3.0570,  X1 loss: 3.0549   X2 loss: 3.0591\n","====== Epoch: 22\n","====> Validation loss: 3.0164,  X1 loss: 3.0194   X2 loss: 3.0133\n","====== Epoch: 23\n","====> Validation loss: 3.0297,  X1 loss: 3.0241   X2 loss: 3.0352\n","====== Epoch: 24\n","====> Validation loss: 3.0436,  X1 loss: 3.0427   X2 loss: 3.0445\n","====== Epoch: 25\n","====> Validation loss: 3.0284,  X1 loss: 3.0309   X2 loss: 3.0259\n","====== Epoch: 26\n","====> Validation loss: 3.1013,  X1 loss: 3.0984   X2 loss: 3.1042\n","====== Epoch: 27\n","====> Validation loss: 3.1128,  X1 loss: 3.1115   X2 loss: 3.1141\n","====== Epoch: 28\n","====> Validation loss: 3.0460,  X1 loss: 3.0456   X2 loss: 3.0463\n","====== Epoch: 29\n","====> Validation loss: 2.9937,  X1 loss: 2.9942   X2 loss: 2.9933\n","====== Epoch: 30\n","====> Validation loss: 3.0769,  X1 loss: 3.0733   X2 loss: 3.0806\n","====== Epoch: 31\n","====> Validation loss: 3.0868,  X1 loss: 3.0888   X2 loss: 3.0847\n","====== Epoch: 32\n","====> Validation loss: 3.0675,  X1 loss: 3.0635   X2 loss: 3.0715\n","====== Epoch: 33\n","====> Validation loss: 3.0642,  X1 loss: 3.0604   X2 loss: 3.0679\n","====== Epoch: 34\n","====> Validation loss: 3.0333,  X1 loss: 3.0336   X2 loss: 3.0331\n","====== Epoch: 35\n","====> Validation loss: 3.0236,  X1 loss: 3.0200   X2 loss: 3.0272\n","====== Epoch: 36\n","====> Validation loss: 2.9970,  X1 loss: 2.9894   X2 loss: 3.0046\n","====== Epoch: 37\n","====> Validation loss: 3.0429,  X1 loss: 3.0417   X2 loss: 3.0441\n","====== Epoch: 38\n","====> Validation loss: 3.0609,  X1 loss: 3.0601   X2 loss: 3.0616\n","====== Epoch: 39\n","====> Validation loss: 3.0913,  X1 loss: 3.0868   X2 loss: 3.0957\n","====== Epoch: 40\n","====> Validation loss: 3.0394,  X1 loss: 3.0384   X2 loss: 3.0405\n","====== Epoch: 41\n","====> Validation loss: 3.0710,  X1 loss: 3.0692   X2 loss: 3.0727\n","====== Epoch: 42\n","====> Validation loss: 3.0430,  X1 loss: 3.0367   X2 loss: 3.0493\n","====== Epoch: 43\n","====> Validation loss: 3.0308,  X1 loss: 3.0318   X2 loss: 3.0297\n","====== Epoch: 44\n","====> Validation loss: 3.0861,  X1 loss: 3.0837   X2 loss: 3.0884\n","====== Epoch: 45\n","====> Validation loss: 3.1299,  X1 loss: 3.1241   X2 loss: 3.1357\n","====== Epoch: 46\n","====> Validation loss: 3.0754,  X1 loss: 3.0773   X2 loss: 3.0734\n","====== Epoch: 47\n","====> Validation loss: 3.1081,  X1 loss: 3.1038   X2 loss: 3.1124\n","====== Epoch: 48\n","====> Validation loss: 3.1353,  X1 loss: 3.1292   X2 loss: 3.1415\n","====== Epoch: 49\n","====> Validation loss: 3.0897,  X1 loss: 3.0831   X2 loss: 3.0963\n","====== Epoch: 50\n","====> Validation loss: 3.1064,  X1 loss: 3.0983   X2 loss: 3.1145\n","====== Epoch: 51\n","====> Validation loss: 3.0900,  X1 loss: 3.0867   X2 loss: 3.0933\n","====== Epoch: 52\n","====> Validation loss: 3.1027,  X1 loss: 3.1025   X2 loss: 3.1029\n","====== Epoch: 53\n","====> Validation loss: 3.1222,  X1 loss: 3.1197   X2 loss: 3.1247\n","====== Epoch: 54\n","====> Validation loss: 3.0972,  X1 loss: 3.1028   X2 loss: 3.0915\n","====== Epoch: 55\n","====> Validation loss: 3.0821,  X1 loss: 3.0748   X2 loss: 3.0894\n","====== Epoch: 56\n","====> Validation loss: 3.0688,  X1 loss: 3.0668   X2 loss: 3.0708\n","====== Epoch: 57\n","====> Validation loss: 3.0234,  X1 loss: 3.0206   X2 loss: 3.0261\n","====== Epoch: 58\n","====> Validation loss: 3.0859,  X1 loss: 3.0811   X2 loss: 3.0907\n","====== Epoch: 59\n","====> Validation loss: 3.1204,  X1 loss: 3.1184   X2 loss: 3.1223\n","====== Epoch: 60\n","====> Validation loss: 3.0838,  X1 loss: 3.0781   X2 loss: 3.0894\n","====== Epoch: 61\n","====> Validation loss: 3.1307,  X1 loss: 3.1249   X2 loss: 3.1365\n","====== Epoch: 62\n","====> Validation loss: 3.0852,  X1 loss: 3.0842   X2 loss: 3.0862\n","====== Epoch: 63\n","====> Validation loss: 3.1222,  X1 loss: 3.1222   X2 loss: 3.1222\n","====== Epoch: 64\n","====> Validation loss: 3.1239,  X1 loss: 3.1183   X2 loss: 3.1295\n","====== Epoch: 65\n","====> Validation loss: 3.1405,  X1 loss: 3.1384   X2 loss: 3.1426\n","====== Epoch: 66\n","====> Validation loss: 3.1439,  X1 loss: 3.1391   X2 loss: 3.1487\n","====== Epoch: 67\n","====> Validation loss: 3.0506,  X1 loss: 3.0458   X2 loss: 3.0554\n","====== Epoch: 68\n","====> Validation loss: 3.1217,  X1 loss: 3.1161   X2 loss: 3.1273\n","====== Epoch: 69\n","====> Validation loss: 3.0809,  X1 loss: 3.0757   X2 loss: 3.0861\n","====== Epoch: 70\n","====> Validation loss: 3.1035,  X1 loss: 3.1017   X2 loss: 3.1053\n","====== Epoch: 71\n","====> Validation loss: 3.1359,  X1 loss: 3.1320   X2 loss: 3.1397\n","====== Epoch: 72\n","====> Validation loss: 3.1344,  X1 loss: 3.1338   X2 loss: 3.1350\n","====== Epoch: 73\n","====> Validation loss: 3.1278,  X1 loss: 3.1294   X2 loss: 3.1262\n","====== Epoch: 74\n","====> Validation loss: 3.0726,  X1 loss: 3.0714   X2 loss: 3.0738\n","====== Epoch: 75\n","====> Validation loss: 3.0794,  X1 loss: 3.0787   X2 loss: 3.0802\n","====== Epoch: 76\n","====> Validation loss: 3.1704,  X1 loss: 3.1685   X2 loss: 3.1723\n","====== Epoch: 77\n","====> Validation loss: 3.1261,  X1 loss: 3.1228   X2 loss: 3.1295\n","====== Epoch: 78\n","====> Validation loss: 3.0960,  X1 loss: 3.0986   X2 loss: 3.0934\n","====== Epoch: 79\n","====> Validation loss: 3.1260,  X1 loss: 3.1224   X2 loss: 3.1295\n","====== Epoch: 80\n","====> Validation loss: 3.1485,  X1 loss: 3.1432   X2 loss: 3.1539\n","====== Epoch: 81\n","====> Validation loss: 3.1520,  X1 loss: 3.1474   X2 loss: 3.1567\n","====== Epoch: 82\n","====> Validation loss: 3.1347,  X1 loss: 3.1321   X2 loss: 3.1374\n","====== Epoch: 83\n","====> Validation loss: 3.1517,  X1 loss: 3.1482   X2 loss: 3.1551\n","====== Epoch: 84\n","====> Validation loss: 3.1227,  X1 loss: 3.1255   X2 loss: 3.1198\n","====== Epoch: 85\n","====> Validation loss: 3.1420,  X1 loss: 3.1392   X2 loss: 3.1449\n","====== Epoch: 86\n","====> Validation loss: 3.1780,  X1 loss: 3.1785   X2 loss: 3.1776\n","====== Epoch: 87\n","====> Validation loss: 3.1817,  X1 loss: 3.1756   X2 loss: 3.1878\n","====== Epoch: 88\n","====> Validation loss: 3.1433,  X1 loss: 3.1444   X2 loss: 3.1423\n","====== Epoch: 89\n","====> Validation loss: 3.1256,  X1 loss: 3.1221   X2 loss: 3.1290\n","====== Epoch: 90\n","====> Validation loss: 3.1466,  X1 loss: 3.1427   X2 loss: 3.1506\n","====== Epoch: 91\n","====> Validation loss: 3.1954,  X1 loss: 3.1989   X2 loss: 3.1918\n","====== Epoch: 92\n","====> Validation loss: 3.1873,  X1 loss: 3.1874   X2 loss: 3.1872\n","====== Epoch: 93\n","====> Validation loss: 3.1665,  X1 loss: 3.1670   X2 loss: 3.1661\n","====== Epoch: 94\n","====> Validation loss: 3.2035,  X1 loss: 3.1967   X2 loss: 3.2102\n","====== Epoch: 95\n","====> Validation loss: 3.1856,  X1 loss: 3.1918   X2 loss: 3.1793\n","====== Epoch: 96\n","====> Validation loss: 3.1657,  X1 loss: 3.1585   X2 loss: 3.1728\n","====== Epoch: 97\n","====> Validation loss: 3.1822,  X1 loss: 3.1850   X2 loss: 3.1793\n","====== Epoch: 98\n","====> Validation loss: 3.2046,  X1 loss: 3.1931   X2 loss: 3.2161\n","====== Epoch: 99\n","====> Validation loss: 3.1443,  X1 loss: 3.1405   X2 loss: 3.1480\n"]}],"source":["\n","lossi = []\n","udri = [] # update / data ratio \n","ud = []\n","\n","lr = 0.001\n","\n","for name, model in models_dict.items():\n","\n","    # Reset for the new model in the loop\n","    print(f\"+--------------New model: {name}----------------------+\")\n","    writer = SummaryWriter(log_dir=f\"runs/{name}_{time.strftime('%Y%m%d_%H%M%S')}\")\n","    model.to(device)\n","    optimizer = optim.NAdam(model.parameters(), lr=lr)\n","    cnt = 0\n","    loss_batches = []\n","\n","\n","    for epoch in range(1, 100):\n","\n","        print(f\"====== Epoch: {epoch}\")\n","\n","        model.train()\n","        for ix_batch, (Xb_eeg, Xb_env) in enumerate(dataloader):\n","\n","            # send to device\n","            Xb_eeg = Xb_eeg.to(device)\n","            Xb_env = Xb_env.to(device)\n","\n","            # Zero out gradients\n","            optimizer.zero_grad()\n","\n","            # forward pass\n","            eeg_features, env_features, logit_scale = model(Xb_eeg, Xb_env) \n","\n","\n","            # normalize features\n","            eeg_features_n = eeg_features / eeg_features.norm(dim=1, keepdim=True)\n","            env_features_n = env_features / env_features.norm(dim=1, keepdim=True)\n","\n","            # logits\n","            logits_per_eeg = logit_scale * eeg_features_n @ env_features_n.t()\n","            logits_per_env = logits_per_eeg.t()\n","\n","            #loss function\n","            labels = torch.arange(batch_size).to(device)\n","            loss_eeg = F.cross_entropy(logits_per_eeg, labels)\n","            loss_env = F.cross_entropy(logits_per_env, labels)\n","            loss   = (loss_eeg + loss_env)/2\n","\n","            # backward pass\n","            loss.backward()\n","            optimizer.step()\n","\n","            loss_batches.append(loss.item())\n","            cnt += 1\n","\n","            with torch.no_grad():\n","                #ud = {f\"p{ix}\":(lr*p.grad.std() / p.data.std()).log10().item() for ix, p in enumerate(model.parameters()) if p.ndim==4 }\n","                #writer.add_scalars('UpdateOData/ud', ud, cnt)\n","                writer.add_scalar('Loss/train_batch', loss.item(), cnt)\n","            \n","            #break   \n","\n","        loss_epoch = loss_batches[-(ix_batch + 1):]  # mean loss across batches\n","        loss_epoch = sum(loss_epoch) / len(loss_epoch)\n","        writer.add_scalar('Loss/train_epoch', loss_epoch, epoch)\n","        #for pname, p in model.named_parameters():\n","        #writer.add_histogram(f'Params/{pname}', p, epoch)\n","        #writer.add_histogram(f'Grads/{pname}', p.grad, epoch)\n","\n","        loss_val, *_ = eval_model_cl(dl_val, model, device=device)\n","        writer.add_scalar('Loss/val_epoch', loss_val, epoch)\n","\n","        \n","        # normalize weights\n","        with torch.no_grad():\n","            normalize_weights_eegnet(model.eeg_encoder)\n","\n","        model.train()\n","            \n","    #break   \n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"13Poz5tFnEFXpegMbhqAinRdIfSvPnPge","timestamp":1677524195292}]},"gpuClass":"premium","kernelspec":{"display_name":"mne","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"8e19e54895c02f0e9343d0fbd6cee45458aaf6f05de9ab3004d10bba5525a5d0"}}},"nbformat":4,"nbformat_minor":0}