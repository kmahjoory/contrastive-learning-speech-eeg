{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v-pxnK4OaI-b","executionInfo":{"status":"ok","timestamp":1677582171380,"user_tz":-60,"elapsed":25994,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"ef1cad12-707f-47f8-a636-466928608c49"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["\n","import os\n","import sys\n","\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"Colab Notebooks/prj_neuroread_analysis/neuroread/\"\n","GOOGLE_DRIVE_PATH = os.path.join(\"/content\", \"drive\", \"MyDrive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))\n","\n","# Add to sys so we can import .py files.\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","os.chdir(GOOGLE_DRIVE_PATH)\n","\n","# Install unavailable packages\n","import pip\n","def import_or_install(package):\n","    try:\n","        __import__(package)\n","    except ImportError:\n","        pip.main(['install', package])\n","\n","import_or_install(\"mne\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":554,"referenced_widgets":["758996c2171341da8fb111a5f1c3b4ef","33c4fb79add44b4b88c96dc6698ded61"]},"id":"ZEopbadxbJH0","executionInfo":{"status":"ok","timestamp":1677579143601,"user_tz":-60,"elapsed":7907,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"cf83d8ee-f247-41e5-a06f-44a158fef4ea"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['.git', '.DS_Store', '.gitignore', 'EEG', 'LICENSE', 'README.md', 'train_cl_eeg2speech_2.ipynb', 'train_cl_eeg2speech_rochester_subj_2.ipynb', 'train_cl_eeg2speech_rochester_v1.ipynb', 'train_eeg2speech_rochester.ipynb', 'train_cl_eeg2speech_rochester_v2.ipynb', 'train_cl_eeg2speech_rochester_v3.ipynb']\n"]},{"output_type":"stream","name":"stderr","text":["WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n","Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n","To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"]},{"output_type":"display_data","data":{"text/plain":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Collecting mne\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Collecting mne\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  Downloading mne-1.3.1-py3-none-any.whl (7.6 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Downloading mne-1.3.1-py3-none-any.whl (7.6 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"758996c2171341da8fb111a5f1c3b4ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":[],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.8/dist-packages (from mne) (1.6.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: pooch&gt;=1.5 in /usr/local/lib/python3.8/dist-packages (from mne) (1.6.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.8/dist-packages (from mne) (1.22.4)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: numpy&gt;=1.15.4 in /usr/local/lib/python3.8/dist-packages (from mne) (1.22.4)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from mne) (2.11.3)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from mne) (2.11.3)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mne) (3.5.3)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mne) (3.5.3)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from mne) (23.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from mne) (23.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from mne) (1.7.3)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: scipy&gt;=1.1.0 in /usr/local/lib/python3.8/dist-packages (from mne) (1.7.3)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from mne) (4.4.2)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from mne) (4.4.2)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from mne) (4.64.1)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from mne) (4.64.1)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.5->mne) (2.25.1)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: requests&gt;=2.19.0 in /usr/local/lib/python3.8/dist-packages (from pooch&gt;=1.5-&gt;mne) (2.25.1)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.5->mne) (1.4.4)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: appdirs&gt;=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch&gt;=1.5-&gt;mne) (1.4.4)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->mne) (2.0.1)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2-&gt;mne) (2.0.1)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (4.38.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (4.38.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (3.0.9)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: pyparsing&gt;=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (3.0.9)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (2.8.2)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (2.8.2)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (8.4.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (8.4.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (0.11.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (0.11.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (1.4.4)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (1.4.4)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib->mne) (1.15.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;mne) (1.15.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.10)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.8/dist-packages (from requests&gt;=2.19.0-&gt;pooch&gt;=1.5-&gt;mne) (2.10)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (4.0.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests&gt;=2.19.0-&gt;pooch&gt;=1.5-&gt;mne) (4.0.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.24.3)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests&gt;=2.19.0-&gt;pooch&gt;=1.5-&gt;mne) (1.24.3)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.12.7)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests&gt;=2.19.0-&gt;pooch&gt;=1.5-&gt;mne) (2022.12.7)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Installing collected packages: mne\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Installing collected packages: mne\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Successfully installed mne-1.3.1\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Successfully installed mne-1.3.1\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"M0LuZowEbY29","executionInfo":{"status":"ok","timestamp":1677579143601,"user_tz":-60,"elapsed":6,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n"," print('Not connected to a GPU')\n","else:\n"," print(gpu_info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HTqIrJcOaQLu","executionInfo":{"status":"ok","timestamp":1677579144412,"user_tz":-60,"elapsed":816,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"b5162338-9c35-42a1-f531-dcbfaa8d04cd"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 27.3 gigabytes of available RAM\n","\n","Tue Feb 28 10:12:23 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   46C    P0    26W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gGggzCoKZQeu","executionInfo":{"status":"ok","timestamp":1677579149130,"user_tz":-60,"elapsed":4720,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"2db10b6f-15c9-4b2d-f8a4-31c3f5defcc4"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["import os, sys, glob\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","import numpy as np\n","\n","import mne\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","source":["def eval_model_cl(dl, model, device=torch.device('cpu'), verbose=True):\n","    \"\"\" \n","    This function calculates the loss on data, setting backward gradients and batchnorm\n","    off. This function is written for contrasting learning where the model takes in two\n","    inputs.\n","\n","    Args:\n","\n","    Returns:\n","      loss_test: Mean loss of all test samples (scalar)\n","\n","    \"\"\"\n","    losses, losses_X1, losses_X2 = [], [], []\n","    model.to(device)  # inplace for model\n","    # Set the model in evaluation mode\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for idx_batch, (X1b, X2b) in enumerate(dl):\n","\n","            X1b = X1b.to(device)\n","            X2b = X2b.to(device)\n","\n","            X1b_features, X2b_features, logit_sc = model(X1b, X2b)\n","\n","            # Normalize features\n","            X1b_f_n = X1b_features / X1b_features.norm(dim=1, keepdim=True)\n","            X2b_f_n = X2b_features / X2b_features.norm(dim=1, keepdim=True)\n","\n","            logits_per_X1 = logit_sc * X1b_f_n @ X2b_f_n.t()\n","            logits_per_X2 = logits_per_X1.t()\n","\n","            # Number of labels equals to the 1st dimension of X1b\n","            labels = torch.arange(X1b.shape[0], device=device)\n","\n","            # Batch Loss \n","            loss_X1 = F.cross_entropy(logits_per_X1, labels)\n","            loss_X2 = F.cross_entropy(logits_per_X2, labels)\n","            loss_batch   = (loss_X1 + loss_X2) / 2\n","            losses.append(loss_batch.item())\n","            losses_X1.append(loss_X1.item())\n","            losses_X2.append(loss_X2.item())\n","\n","        # Epoch loss (mean of batch losses)\n","        loss  = sum(losses) / len(losses)\n","        loss_X1 = sum(losses_X1) / len(losses_X1)\n","        loss_X2 = sum(losses_X2) / len(losses_X2)\n","\n","        if verbose:\n","          print(f\"====> Validation loss: {loss:.4f},  X1 loss: {loss_X1:.4f}   X2 loss: {loss_X2:.4f}\")\n","\n","        return loss, loss_X1, loss_X2\n"],"metadata":{"id":"GYgZe_3OQniG","executionInfo":{"status":"ok","timestamp":1677579149131,"user_tz":-60,"elapsed":11,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"EaAsZ-SLZQey","executionInfo":{"status":"ok","timestamp":1677579149131,"user_tz":-60,"elapsed":10,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}}},"outputs":[],"source":["def unfold_raw(raw, window_size=None, stride=None):\n","    \"\"\"\n","    This function unfolds raw MNE object into a list of raw objects\n","    Args:\n","        raw: a raw MNE object cropped by rejecting bad segments.\n","    Returns:\n","        raw_unfolded: a raw MNE object unfolded by applying a sliding window.\n","    \"\"\"\n","    if window_size is None:\n","        window_size = int(5 * raw.info['sfreq'])\n","    if stride is None:\n","        stride = window_size\n","    nchans = len(raw.ch_names)\n","    sig = torch.tensor(raw.get_data(), dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n","    sig_unf = F.unfold(sig, (nchans, window_size), stride=stride , padding=0)\n","    sig_unf = sig_unf.permute(0, 2, 1).reshape(-1, sig_unf.shape[-1], nchans, window_size)\n","    return sig_unf"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"1cf9EoF3ZQey","executionInfo":{"status":"ok","timestamp":1677579149131,"user_tz":-60,"elapsed":9,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}}},"outputs":[],"source":["def rm_repeated_annotations(raw):\n","    \"\"\"This functions taskes in raw MNE obejct and removes repeated annotations\"\"\"\n","    annots = raw.annotations.copy()\n","    annots_drop = []\n","    for k in annots:\n","        annots_drop.extend([k for kk in annots if (k['onset'] > kk['onset']) and (k['onset']+k['duration'] < kk['onset']+kk['duration']) ])\n","\n","    annots_updated = [i for i in annots if i not in annots_drop]\n","    onsets = [i['onset'] for i in annots_updated]\n","    durations = [i['duration'] for i in annots_updated]\n","    descriptions = [i['description'] for i in annots_updated]\n","    print('Initial num of annots: %d  Num of removed annots: %d  Num of retained annots:  %d' % (len(annots), len(annots_drop), len(annots_updated)))\n","    print(f' New annots: {annots_updated}')\n","    raw.set_annotations(mne.Annotations(onsets, durations, descriptions) ) \n","    return raw"]},{"cell_type":"markdown","metadata":{"id":"q4sGJAdFZQez"},"source":["## Read Data"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KdQFfHcJZQe0","executionInfo":{"status":"ok","timestamp":1677579149132,"user_tz":-60,"elapsed":10,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"afad47cb-9bd4-474b-f550-aa350cb6ee74"},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------\n","window_size: 640  stride_size_test: 320\n","data_path: ../outputs/rochester_data/natural_speech\n"]}],"source":["subj_ids = [1, 2, 3, 4, 5]\n","fs = 128\n","window_size = int(5 * fs)\n","stride_size_train, stride_size_val, stride_size_test = int(2.5 * fs), int(2.5 * fs), int(2.5 * fs)\n","n_channs = 129 # 128 for eeg, 1 for env\n","batch_size = int(64)\n","print('-------------------------------------')\n","print(f'window_size: {window_size}  stride_size_test: {stride_size_test}')\n","\n","dataset_name = ['rochester_data', 'natural_speech']\n","outputs_path = f'../outputs/'\n","data_path = os.path.join(outputs_path, dataset_name[0], dataset_name[1])\n","print(f'data_path: {data_path}')"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fIhJi5IxZQe1","executionInfo":{"status":"ok","timestamp":1677579209647,"user_tz":-60,"elapsed":60522,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"879e9f1c-e415-4289-fc70-d77c5313fc54"},"outputs":[{"output_type":"stream","name":"stdout","text":["Opening raw data file ../outputs/rochester_data/natural_speech/subj_1/after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 42  Num of removed annots: 19  Num of retained annots:  23\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.784332), ('duration', 2.0346832275390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.548248), ('duration', 1.66888427734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.407166), ('duration', 1.8746337890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 719.272339), ('duration', 0.75445556640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 899.217163), ('duration', 1.8060302734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.1604), ('duration', 3.612060546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1267.022461), ('duration', 1.8746337890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.1521), ('duration', 1.92041015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1613.958984), ('duration', 5.4410400390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.862427), ('duration', 2.42333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1977.397705), ('duration', 8.6873779296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2058.906494), ('duration', 3.932373046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2162.312988), ('duration', 2.354736328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.353516), ('duration', 1.943359375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.958984), ('duration', 4.137939453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2707.411133), ('duration', 3.65771484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.364502), ('duration', 1.4404296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3080.712646), ('duration', 5.760986328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3259.638184), ('duration', 8.61865234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3439.719971), ('duration', 3.97802734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3486.391846), ('duration', 5.120849609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3610.283691), ('duration', 5.668701171875), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 20  N val: 1  N test: 1\n","Opening raw data file ../outputs/rochester_data/natural_speech/subj_2/after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 65  Num of removed annots: 19  Num of retained annots:  46\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 177.009033), ('duration', 4.4808807373046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 233.547455), ('duration', 3.1549072265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 256.64325), ('duration', 4.549468994140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.877777), ('duration', 1.554595947265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 419.798157), ('duration', 4.18365478515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 521.238403), ('duration', 3.72650146484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 536.986145), ('duration', 8.36737060546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 591.58136), ('duration', 12.36810302734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 684.541626), ('duration', 37.07598876953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 731.651611), ('duration', 0.9830322265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 760.453308), ('duration', 5.80682373046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 895.355164), ('duration', 14.21990966796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 955.299988), ('duration', 3.9779052734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1010.325623), ('duration', 5.5782470703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1047.856567), ('duration', 3.0177001953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.390625), ('duration', 4.892333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1266.681152), ('duration', 8.984619140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1304.783569), ('duration', 3.2464599609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1385.044556), ('duration', 3.749267578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.464355), ('duration', 2.171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1613.947266), ('duration', 13.3740234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1636.351562), ('duration', 1.600341796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1650.508545), ('duration', 8.5731201171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1683.526245), ('duration', 6.5384521484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1726.90979), ('duration', 6.2640380859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1807.079224), ('duration', 13.076904296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.602295), ('duration', 1.348876953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1991.061035), ('duration', 2.126220703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2049.411133), ('duration', 4.503662109375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2136.050537), ('duration', 4.732421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2162.68042), ('duration', 11.659423828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2234.577881), ('duration', 7.315673828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.926758), ('duration', 14.63134765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.343262), ('duration', 9.076171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2546.635498), ('duration', 3.177734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2707.869873), ('duration', 15.68310546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.512939), ('duration', 8.481689453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3081.322021), ('duration', 2.468994140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3087.928955), ('duration', 6.835693359375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3116.119141), ('duration', 3.886474609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3257.709961), ('duration', 6.652587890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3280.65918), ('duration', 5.71533203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3440.005615), ('duration', 0.914306640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3452.968018), ('duration', 7.29296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3604.318115), ('duration', 3.817138671875), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 62  N val: 2  N test: 2\n","Opening raw data file ../outputs/rochester_data/natural_speech/subj_3/after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 47  Num of removed annots: 19  Num of retained annots:  28\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 43.135948), ('duration', 4.115089416503906), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 121.796593), ('duration', 5.6010894775390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.748077), ('duration', 1.760345458984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 275.317291), ('duration', 2.994842529296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.205322), ('duration', 2.126129150390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.65863), ('duration', 1.89752197265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.599854), ('duration', 2.12615966796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 898.988525), ('duration', 1.6231689453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1033.410278), ('duration', 0.9830322265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.293579), ('duration', 1.7603759765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1266.313721), ('duration', 3.56640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1307.502563), ('duration', 0.86865234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.074097), ('duration', 2.4461669921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1583.876953), ('duration', 0.3658447265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1613.867554), ('duration', 1.9432373046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.839478), ('duration', 1.9661865234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.358643), ('duration', 2.1947021484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2162.061523), ('duration', 2.14892578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.888916), ('duration', 0.8916015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.831299), ('duration', 2.92626953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2707.159668), ('duration', 2.01171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2790.049561), ('duration', 9.510498046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.086426), ('duration', 2.400390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3081.215576), ('duration', 1.874755859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.645996), ('duration', 2.14892578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3439.272217), ('duration', 2.42333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3525.888916), ('duration', 1.87451171875), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 87  N val: 3  N test: 3\n","Opening raw data file ../outputs/rochester_data/natural_speech/subj_4/after_ica_raw.fif...\n","    Range : 0 ... 464394 =      0.000 ...  3628.078 secs\n","Ready.\n","Reading 0 ... 464394  =      0.000 ...  3628.078 secs...\n","Initial num of annots: 41  Num of removed annots: 19  Num of retained annots:  22\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.316391), ('duration', 2.4461822509765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.619507), ('duration', 1.074493408203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.230835), ('duration', 1.851806640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.968323), ('duration', 4.89239501953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 898.236755), ('duration', 2.56048583984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1100.372314), ('duration', 2.5833740234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1265.114136), ('duration', 4.6181640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1437.281372), ('duration', 1.874755859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1613.327759), ('duration', 1.783203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.417358), ('duration', 1.2344970703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1981.789795), ('duration', 2.720458984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2161.278076), ('duration', 4.8466796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2344.500732), ('duration', 1.80615234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.130127), ('duration', 2.194580078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2706.033447), ('duration', 2.743408203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2876.088623), ('duration', 1.6689453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2892.718506), ('duration', 0.822998046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3079.622803), ('duration', 2.217529296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.026855), ('duration', 6.172607421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3438.603516), ('duration', 0.822998046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3535.585693), ('duration', 2.720458984375), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 106  N val: 4  N test: 4\n","Opening raw data file ../outputs/rochester_data/natural_speech/subj_5/after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 41  Num of removed annots: 19  Num of retained annots:  22\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 177.408173), ('duration', 0.270477294921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 358.121857), ('duration', 0.270477294921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.280273), ('duration', 1.9835205078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.369812), ('duration', 2.3441162109375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 899.271362), ('duration', 1.3974609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.870972), ('duration', 0.4508056640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1267.272949), ('duration', 1.4425048828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.279663), ('duration', 1.126953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1614.049805), ('duration', 1.8707275390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1701.652954), ('duration', 0.96923828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.822144), ('duration', 2.434326171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.066284), ('duration', 2.186279296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2161.976074), ('duration', 1.983642578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.484619), ('duration', 1.870849609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2524.411133), ('duration', 0.541015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2707.077393), ('duration', 2.569580078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.742432), ('duration', 0.8564453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3081.496094), ('duration', 0.653564453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3229.895752), ('duration', 1.84814453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.698975), ('duration', 2.727294921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3439.993408), ('duration', 0.901611328125), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 125  N val: 5  N test: 5\n","Shape Trian: torch.Size([5925, 1, 129, 640])  Shape Val: torch.Size([381, 1, 129, 640])  Shape Test: torch.Size([363, 1, 129, 640])\n","-------------------------------------\n","Shape EEG Train: torch.Size([5925, 1, 128, 640])  Val: torch.Size([381, 1, 128, 640])  Test: torch.Size([363, 1, 128, 640])\n","Mean: 7.502971854922436e-11  Std: 5.03486990055535e-06\n","Shape Env Train: torch.Size([5925, 1, 1, 640])  Val: torch.Size([381, 1, 1, 640])  Test: torch.Size([363, 1, 1, 640])\n","Mean Env: 2.3605871200561523  Std Env: 2.5989928245544434\n"]}],"source":["raws_train_windowed, raws_val_windowed, raws_test_windowed = [], [], []\n","\n","for subj_id in subj_ids:\n","    subj_path = os.path.join(data_path, f'subj_{subj_id}')\n","\n","    # load subject raw MNE object\n","    raw = mne.io.read_raw(os.path.join(subj_path, 'after_ica_raw.fif'), preload=True)\n","    # drop M1 and M2 channels\n","    raw.drop_channels(['M1', 'M2'])\n","    assert raw.info['nchan'] == n_channs\n","\n","    raw = rm_repeated_annotations(raw)\n","    annots = raw.annotations.copy()\n","    raw_split = [raw.copy().crop(t1, t2) for t1, t2 in zip(annots.onset[:-1]+annots.duration[:-1], annots.onset[1:])]\n","\n","    # Pick the split with the longest duration for validation, supposedly less noisy\n","    ix_val = np.argmax([i.get_data().shape[1] for i in raw_split])\n","    raw_val = [raw_split.pop(ix_val)] # create a list to make it iterable. later may be used for multiple splits\n","\n","    # Pick the next split with the longest duration for testing, supposedly less noisy\n","    ix_test = np.argmax([i.get_data().shape[1] for i in raw_split])\n","    raw_test = [raw_split.pop(ix_test)]\n","    \n","    # creat list of unfolded tensor raw objects\n","    fs = raw.info['sfreq']\n","    raws_train_windowed.extend([unfold_raw(i, window_size=window_size, stride=stride_size_train) for i in raw_split if i.get_data().shape[1] > window_size])\n","    raws_val_windowed.extend([unfold_raw(i, window_size=window_size, stride=stride_size_val) for i in raw_val if i.get_data().shape[1] > window_size])\n","    raws_test_windowed.extend([unfold_raw(i, window_size=window_size, stride=stride_size_test) for i in raw_test if i.get_data().shape[1] > window_size])\n","    print(\"-------------------------------------\")\n","    print('N train: %d  N val: %d  N test: %d' % (len(raws_train_windowed), len(raws_val_windowed), len(raws_test_windowed)))\n","\n","# concatenate all in second dimension\n","sigs_train = torch.cat(raws_train_windowed, dim=1).permute(1, 0, 2, 3)\n","sigs_val = torch.cat(raws_val_windowed, dim=1).permute(1, 0, 2, 3)\n","sigs_test = torch.cat(raws_test_windowed, dim=1).permute(1, 0, 2, 3)\n","print(f\"Shape Trian: {sigs_train.shape}  Shape Val: {sigs_val.shape}  Shape Test: {sigs_test.shape}\")\n","\n","eegs_train = sigs_train[:, :, :-1, :]\n","eegs_val = sigs_val[:, :, :-1, :]\n","eegs_test = sigs_test[:, :, :-1, :]\n","print(\"-------------------------------------\")\n","print(f\"Shape EEG Train: {eegs_train.shape}  Val: {eegs_val.shape}  Test: {eegs_test.shape}\")\n","\n","# To avoid information leakage, we estimate the mean and std from the training set only.\n","mean_eeg_train =  eegs_train.mean()\n","std_eeg_train = eegs_train.std()\n","print(f\"Mean: {mean_eeg_train}  Std: {std_eeg_train}\")\n","\n","envs_train = sigs_train[:, :, [-1], :]\n","envs_val = sigs_val[:, :, [-1], :]\n","envs_test = sigs_test[:, :, [-1], :]\n","print(f\"Shape Env Train: {envs_train.shape}  Val: {envs_val.shape}  Test: {envs_test.shape}\")\n","\n","# Estimate mean and std of the Envelope data set\n","mean_env_train =  envs_train.mean()\n","std_env_train = envs_train.std()\n","print(f\"Mean Env: {mean_env_train}  Std Env: {std_env_train}\")\n","\n","# Normalize the data\n","eegs_train = (eegs_train - mean_eeg_train) / std_eeg_train\n","eegs_val = (eegs_val - mean_eeg_train) / std_eeg_train\n","eegs_test = (eegs_test - mean_eeg_train) / std_eeg_train\n","\n","envs_train = (envs_train - mean_env_train) / std_env_train\n","envs_val = (envs_val - mean_env_train) / std_env_train\n","envs_test = (envs_test - mean_env_train) / std_env_train\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":513},"id":"qLIhndDfZQe2","executionInfo":{"status":"ok","timestamp":1677579218061,"user_tz":-60,"elapsed":8436,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"ac1b0df0-cf28-4229-9106-f5a612474f16"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 720x504 with 6 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["fig, ax = plt.subplots(2, 3, figsize=(10, 7), sharex='row')\n","ax[0, 0].hist(eegs_train.flatten().numpy(), bins=100);\n","ax[0, 0].set_title('Train EEG')\n","ax[0, 1].hist(eegs_val.flatten().numpy(), bins=100);\n","ax[0, 1].set_title('Val EEG')\n","ax[0, 2].hist(eegs_test.flatten().numpy(), bins=100);\n","ax[0, 2].set_title('Test EEG')\n","ax[1, 0].hist(envs_train.flatten().numpy(), bins=100);\n","ax[1, 0].set_title('Train Env')\n","ax[1, 1].hist(envs_val.flatten().numpy(), bins=100);\n","ax[1, 1].set_title('Val Env')\n","ax[1, 2].hist(envs_test.flatten().numpy(), bins=100);\n","ax[1, 2].set_title('Test Env')\n","plt.tight_layout()\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"tesRTfOdZQe2"},"source":["### Pytorch dataloader"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"HviGOmH1ZQe2","executionInfo":{"status":"ok","timestamp":1677579218062,"user_tz":-60,"elapsed":15,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}}},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, eeg, env):\n","        self.eeg = eeg\n","        self.env = env\n","    \n","    def __getitem__(self, index):\n","        return self.eeg[index], self.env[index]\n","    \n","    def __len__(self):\n","        return len(self.eeg)\n","    \n","dataset_train = MyDataset(eegs_train, envs_train)\n","dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)\n","\n","dl_val = DataLoader(MyDataset(eegs_val, envs_val), batch_size=batch_size, shuffle=True, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"XMjI2NeFZQe3"},"source":["## Model"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"57o2oV6VZQe3","executionInfo":{"status":"ok","timestamp":1677579218063,"user_tz":-60,"elapsed":15,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}}},"outputs":[],"source":["class Conv2d(nn.Conv2d):\n","    def __init__(self, in_channels, out_channels, kernel_size, **kargs):\n","        super().__init__(in_channels, out_channels, kernel_size, **kargs)\n","\n","    def __call__(self, inp):\n","        self.out = super().__call__(inp)\n","\n","        if self.out.requires_grad:\n","            self.out.retain_grad()\n","\n","        return self.out\n","    \n","    # -----------------------------------------------------------------------------------------------\n","class Flatten:\n","    \n","  def __call__(self, x):\n","    self.out = x.view(x.shape[0], -1)\n","    return self.out\n","  \n","  def parameters(self):\n","    return []\n","  \n","  # -----------------------------------------------------------------------------------------------\n","class Linear(nn.Linear):\n","    def __init__(self, x, y, **kargs):\n","        super().__init__(x, y, **kargs)\n","\n","    def __call__(self, inp):\n","        self.out = super().__call__(inp)\n","        return self.out\n","  # -----------------------------------------------------------------------------------------------\n","   \n","class ELU(nn.ELU):\n","    def __init__(self, alpha=1.0, inplace=False):\n","        super().__init__(alpha=1.0, inplace=False)\n","\n","    def __call__(self, inp):\n","        self.out = super().__call__(inp)\n","        if self.out.requires_grad:\n","            self.out.retain_grad()\n","        return self.out\n","\n","  # -----------------------------------------------------------------------------------------------\n","class Sequential:\n","  \n","    def __init__(self, layers):\n","        self.layers = layers\n","\n","    def __call__(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        self.out = x\n","        return self.out\n","\n","    def parameters(self):\n","        # get parameters of all layers and stretch them out into one list\n","        return [p for layer in self.layers for p in layer.parameters()]\n","\n","    def named_parameters(self):\n","        # get parameters of all layers and stretch them out into one list\n","        return ((n, p) for layer in self.layers for n, p in layer.named_parameters())"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"IffWKmD6ZQe3","executionInfo":{"status":"ok","timestamp":1677579218063,"user_tz":-60,"elapsed":14,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}}},"outputs":[],"source":["# My implementation of the shallow convnet\n","\n","fs = 64 # sampling rate\n","T = 5 * fs # number of time points in each trial\n","C = 64 # number of EEG channels\n","F1 = 8 # number of channels (depth) in the first conv layer\n","D = 2 # number of spatial filters in the second conv layer\n","F2 = D * F1 # number of channels (depth) in the pont-wise conv layer\n","num_classes = 4 # number of classes\n","\n","shallow_covnet = Sequential([\n","    Conv2d(1, 40, (1, int(fs//2)), padding='same', bias=True),\n","    Conv2d(40, 40, (C, 1), padding=(0, 0), bias=False), nn.BatchNorm2d(40, affine=True), \n","    nn.AvgPool2d((1, 75), (1, 15)), nn.Dropout(0.5),\n","    Conv2d(40, 4, kernel_size=(1, 30), padding='same', stride=(1, 1), bias=True),\n","    nn.Flatten(1, -1), # Flatten start_dim=1, end_dim=-1\n","    Linear(62*4, 4, bias=True),\n","])\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sQjEWHA4ZQe4","executionInfo":{"status":"ok","timestamp":1677579218069,"user_tz":-60,"elapsed":20,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"6ea491a7-eb12-45cf-a2db-38fc516c6683"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 32])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)\n","  return F.conv2d(input, weight, bias, self.stride,\n"]}],"source":["\n","class EEGEncoder(nn.Module):\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            C = 128, # number of EEG channels\n","            F1 = 8, # 8 or 4 number of channels (depth) in the first conv layer\n","            D = 2, # number of spatial filters in the second conv layer\n","            F2 = None # number of channels (depth) in the pont-wise conv layer\n","        ):\n","        super(EEGEncoder, self).__init__()\n","\n","        if F2 is None:\n","            F2 = D * F1\n","\n","        self.eeg_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs/2)), padding='same', bias=True, groups=1),\n","            nn.BatchNorm2d(F1, affine=True),\n","            Conv2d(F1, out_channels=D*F1, kernel_size=(C, 1), padding=(0, 0), bias=False, groups=F1),\n","            nn.BatchNorm2d(D*F1, affine=True), ELU(), nn.AvgPool2d(1, 4), nn.Dropout(0.25),\n","                    \n","            Conv2d(F2, F2, (1, int(fs/(2*4))), padding='same', bias=False, groups=D*F1),\n","            Conv2d(D*F1, F2, kernel_size=(1, 1), padding=(0, 0), groups=1, bias=False),\n","            nn.BatchNorm2d(F2, affine=True), ELU(), nn.AvgPool2d(1, 8), nn.Dropout(0.25),\n","\n","            nn.Flatten(),\n","            nn.Linear(F2*int((T*fs)//(8*4)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.eeg_encoder(x)\n","        return x\n","\n","\n","def normalize_weights_eegnet(eeg_encoder):\n","\n","    for ix, (name, param) in enumerate(eeg_encoder.named_parameters()):\n","        if  name == 'weight' and param.ndim==4 and ix==1: # normalize conv weights to max norm 1\n","            param.data = torch.renorm(param.data, 2, 0, maxnorm=1)\n","            #param.data /=  eps + param.data.norm(2, dim=0, keepdim=True)\n","        elif name == 'weight' and param.ndim==2: # normalize fc weights to max norm 0.25\n","            param.data = torch.renorm(param.data, 2, 0, maxnorm=0.25)\n","            #param.data /=  eps + param.data.norm(2, dim=0, keepdim=True)\n","\n","eeg_encoder = EEGEncoder()\n","eeg_encoder.eeg_encoder\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(eeg_encoder(eegs_train[:32, :, :, :]).shape)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SfO-UwlzZQe4","executionInfo":{"status":"ok","timestamp":1677579218481,"user_tz":-60,"elapsed":427,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"95c9e961-3bb3-4905-dd6e-afc3d6f873ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 32])\n"]}],"source":["class EnvEncoder(nn.Module):\n","\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            F1 = 4\n","        ):\n","        super(EnvEncoder, self).__init__()\n","\n","        self.env_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs//2)), padding='same', bias=True),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1, (1, int(fs//4)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1, (1, int(fs//8)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 4), nn.Dropout(0.5),\n","            nn.Flatten(),\n","            nn.Linear(F1*int((T*fs)//(2*8)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.env_encoder(x)\n","        return x\n","\n","env_encoder = EnvEncoder()\n","env_encoder.env_encoder\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(env_encoder(envs_train[:32, :, :, :]).shape)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bp8HVS-aZQe4","executionInfo":{"status":"ok","timestamp":1677579222903,"user_tz":-60,"elapsed":4424,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"f07a275d-4f33-4289-9521-b9e9fc8ccc01"},"outputs":[{"output_type":"stream","name":"stdout","text":["logit_scale torch.Size([])\n","eeg_encoder.0.weight torch.Size([8, 1, 1, 64])\n","eeg_encoder.0.bias torch.Size([8])\n","eeg_encoder.1.weight torch.Size([8])\n","eeg_encoder.1.bias torch.Size([8])\n","eeg_encoder.2.weight torch.Size([16, 1, 128, 1])\n","eeg_encoder.3.weight torch.Size([16])\n","eeg_encoder.3.bias torch.Size([16])\n","eeg_encoder.7.weight torch.Size([16, 1, 1, 16])\n","eeg_encoder.8.weight torch.Size([16, 16, 1, 1])\n","eeg_encoder.9.weight torch.Size([16])\n","eeg_encoder.9.bias torch.Size([16])\n","eeg_encoder.14.weight torch.Size([32, 320])\n","eeg_encoder.14.bias torch.Size([32])\n","env_encoder.0.weight torch.Size([4, 1, 1, 64])\n","env_encoder.0.bias torch.Size([4])\n","env_encoder.1.weight torch.Size([4])\n","env_encoder.1.bias torch.Size([4])\n","env_encoder.5.weight torch.Size([4, 4, 1, 32])\n","env_encoder.6.weight torch.Size([4])\n","env_encoder.6.bias torch.Size([4])\n","env_encoder.10.weight torch.Size([4, 4, 1, 16])\n","env_encoder.11.weight torch.Size([4])\n","env_encoder.11.bias torch.Size([4])\n","env_encoder.16.weight torch.Size([32, 160])\n","env_encoder.16.bias torch.Size([32])\n"]}],"source":["class CES(nn.Module):\n","    def __init__(self, \n","                 eeg_encoder= eeg_encoder.eeg_encoder,\n","                 env_encoder = env_encoder.env_encoder): \n","        super().__init__()\n","\n","        self.eeg_encoder = eeg_encoder\n","        self.env_encoder = env_encoder\n","        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n","\n","    def encode_eeg(self, x):\n","        return self.eeg_encoder(x)\n","    \n","    def encode_env(self, x):\n","        return self.env_encoder(x)\n","    \n","    def forward(self, eeg, env):\n","        eeg_features = self.encode_eeg(eeg)\n","        env_features = self.encode_env(env)\n","        return eeg_features, env_features, self.logit_scale.exp()\n","  \n","\n","model = CES()\n","model.to(device)\n","for n,p in model.named_parameters():\n","    print(n, p.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"54QNcdAwZQe5"},"source":["## Model Setup"]},{"cell_type":"markdown","metadata":{"id":"RS9RoOi6ZQe5"},"source":["## Optimization"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H_n7xsWjZQe5","executionInfo":{"status":"ok","timestamp":1677579222904,"user_tz":-60,"elapsed":15,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"235e59da-6cda-4e8e-cff5-d092dec591ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["19637\n","Sequential(\n","  (0): Conv2d(1, 8, kernel_size=(1, 64), stride=(1, 1), padding=same)\n","  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (2): Conv2d(8, 16, kernel_size=(128, 1), stride=(1, 1), groups=8, bias=False)\n","  (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (4): ELU(alpha=1.0)\n","  (5): AvgPool2d(kernel_size=1, stride=4, padding=0)\n","  (6): Dropout(p=0.25, inplace=False)\n","  (7): Conv2d(16, 16, kernel_size=(1, 16), stride=(1, 1), padding=same, groups=16, bias=False)\n","  (8): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","  (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (10): ELU(alpha=1.0)\n","  (11): AvgPool2d(kernel_size=1, stride=8, padding=0)\n","  (12): Dropout(p=0.25, inplace=False)\n","  (13): Flatten(start_dim=1, end_dim=-1)\n","  (14): Linear(in_features=320, out_features=32, bias=True)\n",")\n","Sequential(\n","  (0): Conv2d(1, 4, kernel_size=(1, 64), stride=(1, 1), padding=same)\n","  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (2): ELU(alpha=1.0)\n","  (3): AvgPool2d(kernel_size=1, stride=2, padding=0)\n","  (4): Dropout(p=0.5, inplace=False)\n","  (5): Conv2d(4, 4, kernel_size=(1, 32), stride=(1, 1), padding=same, bias=False)\n","  (6): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (7): ELU(alpha=1.0)\n","  (8): AvgPool2d(kernel_size=1, stride=2, padding=0)\n","  (9): Dropout(p=0.5, inplace=False)\n","  (10): Conv2d(4, 4, kernel_size=(1, 16), stride=(1, 1), padding=same, bias=False)\n","  (11): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (12): ELU(alpha=1.0)\n","  (13): AvgPool2d(kernel_size=1, stride=4, padding=0)\n","  (14): Dropout(p=0.5, inplace=False)\n","  (15): Flatten(start_dim=1, end_dim=-1)\n","  (16): Linear(in_features=160, out_features=32, bias=True)\n",")\n"]}],"source":["\n","with torch.no_grad():\n","    for ix, layer in enumerate(model.eeg_encoder + model.env_encoder):\n","        if isinstance(layer, nn.Conv2d):\n","            layer.weight *= 0.01\n","            if layer.bias is not None:\n","                layer.bias *= 0\n","\n","        if isinstance(layer, nn.Linear):\n","          layer.weight *= 0.01\n","          if layer.bias is not None:\n","            layer.bias *=0\n","\n","\n","print(sum(p.nelement() for p in model.parameters())) # number of parameters in total\n","for p in model.parameters():\n","    p.requires_grad = True\n","\n","print(model.eeg_encoder)\n","print(model.env_encoder)\n","\n","lr = 0.001\n","optimizer_adamw = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n","optimizer_nadam = optim.NAdam(model.parameters(), lr=lr)\n","optimizer = optimizer_nadam\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5vA3bwBkZQe6","executionInfo":{"status":"ok","timestamp":1677579222904,"user_tz":-60,"elapsed":10,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"5cc07d91-03b8-4401-d31f-9b10c6940919"},"outputs":[{"output_type":"stream","name":"stdout","text":["---  Data specs:   ---\n"," Train Set: shape EEG: torch.Size([5925, 1, 128, 640])   Env: torch.Size([5925, 1, 1, 640])\n"," Val Set: shape EEG: torch.Size([381, 1, 128, 640])   Env: torch.Size([381, 1, 1, 640])\n","\n","---  Model specs:  ---\n"," Number of parameters: 19637\n","\n","---  Training specs:  ---\n"," Train set: Batch size: 64\n"," Train set: number of batches: 92\n"," Val set: Batch size: 64\n"," Val set: number of batches: 5\n","Chance level loss for Train set: -4.1589\n","Learning rate: 0.001\n"]}],"source":["print(\"---  Data specs:   ---\")\n","print(f\" Train Set: shape EEG: {dataloader.dataset.eeg.shape}   Env: {dataloader.dataset.env.shape}\")\n","print(f\" Val Set: shape EEG: {eegs_val.shape}   Env: {envs_val.shape}\")\n","print()\n","print(\"---  Model specs:  ---\")\n","print(f\" Number of parameters: {sum(p.nelement() for p in model.parameters())}\") # number of parameters in total\n","print()\n","print(\"---  Training specs:  ---\")\n","print(f\" Train set: Batch size: {dataloader.batch_size}\")\n","print(f\" Train set: number of batches: {len(dataloader)}\")\n","print(f\" Val set: Batch size: {dl_val.batch_size}\")\n","print(f\" Val set: number of batches: {len(dl_val)}\")\n","print(f\"Chance level loss for Train set: {torch.tensor(1/batch_size).log():.4f}\")\n","print(f\"Learning rate: {lr}\")\n"]},{"cell_type":"markdown","metadata":{"id":"DaEsSJb1ZQe6"},"source":["# gradient not getting updateing\n","maybe because the chain removes the referencing ..."]},{"cell_type":"markdown","metadata":{"id":"KQOCRJTpZQe6"},"source":["links to check for implementing the loss function:  \n","\n","https://www.kaggle.com/code/moeinshariatnia/openai-clip-simple-implementation/notebook  \n","\n","\n","https://github.com/openai/CLIP/issues/83\n","\n","\n","https://github.com/mlfoundations/open_clip\n","\n","https://github.com/mlfoundations/open_clip/blob/main/src/open_clip/loss.py"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nidtXAIgZQe6","executionInfo":{"status":"ok","timestamp":1677579222905,"user_tz":-60,"elapsed":8,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"60a4b10e-44e1-4f83-b7c1-ac04d0f7bd08"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1941.504048"]},"metadata":{},"execution_count":20}],"source":["sys.getsizeof(eegs_train.storage())/1e6"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4iJiosNZQe7","executionInfo":{"status":"ok","timestamp":1677579741668,"user_tz":-60,"elapsed":518769,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"7abb013b-c3de-43f3-980f-03d01f52c0e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Batch:       0: 6.8095  6.7996  6.8193   0.0001  0.0056\n","Batch:      10: 5.4698  6.5907  4.3488   0.0024  0.0371\n","Batch:      20: 4.5465  4.7994  4.2937   0.0035  0.0547\n","Batch:      30: 4.3046  4.3976  4.2117   0.0043  0.0614\n","Batch:      40: 4.2834  4.3463  4.2206   0.0049  0.0683\n","Batch:      50: 4.2388  4.2667  4.2109   0.0053  0.0764\n","Batch:      60: 4.1934  4.2299  4.1570   0.0056  0.0773\n","Batch:      70: 4.2058  4.2250  4.1867   0.0059  0.0742\n","Batch:      80: 4.1652  4.1726  4.1578   0.0061  0.0778\n","Batch:      90: 4.1554  4.1614  4.1494   0.0062  0.0809\n","====> Validation loss: 4.1583,  X1 loss: 4.1617   X2 loss: 4.1550\n","Batch:       0: 4.2014  4.2054  4.1975   0.0063  0.0787\n","Batch:      10: 4.1524  4.1614  4.1434   0.0067  0.0860\n","Batch:      20: 4.1161  4.1227  4.1095   0.0070  0.0929\n","Batch:      30: 4.1405  4.1521  4.1290   0.0073  0.0899\n","Batch:      40: 4.1648  4.1744  4.1552   0.0078  0.0874\n","Batch:      50: 4.1591  4.1620  4.1563   0.0082  0.0951\n","Batch:      60: 4.1504  4.1556  4.1452   0.0086  0.0974\n","Batch:      70: 4.0501  4.0610  4.0392   0.0093  0.0983\n","Batch:      80: 4.0653  4.0657  4.0648   0.0097  0.0986\n","Batch:      90: 4.1391  4.1401  4.1382   0.0106  0.1046\n","====> Validation loss: 4.0493,  X1 loss: 4.0480   X2 loss: 4.0506\n","Batch:       0: 4.0451  4.0460  4.0442   0.0107  0.1042\n","Batch:      10: 4.1779  4.1792  4.1766   0.0112  0.1056\n","Batch:      20: 4.1743  4.1797  4.1689   0.0120  0.1017\n","Batch:      30: 4.1256  4.1236  4.1277   0.0132  0.1068\n","Batch:      40: 4.1811  4.1756  4.1866   0.0146  0.1151\n","Batch:      50: 4.1647  4.1639  4.1656   0.0151  0.1138\n","Batch:      60: 4.0955  4.0940  4.0971   0.0166  0.1223\n","Batch:      70: 4.0110  4.0054  4.0166   0.0178  0.1231\n","Batch:      80: 4.0017  4.0006  4.0028   0.0191  0.1266\n","Batch:      90: 4.0920  4.0912  4.0927   0.0192  0.1300\n","====> Validation loss: 3.9685,  X1 loss: 3.9663   X2 loss: 3.9707\n","Batch:       0: 4.0047  4.0028  4.0066   0.0208  0.1303\n","Batch:      10: 4.0151  4.0118  4.0183   0.0215  0.1314\n","Batch:      20: 3.9225  3.9153  3.9296   0.0228  0.1394\n","Batch:      30: 3.9040  3.8970  3.9110   0.0255  0.1326\n","Batch:      40: 3.7922  3.7946  3.7898   0.0256  0.1435\n","Batch:      50: 3.7185  3.7142  3.7228   0.0296  0.1393\n","Batch:      60: 3.8177  3.8130  3.8225   0.0303  0.1453\n","Batch:      70: 4.0002  3.9970  4.0034   0.0316  0.1471\n","Batch:      80: 3.9423  3.9413  3.9434   0.0326  0.1549\n","Batch:      90: 3.9508  3.9509  3.9507   0.0333  0.1534\n","====> Validation loss: 3.8855,  X1 loss: 3.8784   X2 loss: 3.8925\n","Batch:       0: 3.8027  3.7975  3.8079   0.0345  0.1543\n","Batch:      10: 3.8852  3.8818  3.8886   0.0362  0.1616\n","Batch:      20: 3.8087  3.8121  3.8052   0.0397  0.1620\n","Batch:      30: 3.9347  3.9354  3.9340   0.0388  0.1651\n","Batch:      40: 3.8855  3.8831  3.8879   0.0429  0.1715\n","Batch:      50: 3.7747  3.7708  3.7785   0.0453  0.1769\n","Batch:      60: 3.9484  3.9436  3.9531   0.0445  0.1718\n","Batch:      70: 3.7232  3.7268  3.7197   0.0446  0.1742\n","Batch:      80: 4.0094  4.0109  4.0079   0.0486  0.1699\n","Batch:      90: 3.6174  3.6168  3.6180   0.0508  0.1782\n","====> Validation loss: 3.8439,  X1 loss: 3.8405   X2 loss: 3.8473\n","Batch:       0: 3.9408  3.9327  3.9489   0.0506  0.1857\n","Batch:      10: 3.9030  3.9078  3.8982   0.0498  0.1832\n","Batch:      20: 4.0191  4.0215  4.0167   0.0548  0.1848\n","Batch:      30: 3.7340  3.7409  3.7271   0.0536  0.1859\n","Batch:      40: 3.7488  3.7435  3.7542   0.0561  0.1873\n","Batch:      50: 3.8944  3.8877  3.9011   0.0575  0.1983\n","Batch:      60: 3.9190  3.9184  3.9197   0.0561  0.1967\n","Batch:      70: 3.7313  3.7312  3.7313   0.0627  0.2108\n","Batch:      80: 3.8787  3.8830  3.8743   0.0616  0.1991\n","Batch:      90: 3.8529  3.8462  3.8596   0.0658  0.2110\n","====> Validation loss: 3.8695,  X1 loss: 3.8639   X2 loss: 3.8751\n","Batch:       0: 3.7305  3.7286  3.7325   0.0657  0.2142\n","Batch:      10: 3.6928  3.6924  3.6933   0.0705  0.2186\n","Batch:      20: 3.8346  3.8273  3.8419   0.0716  0.2250\n","Batch:      30: 3.7916  3.7841  3.7991   0.0712  0.2160\n","Batch:      40: 3.8139  3.8079  3.8199   0.0711  0.2246\n","Batch:      50: 3.9933  3.9988  3.9878   0.0715  0.2201\n","Batch:      60: 3.7823  3.7827  3.7818   0.0736  0.2225\n","Batch:      70: 3.8701  3.8719  3.8683   0.0773  0.2260\n","Batch:      80: 3.7631  3.7547  3.7715   0.0808  0.2352\n","Batch:      90: 3.9119  3.9057  3.9180   0.0796  0.2299\n","====> Validation loss: 3.8004,  X1 loss: 3.7951   X2 loss: 3.8057\n","Batch:       0: 3.7150  3.7218  3.7082   0.0824  0.2255\n","Batch:      10: 3.6874  3.6779  3.6968   0.0789  0.2352\n","Batch:      20: 3.8416  3.8414  3.8419   0.0797  0.2414\n","Batch:      30: 3.8202  3.8094  3.8309   0.0892  0.2447\n","Batch:      40: 3.6191  3.6210  3.6171   0.0885  0.2484\n","Batch:      50: 3.8447  3.8421  3.8474   0.0872  0.2468\n","Batch:      60: 3.7837  3.7847  3.7827   0.0897  0.2563\n","Batch:      70: 3.9852  3.9863  3.9841   0.0930  0.2535\n","Batch:      80: 3.8438  3.8446  3.8431   0.1023  0.2474\n","Batch:      90: 3.8047  3.8008  3.8085   0.0958  0.2581\n","====> Validation loss: 3.7772,  X1 loss: 3.7718   X2 loss: 3.7826\n","Batch:       0: 3.6975  3.6953  3.6998   0.1004  0.2711\n","Batch:      10: 3.5294  3.5273  3.5314   0.0982  0.2649\n","Batch:      20: 3.6921  3.6893  3.6949   0.1079  0.2689\n","Batch:      30: 3.5775  3.5785  3.5766   0.1057  0.2650\n","Batch:      40: 3.6461  3.6442  3.6479   0.1007  0.2876\n","Batch:      50: 3.6950  3.6955  3.6944   0.1080  0.2796\n","Batch:      60: 4.0024  3.9989  4.0058   0.1079  0.2692\n","Batch:      70: 3.5517  3.5502  3.5531   0.1086  0.2864\n","Batch:      80: 3.7462  3.7450  3.7473   0.1128  0.2818\n","Batch:      90: 3.7582  3.7556  3.7608   0.1121  0.2810\n","====> Validation loss: 3.7741,  X1 loss: 3.7700   X2 loss: 3.7783\n","Batch:       0: 3.7143  3.7126  3.7160   0.1126  0.2916\n","Batch:      10: 3.7499  3.7514  3.7484   0.1149  0.2824\n","Batch:      20: 3.6123  3.6079  3.6168   0.1205  0.2818\n","Batch:      30: 3.8475  3.8524  3.8425   0.1193  0.2925\n","Batch:      40: 3.8440  3.8392  3.8488   0.1201  0.3002\n","Batch:      50: 3.7057  3.7105  3.7010   0.1260  0.3031\n","Batch:      60: 3.6870  3.6815  3.6925   0.1309  0.3006\n","Batch:      70: 3.7321  3.7286  3.7355   0.1280  0.2942\n","Batch:      80: 3.7113  3.7083  3.7142   0.1238  0.2939\n","Batch:      90: 3.7427  3.7389  3.7464   0.1267  0.3051\n","====> Validation loss: 3.7631,  X1 loss: 3.7576   X2 loss: 3.7686\n","Batch:       0: 3.5771  3.5843  3.5699   0.1316  0.3072\n","Batch:      10: 3.8025  3.8080  3.7971   0.1371  0.3074\n","Batch:      20: 3.5846  3.5793  3.5899   0.1421  0.3248\n","Batch:      30: 3.7384  3.7395  3.7373   0.1308  0.3263\n","Batch:      40: 3.8371  3.8339  3.8403   0.1452  0.3102\n","Batch:      50: 3.8111  3.8092  3.8130   0.1368  0.3265\n","Batch:      60: 3.8316  3.8307  3.8324   0.1473  0.3200\n","Batch:      70: 3.9076  3.9069  3.9083   0.1434  0.3170\n","Batch:      80: 3.5827  3.5794  3.5861   0.1582  0.3237\n","Batch:      90: 3.7816  3.7782  3.7849   0.1607  0.3365\n","====> Validation loss: 3.7144,  X1 loss: 3.7068   X2 loss: 3.7220\n","Batch:       0: 3.6796  3.6776  3.6817   0.1567  0.3264\n","Batch:      10: 3.7861  3.7845  3.7878   0.1577  0.3238\n","Batch:      20: 3.7932  3.7909  3.7954   0.1520  0.3316\n","Batch:      30: 3.3405  3.3287  3.3523   0.1562  0.3214\n","Batch:      40: 3.6513  3.6443  3.6583   0.1600  0.3405\n","Batch:      50: 3.8211  3.8187  3.8236   0.1614  0.3423\n","Batch:      60: 3.7195  3.7191  3.7200   0.1706  0.3461\n","Batch:      70: 3.7529  3.7518  3.7540   0.1683  0.3458\n","Batch:      80: 3.6068  3.6087  3.6048   0.1717  0.3504\n","Batch:      90: 3.6767  3.6751  3.6782   0.1738  0.3528\n","====> Validation loss: 3.7171,  X1 loss: 3.7115   X2 loss: 3.7228\n","Batch:       0: 3.5872  3.5943  3.5801   0.1774  0.3510\n","Batch:      10: 3.7184  3.7134  3.7235   0.1745  0.3541\n","Batch:      20: 3.7733  3.7682  3.7785   0.1792  0.3585\n","Batch:      30: 3.6806  3.6776  3.6837   0.1761  0.3534\n","Batch:      40: 3.6907  3.6873  3.6941   0.1866  0.3621\n","Batch:      50: 3.7521  3.7480  3.7562   0.1785  0.3448\n","Batch:      60: 3.6291  3.6255  3.6328   0.1944  0.3663\n","Batch:      70: 3.6141  3.6185  3.6098   0.1914  0.3615\n","Batch:      80: 3.6450  3.6465  3.6436   0.1924  0.3654\n","Batch:      90: 3.4836  3.4790  3.4882   0.2053  0.3614\n","====> Validation loss: 3.7224,  X1 loss: 3.7192   X2 loss: 3.7257\n","Batch:       0: 3.5528  3.5530  3.5526   0.2018  0.3645\n","Batch:      10: 3.3768  3.3829  3.3707   0.1927  0.3819\n","Batch:      20: 3.5513  3.5443  3.5582   0.1931  0.3699\n","Batch:      30: 3.5947  3.5916  3.5978   0.2031  0.3712\n","Batch:      40: 3.6538  3.6533  3.6542   0.1949  0.3761\n","Batch:      50: 3.6302  3.6246  3.6358   0.1982  0.3713\n","Batch:      60: 3.6344  3.6385  3.6303   0.1987  0.3803\n","Batch:      70: 3.4884  3.4889  3.4879   0.2107  0.3784\n","Batch:      80: 3.6816  3.6771  3.6862   0.2172  0.3746\n","Batch:      90: 3.5634  3.5638  3.5629   0.2214  0.3781\n","====> Validation loss: 3.6806,  X1 loss: 3.6790   X2 loss: 3.6823\n","Batch:       0: 3.2258  3.2217  3.2299   0.2202  0.3753\n","Batch:      10: 3.4469  3.4326  3.4612   0.2202  0.3892\n","Batch:      20: 3.5883  3.5964  3.5801   0.2143  0.3841\n","Batch:      30: 3.7374  3.7315  3.7433   0.2146  0.3806\n","Batch:      40: 3.6186  3.6203  3.6168   0.2222  0.3870\n","Batch:      50: 3.5246  3.5227  3.5265   0.2277  0.3888\n","Batch:      60: 3.8139  3.8096  3.8182   0.2335  0.3876\n","Batch:      70: 3.7522  3.7639  3.7404   0.2283  0.4098\n","Batch:      80: 3.5092  3.5054  3.5131   0.2314  0.4054\n","Batch:      90: 3.8144  3.8136  3.8153   0.2404  0.4192\n","====> Validation loss: 3.6861,  X1 loss: 3.6774   X2 loss: 3.6947\n","Batch:       0: 3.5918  3.5781  3.6055   0.2339  0.3882\n","Batch:      10: 3.7257  3.7257  3.7256   0.2474  0.4047\n","Batch:      20: 3.7048  3.7028  3.7068   0.2451  0.4156\n","Batch:      30: 3.6557  3.6374  3.6740   0.2427  0.4000\n","Batch:      40: 3.5788  3.5789  3.5787   0.2591  0.4211\n","Batch:      50: 3.9205  3.9177  3.9234   0.2520  0.4079\n","Batch:      60: 3.4427  3.4377  3.4477   0.2395  0.4015\n","Batch:      70: 3.4545  3.4605  3.4485   0.2409  0.4161\n","Batch:      80: 3.8943  3.8923  3.8963   0.2593  0.4055\n","Batch:      90: 3.4902  3.4885  3.4919   0.2742  0.4304\n","====> Validation loss: 3.7261,  X1 loss: 3.7219   X2 loss: 3.7303\n","Batch:       0: 3.5244  3.5243  3.5245   0.2649  0.4211\n","Batch:      10: 3.6384  3.6343  3.6425   0.2553  0.4125\n","Batch:      20: 3.6329  3.6289  3.6369   0.2611  0.4306\n","Batch:      30: 3.4523  3.4461  3.4584   0.2597  0.4309\n","Batch:      40: 3.4412  3.4362  3.4462   0.2742  0.4170\n","Batch:      50: 3.3850  3.3876  3.3824   0.2893  0.4170\n","Batch:      60: 3.5069  3.5018  3.5120   0.2703  0.4183\n","Batch:      70: 3.5416  3.5316  3.5517   0.2815  0.4365\n","Batch:      80: 3.6978  3.6974  3.6983   0.2903  0.4294\n","Batch:      90: 4.1011  4.0993  4.1029   0.2861  0.4354\n","====> Validation loss: 3.7081,  X1 loss: 3.7038   X2 loss: 3.7124\n","Batch:       0: 3.3675  3.3695  3.3656   0.2863  0.4419\n","Batch:      10: 3.5810  3.5884  3.5736   0.3099  0.4409\n","Batch:      20: 3.4780  3.4762  3.4798   0.2889  0.4407\n","Batch:      30: 3.7079  3.7138  3.7020   0.2926  0.4408\n","Batch:      40: 3.3390  3.3403  3.3378   0.2873  0.4507\n","Batch:      50: 3.3582  3.3592  3.3572   0.2980  0.4434\n","Batch:      60: 3.5235  3.5268  3.5203   0.3166  0.4462\n","Batch:      70: 3.7411  3.7384  3.7438   0.2985  0.4323\n","Batch:      80: 3.4881  3.4874  3.4888   0.2994  0.4368\n","Batch:      90: 3.5171  3.5166  3.5176   0.2943  0.4476\n","====> Validation loss: 3.6741,  X1 loss: 3.6698   X2 loss: 3.6784\n","Batch:       0: 3.4108  3.4125  3.4090   0.3101  0.4492\n","Batch:      10: 3.6458  3.6403  3.6514   0.3100  0.4724\n","Batch:      20: 3.6400  3.6389  3.6411   0.3125  0.4371\n","Batch:      30: 3.3880  3.3892  3.3868   0.3163  0.4332\n","Batch:      40: 3.5842  3.5751  3.5934   0.3251  0.4470\n","Batch:      50: 3.5525  3.5505  3.5544   0.3150  0.4545\n","Batch:      60: 3.6865  3.6867  3.6864   0.3170  0.4472\n","Batch:      70: 3.6388  3.6396  3.6381   0.3234  0.4401\n","Batch:      80: 3.7261  3.7209  3.7313   0.3090  0.4643\n","Batch:      90: 3.4617  3.4621  3.4612   0.3296  0.4532\n","====> Validation loss: 3.6851,  X1 loss: 3.6795   X2 loss: 3.6907\n","Batch:       0: 3.2612  3.2625  3.2599   0.3182  0.4561\n","Batch:      10: 3.2936  3.2946  3.2926   0.3393  0.4712\n","Batch:      20: 3.6203  3.6240  3.6165   0.3247  0.4523\n","Batch:      30: 3.3504  3.3493  3.3516   0.3342  0.4467\n","Batch:      40: 3.5681  3.5671  3.5692   0.3191  0.4658\n","Batch:      50: 3.3772  3.3778  3.3766   0.3466  0.4719\n","Batch:      60: 3.4174  3.4164  3.4184   0.3484  0.4583\n","Batch:      70: 3.5422  3.5438  3.5406   0.3445  0.4602\n","Batch:      80: 3.5377  3.5450  3.5303   0.3393  0.4691\n","Batch:      90: 3.6630  3.6744  3.6516   0.3581  0.4672\n","====> Validation loss: 3.6678,  X1 loss: 3.6637   X2 loss: 3.6719\n","Batch:       0: 3.5714  3.5714  3.5714   0.3438  0.4635\n","Batch:      10: 3.7077  3.7040  3.7115   0.3491  0.4805\n","Batch:      20: 3.6962  3.6904  3.7019   0.3685  0.4670\n","Batch:      30: 3.3197  3.3171  3.3224   0.3474  0.4822\n","Batch:      40: 3.3041  3.2993  3.3090   0.3384  0.4903\n","Batch:      50: 3.5606  3.5606  3.5607   0.3428  0.4895\n","Batch:      60: 3.6661  3.6613  3.6709   0.3518  0.4771\n","Batch:      70: 3.5500  3.5601  3.5400   0.3587  0.4704\n","Batch:      80: 3.3661  3.3654  3.3668   0.3593  0.4839\n","Batch:      90: 3.5630  3.5657  3.5603   0.3561  0.4761\n","====> Validation loss: 3.7398,  X1 loss: 3.7356   X2 loss: 3.7440\n","Batch:       0: 3.4241  3.4208  3.4273   0.3499  0.4751\n","Batch:      10: 3.6266  3.6256  3.6275   0.3658  0.4657\n","Batch:      20: 3.3616  3.3606  3.3627   0.3654  0.5008\n","Batch:      30: 3.6454  3.6270  3.6638   0.3608  0.4899\n","Batch:      40: 3.5905  3.5828  3.5983   0.3644  0.4857\n","Batch:      50: 3.3745  3.3608  3.3883   0.3695  0.4805\n","Batch:      60: 3.4253  3.4148  3.4358   0.3888  0.4818\n","Batch:      70: 3.6121  3.6091  3.6151   0.3671  0.5016\n","Batch:      80: 3.4803  3.4720  3.4886   0.3761  0.4975\n","Batch:      90: 3.4215  3.4109  3.4321   0.3670  0.5090\n","====> Validation loss: 3.7534,  X1 loss: 3.7448   X2 loss: 3.7619\n","Batch:       0: 3.2744  3.2677  3.2811   0.3582  0.5158\n","Batch:      10: 3.3747  3.3762  3.3731   0.3712  0.5040\n","Batch:      20: 3.1368  3.1281  3.1456   0.4017  0.5210\n","Batch:      30: 3.5688  3.5666  3.5710   0.3777  0.5011\n","Batch:      40: 3.5114  3.5061  3.5168   0.3738  0.4925\n","Batch:      50: 3.2831  3.2872  3.2789   0.3761  0.5157\n","Batch:      60: 3.5883  3.5862  3.5905   0.4049  0.4970\n","Batch:      70: 3.7571  3.7542  3.7600   0.4071  0.5199\n","Batch:      80: 3.5932  3.5899  3.5965   0.3776  0.5040\n","Batch:      90: 3.3742  3.3813  3.3671   0.4094  0.4954\n","====> Validation loss: 3.7363,  X1 loss: 3.7316   X2 loss: 3.7410\n","Batch:       0: 3.5313  3.5290  3.5336   0.3879  0.5090\n","Batch:      10: 3.3606  3.3554  3.3658   0.3860  0.5144\n","Batch:      20: 3.1757  3.1766  3.1749   0.4061  0.5325\n","Batch:      30: 3.5655  3.5609  3.5702   0.3983  0.5048\n","Batch:      40: 3.1209  3.1218  3.1200   0.4009  0.5109\n","Batch:      50: 3.6814  3.6761  3.6867   0.3943  0.5109\n","Batch:      60: 3.5677  3.5703  3.5650   0.3967  0.5178\n","Batch:      70: 3.5204  3.5145  3.5262   0.4161  0.5316\n","Batch:      80: 3.3695  3.3676  3.3715   0.4184  0.5347\n","Batch:      90: 3.4916  3.4868  3.4965   0.4099  0.5244\n","====> Validation loss: 3.7117,  X1 loss: 3.7048   X2 loss: 3.7187\n","Batch:       0: 3.2968  3.2997  3.2940   0.4105  0.5253\n","Batch:      10: 3.7461  3.7426  3.7495   0.4121  0.5224\n","Batch:      20: 3.1697  3.1651  3.1744   0.3999  0.5070\n","Batch:      30: 3.4746  3.4731  3.4760   0.4203  0.4893\n","Batch:      40: 3.3682  3.3720  3.3645   0.3869  0.5097\n","Batch:      50: 3.5236  3.5127  3.5344   0.4111  0.5231\n","Batch:      60: 3.6179  3.6162  3.6197   0.4159  0.5317\n","Batch:      70: 3.4566  3.4639  3.4494   0.4047  0.5425\n","Batch:      80: 3.5062  3.5066  3.5058   0.4311  0.5441\n","Batch:      90: 3.5671  3.5648  3.5693   0.4270  0.5214\n","====> Validation loss: 3.6896,  X1 loss: 3.6815   X2 loss: 3.6977\n","Batch:       0: 3.4062  3.3975  3.4149   0.4157  0.5325\n","Batch:      10: 3.5243  3.5173  3.5313   0.4102  0.5281\n","Batch:      20: 3.3501  3.3500  3.3501   0.4379  0.5048\n","Batch:      30: 3.3702  3.3641  3.3764   0.4145  0.5268\n","Batch:      40: 3.5443  3.5432  3.5454   0.4152  0.5137\n","Batch:      50: 3.6460  3.6512  3.6408   0.4213  0.5419\n","Batch:      60: 3.2727  3.2661  3.2793   0.4464  0.5322\n","Batch:      70: 3.4822  3.4798  3.4845   0.4315  0.5428\n","Batch:      80: 3.3892  3.3870  3.3914   0.4169  0.5407\n","Batch:      90: 3.2902  3.2901  3.2902   0.4164  0.5374\n","====> Validation loss: 3.7828,  X1 loss: 3.7761   X2 loss: 3.7896\n","Batch:       0: 3.5106  3.5069  3.5143   0.4427  0.5631\n","Batch:      10: 3.5502  3.5417  3.5588   0.4311  0.5418\n","Batch:      20: 3.5907  3.5826  3.5988   0.4192  0.5501\n","Batch:      30: 3.3403  3.3355  3.3451   0.4339  0.5336\n","Batch:      40: 3.6275  3.6203  3.6346   0.4271  0.5406\n","Batch:      50: 3.3720  3.3773  3.3667   0.4113  0.5296\n","Batch:      60: 3.1755  3.1710  3.1800   0.4119  0.5330\n","Batch:      70: 3.5945  3.5984  3.5906   0.4299  0.5717\n","Batch:      80: 3.5927  3.5933  3.5921   0.4361  0.5560\n","Batch:      90: 3.3573  3.3592  3.3554   0.4387  0.5555\n","====> Validation loss: 3.7480,  X1 loss: 3.7436   X2 loss: 3.7525\n","Batch:       0: 3.2581  3.2507  3.2655   0.4268  0.5671\n","Batch:      10: 3.4954  3.4931  3.4977   0.4630  0.5514\n","Batch:      20: 3.3170  3.3155  3.3185   0.4428  0.5762\n","Batch:      30: 3.4078  3.4049  3.4106   0.4496  0.5661\n","Batch:      40: 3.4435  3.4390  3.4480   0.4510  0.5332\n","Batch:      50: 3.5123  3.5072  3.5173   0.4461  0.5693\n","Batch:      60: 3.3683  3.3676  3.3689   0.4306  0.5812\n","Batch:      70: 3.3220  3.3255  3.3185   0.4464  0.5527\n","Batch:      80: 3.3296  3.3223  3.3370   0.4387  0.5532\n","Batch:      90: 3.5549  3.5491  3.5606   0.4165  0.5552\n","====> Validation loss: 3.7533,  X1 loss: 3.7475   X2 loss: 3.7591\n","Batch:       0: 3.0989  3.1008  3.0969   0.4484  0.5554\n","Batch:      10: 3.5256  3.5212  3.5301   0.4413  0.5664\n","Batch:      20: 3.3593  3.3651  3.3535   0.4355  0.5648\n","Batch:      30: 3.4877  3.4848  3.4907   0.4478  0.5574\n","Batch:      40: 3.3202  3.3203  3.3202   0.4405  0.5757\n","Batch:      50: 3.3579  3.3576  3.3583   0.4497  0.5637\n","Batch:      60: 3.3924  3.3921  3.3928   0.4334  0.5589\n","Batch:      70: 3.5762  3.5744  3.5779   0.4394  0.5502\n","Batch:      80: 3.4113  3.4072  3.4153   0.4357  0.5749\n","Batch:      90: 3.5539  3.5521  3.5558   0.4324  0.5775\n","====> Validation loss: 3.8002,  X1 loss: 3.7954   X2 loss: 3.8049\n","Batch:       0: 3.3648  3.3539  3.3756   0.4797  0.5660\n","Batch:      10: 3.3341  3.3353  3.3328   0.4521  0.5698\n","Batch:      20: 3.2518  3.2547  3.2490   0.4301  0.5635\n","Batch:      30: 3.1252  3.1275  3.1229   0.4201  0.5781\n","Batch:      40: 3.2648  3.2694  3.2602   0.4671  0.5868\n","Batch:      50: 3.2967  3.2943  3.2990   0.4629  0.5684\n","Batch:      60: 3.4556  3.4548  3.4563   0.4579  0.5808\n","Batch:      70: 3.1137  3.1102  3.1171   0.4429  0.5876\n","Batch:      80: 3.5529  3.5560  3.5497   0.4690  0.5699\n","Batch:      90: 3.5355  3.5311  3.5400   0.4667  0.5824\n","====> Validation loss: 3.6939,  X1 loss: 3.6888   X2 loss: 3.6990\n","Batch:       0: 3.1911  3.1841  3.1982   0.4849  0.5742\n","Batch:      10: 3.3857  3.3820  3.3895   0.4640  0.5390\n","Batch:      20: 3.0863  3.0899  3.0827   0.4726  0.6012\n","Batch:      30: 3.3561  3.3510  3.3613   0.5002  0.5730\n","Batch:      40: 3.4411  3.4323  3.4498   0.4609  0.5779\n","Batch:      50: 3.4673  3.4605  3.4741   0.4615  0.5953\n","Batch:      60: 3.2021  3.2092  3.1950   0.4495  0.5678\n","Batch:      70: 3.4672  3.4630  3.4714   0.4267  0.5770\n","Batch:      80: 3.5463  3.5394  3.5533   0.4505  0.6035\n","Batch:      90: 3.4152  3.4089  3.4215   0.4856  0.6113\n","====> Validation loss: 3.8129,  X1 loss: 3.8067   X2 loss: 3.8191\n","Batch:       0: 3.3708  3.3689  3.3728   0.4707  0.5867\n","Batch:      10: 3.1838  3.1842  3.1834   0.4783  0.5786\n","Batch:      20: 3.5785  3.5777  3.5793   0.4610  0.5692\n","Batch:      30: 3.3306  3.3384  3.3229   0.4771  0.5749\n","Batch:      40: 3.1098  3.1059  3.1138   0.4680  0.6159\n","Batch:      50: 3.4924  3.4980  3.4869   0.4787  0.5936\n","Batch:      60: 3.1772  3.1804  3.1740   0.4779  0.5633\n","Batch:      70: 3.4951  3.4939  3.4963   0.4790  0.5930\n","Batch:      80: 3.4105  3.4071  3.4139   0.4801  0.5989\n","Batch:      90: 3.5913  3.5791  3.6036   0.4706  0.5917\n","====> Validation loss: 3.7967,  X1 loss: 3.7862   X2 loss: 3.8072\n","Batch:       0: 3.3894  3.3870  3.3919   0.4832  0.6316\n","Batch:      10: 3.0824  3.0786  3.0862   0.4614  0.5880\n","Batch:      20: 3.5185  3.5122  3.5247   0.4569  0.6087\n","Batch:      30: 3.2583  3.2591  3.2576   0.4718  0.6350\n","Batch:      40: 3.2414  3.2338  3.2490   0.4743  0.5984\n","Batch:      50: 3.4550  3.4494  3.4606   0.4909  0.6202\n","Batch:      60: 3.7321  3.7341  3.7301   0.4644  0.6216\n","Batch:      70: 3.4352  3.4362  3.4343   0.4842  0.6137\n","Batch:      80: 3.1962  3.1955  3.1968   0.4844  0.6043\n","Batch:      90: 3.4025  3.4021  3.4029   0.4745  0.6219\n","====> Validation loss: 3.7645,  X1 loss: 3.7589   X2 loss: 3.7700\n","Batch:       0: 3.6376  3.6276  3.6476   0.4824  0.6340\n","Batch:      10: 3.6632  3.6558  3.6707   0.4808  0.5998\n","Batch:      20: 3.2544  3.2522  3.2567   0.4896  0.6356\n","Batch:      30: 3.1070  3.1075  3.1065   0.4877  0.5834\n","Batch:      40: 3.2485  3.2471  3.2499   0.4958  0.5861\n","Batch:      50: 3.4866  3.4882  3.4851   0.4750  0.5896\n","Batch:      60: 3.4629  3.4589  3.4668   0.4979  0.6068\n","Batch:      70: 2.9150  2.9075  2.9225   0.4904  0.5898\n","Batch:      80: 3.4832  3.4785  3.4878   0.4786  0.6145\n","Batch:      90: 3.4005  3.4004  3.4005   0.4876  0.6080\n","====> Validation loss: 3.7905,  X1 loss: 3.7834   X2 loss: 3.7975\n","Batch:       0: 3.5182  3.5194  3.5169   0.4880  0.6251\n","Batch:      10: 3.0863  3.0896  3.0831   0.5022  0.6243\n","Batch:      20: 3.3694  3.3636  3.3751   0.4905  0.6071\n","Batch:      30: 3.2519  3.2488  3.2551   0.4921  0.6219\n","Batch:      40: 3.2212  3.2153  3.2271   0.4768  0.6157\n","Batch:      50: 3.0547  3.0506  3.0588   0.4739  0.6004\n","Batch:      60: 3.4345  3.4389  3.4301   0.4962  0.6264\n","Batch:      70: 3.3576  3.3548  3.3605   0.5107  0.6236\n","Batch:      80: 3.2259  3.2238  3.2280   0.4842  0.6427\n","Batch:      90: 3.2933  3.2947  3.2918   0.5032  0.6165\n","====> Validation loss: 3.8130,  X1 loss: 3.8087   X2 loss: 3.8173\n","Batch:       0: 3.3648  3.3628  3.3668   0.5147  0.6286\n","Batch:      10: 3.0763  3.0746  3.0781   0.4946  0.6572\n","Batch:      20: 3.4047  3.4054  3.4040   0.5053  0.6484\n","Batch:      30: 3.4389  3.4447  3.4331   0.5163  0.6526\n","Batch:      40: 3.3678  3.3698  3.3657   0.5012  0.6593\n","Batch:      50: 3.4675  3.4670  3.4680   0.4957  0.6406\n","Batch:      60: 3.2457  3.2475  3.2440   0.4869  0.6283\n","Batch:      70: 3.5990  3.5908  3.6072   0.4899  0.6125\n","Batch:      80: 3.3020  3.2953  3.3086   0.5045  0.6097\n","Batch:      90: 3.2585  3.2652  3.2518   0.5060  0.6245\n","====> Validation loss: 3.8697,  X1 loss: 3.8621   X2 loss: 3.8773\n","Batch:       0: 3.0770  3.0716  3.0823   0.5111  0.6373\n","Batch:      10: 3.2525  3.2544  3.2506   0.5137  0.6305\n","Batch:      20: 3.0976  3.0968  3.0983   0.5161  0.6243\n","Batch:      30: 3.3851  3.3864  3.3838   0.5042  0.6326\n","Batch:      40: 3.2954  3.2847  3.3061   0.5027  0.6326\n","Batch:      50: 3.2779  3.2844  3.2715   0.4850  0.6690\n","Batch:      60: 3.3489  3.3462  3.3516   0.5009  0.6567\n","Batch:      70: 3.3637  3.3665  3.3609   0.4941  0.6300\n","Batch:      80: 3.2400  3.2412  3.2388   0.5152  0.6620\n","Batch:      90: 3.6182  3.6109  3.6256   0.4783  0.6628\n","====> Validation loss: 3.8375,  X1 loss: 3.8287   X2 loss: 3.8463\n","Batch:       0: 3.2243  3.2151  3.2334   0.5212  0.6327\n","Batch:      10: 3.4270  3.4300  3.4240   0.4855  0.6704\n","Batch:      20: 3.2706  3.2759  3.2654   0.4957  0.6109\n","Batch:      30: 3.1634  3.1619  3.1648   0.5110  0.6643\n","Batch:      40: 3.4206  3.4211  3.4201   0.5227  0.6432\n","Batch:      50: 3.4558  3.4551  3.4565   0.5142  0.6658\n","Batch:      60: 3.3073  3.3138  3.3008   0.5111  0.6484\n","Batch:      70: 3.3614  3.3567  3.3661   0.5211  0.6604\n","Batch:      80: 3.3269  3.3254  3.3283   0.5047  0.6748\n","Batch:      90: 3.2849  3.2657  3.3041   0.5088  0.6644\n","====> Validation loss: 3.8752,  X1 loss: 3.8715   X2 loss: 3.8789\n","Batch:       0: 3.1642  3.1683  3.1602   0.5076  0.6501\n","Batch:      10: 3.3221  3.3253  3.3189   0.5247  0.6583\n","Batch:      20: 3.2927  3.3024  3.2831   0.5138  0.6440\n","Batch:      30: 3.3128  3.3125  3.3132   0.5095  0.6304\n","Batch:      40: 3.5295  3.5259  3.5330   0.5199  0.6437\n","Batch:      50: 3.3937  3.3964  3.3911   0.5103  0.6577\n","Batch:      60: 3.3787  3.3831  3.3743   0.5354  0.6558\n","Batch:      70: 3.3955  3.3915  3.3994   0.5059  0.6635\n","Batch:      80: 3.3371  3.3329  3.3414   0.4955  0.6857\n","Batch:      90: 3.4187  3.4153  3.4220   0.5036  0.6704\n","====> Validation loss: 3.8053,  X1 loss: 3.7917   X2 loss: 3.8188\n","Batch:       0: 3.1348  3.1367  3.1328   0.5308  0.6569\n","Batch:      10: 3.1887  3.1872  3.1903   0.5060  0.6963\n","Batch:      20: 3.3182  3.3201  3.3162   0.5342  0.7044\n","Batch:      30: 3.2527  3.2534  3.2519   0.5086  0.6732\n","Batch:      40: 3.4824  3.4767  3.4882   0.5251  0.6698\n","Batch:      50: 3.3268  3.3218  3.3319   0.4837  0.6283\n","Batch:      60: 3.2003  3.1952  3.2053   0.4959  0.6936\n","Batch:      70: 3.5801  3.5771  3.5832   0.5143  0.6604\n","Batch:      80: 3.3002  3.2910  3.3094   0.5126  0.6508\n","Batch:      90: 3.5319  3.5349  3.5290   0.5132  0.6868\n","====> Validation loss: 3.7651,  X1 loss: 3.7618   X2 loss: 3.7684\n","Batch:       0: 3.0806  3.0752  3.0861   0.5228  0.6732\n","Batch:      10: 3.1102  3.1102  3.1101   0.5276  0.6773\n","Batch:      20: 3.3256  3.3159  3.3353   0.5114  0.6772\n","Batch:      30: 3.4980  3.4907  3.5054   0.5197  0.6707\n","Batch:      40: 3.2252  3.2125  3.2380   0.5183  0.7046\n","Batch:      50: 3.1486  3.1402  3.1570   0.5342  0.6560\n","Batch:      60: 3.4421  3.4369  3.4472   0.5122  0.7150\n","Batch:      70: 3.2889  3.2783  3.2994   0.5140  0.7178\n","Batch:      80: 3.5321  3.5447  3.5195   0.5272  0.6852\n","Batch:      90: 3.3072  3.2976  3.3168   0.5263  0.6918\n","====> Validation loss: 3.8274,  X1 loss: 3.8203   X2 loss: 3.8345\n","Batch:       0: 3.1013  3.0973  3.1053   0.5377  0.7353\n","Batch:      10: 3.1829  3.1674  3.1983   0.5283  0.6950\n","Batch:      20: 3.3080  3.3026  3.3134   0.5337  0.6979\n","Batch:      30: 3.4257  3.4227  3.4287   0.5262  0.6929\n","Batch:      40: 2.9539  2.9515  2.9563   0.5417  0.6962\n","Batch:      50: 3.3275  3.3138  3.3412   0.5531  0.7088\n","Batch:      60: 3.4671  3.4537  3.4804   0.5503  0.7169\n","Batch:      70: 3.0867  3.0861  3.0874   0.5131  0.6948\n","Batch:      80: 3.3011  3.2885  3.3137   0.5635  0.6986\n","Batch:      90: 3.3824  3.3801  3.3846   0.5272  0.6907\n","====> Validation loss: 3.8252,  X1 loss: 3.8191   X2 loss: 3.8313\n","Batch:       0: 3.1058  3.1028  3.1089   0.5508  0.6963\n","Batch:      10: 3.3293  3.3281  3.3304   0.5044  0.7324\n","Batch:      20: 3.2770  3.2612  3.2927   0.5504  0.6983\n","Batch:      30: 3.2354  3.2320  3.2389   0.5340  0.6968\n","Batch:      40: 3.4059  3.4026  3.4093   0.5608  0.7188\n","Batch:      50: 3.6417  3.6346  3.6488   0.5561  0.6592\n","Batch:      60: 3.4175  3.4156  3.4194   0.5569  0.7074\n","Batch:      70: 3.1892  3.1843  3.1941   0.5458  0.7014\n","Batch:      80: 3.2889  3.2901  3.2878   0.5166  0.7315\n","Batch:      90: 3.1040  3.1038  3.1042   0.5580  0.6790\n","====> Validation loss: 3.8660,  X1 loss: 3.8535   X2 loss: 3.8785\n","Batch:       0: 3.1271  3.1274  3.1269   0.5556  0.7183\n","Batch:      10: 3.1697  3.1686  3.1707   0.5699  0.7314\n","Batch:      20: 3.1973  3.2042  3.1904   0.5438  0.7277\n","Batch:      30: 3.1792  3.1741  3.1842   0.5472  0.7345\n","Batch:      40: 3.2549  3.2465  3.2633   0.5207  0.7248\n","Batch:      50: 3.1809  3.1859  3.1760   0.5414  0.6925\n","Batch:      60: 3.4227  3.4286  3.4168   0.5318  0.7562\n","Batch:      70: 3.3593  3.3552  3.3634   0.5727  0.7151\n","Batch:      80: 3.5259  3.5239  3.5280   0.5389  0.7416\n","Batch:      90: 3.7513  3.7526  3.7500   0.5552  0.7060\n","====> Validation loss: 3.8701,  X1 loss: 3.8630   X2 loss: 3.8773\n","Batch:       0: 3.1091  3.1081  3.1101   0.5648  0.7394\n","Batch:      10: 3.2084  3.2046  3.2122   0.5568  0.7364\n","Batch:      20: 3.1438  3.1399  3.1477   0.5476  0.7365\n","Batch:      30: 2.9489  2.9438  2.9539   0.5596  0.7447\n","Batch:      40: 3.4706  3.4654  3.4758   0.5337  0.7644\n","Batch:      50: 3.3278  3.3248  3.3309   0.5461  0.7047\n","Batch:      60: 3.4288  3.4209  3.4368   0.5606  0.7351\n","Batch:      70: 3.3585  3.3583  3.3587   0.5332  0.7276\n","Batch:      80: 3.1081  3.1012  3.1150   0.5498  0.7462\n","Batch:      90: 3.1951  3.1992  3.1910   0.5607  0.6970\n","====> Validation loss: 3.8435,  X1 loss: 3.8339   X2 loss: 3.8530\n","Batch:       0: 3.2140  3.2119  3.2160   0.5396  0.7589\n","Batch:      10: 3.2138  3.2114  3.2163   0.5564  0.7711\n","Batch:      20: 3.5737  3.5753  3.5722   0.5684  0.7375\n","Batch:      30: 3.4922  3.4914  3.4931   0.5652  0.7307\n","Batch:      40: 2.9731  2.9640  2.9822   0.5557  0.7176\n","Batch:      50: 3.3097  3.3101  3.3093   0.5800  0.7338\n","Batch:      60: 3.2676  3.2605  3.2747   0.5577  0.7048\n","Batch:      70: 3.2241  3.2148  3.2334   0.5482  0.7397\n","Batch:      80: 3.2683  3.2587  3.2779   0.5466  0.7584\n","Batch:      90: 3.4149  3.3964  3.4334   0.5682  0.7613\n","====> Validation loss: 3.8983,  X1 loss: 3.8887   X2 loss: 3.9080\n","Batch:       0: 3.3704  3.3607  3.3801   0.5743  0.7514\n","Batch:      10: 3.3208  3.3263  3.3154   0.5612  0.7189\n","Batch:      20: 3.1641  3.1679  3.1604   0.5457  0.7465\n","Batch:      30: 3.2277  3.2247  3.2308   0.5344  0.7412\n","Batch:      40: 3.4334  3.4303  3.4365   0.5636  0.7692\n","Batch:      50: 3.1337  3.1197  3.1477   0.5821  0.7761\n","Batch:      60: 2.9474  2.9473  2.9476   0.5713  0.7737\n","Batch:      70: 3.0958  3.1004  3.0912   0.5771  0.7818\n","Batch:      80: 3.4128  3.4148  3.4107   0.5846  0.7586\n","Batch:      90: 3.4182  3.4144  3.4220   0.5897  0.7693\n","====> Validation loss: 3.8125,  X1 loss: 3.8035   X2 loss: 3.8214\n","Batch:       0: 3.2294  3.2195  3.2392   0.5703  0.7984\n","Batch:      10: 2.9883  2.9991  2.9775   0.5736  0.7870\n","Batch:      20: 3.4999  3.5052  3.4946   0.5771  0.7563\n","Batch:      30: 3.1782  3.1706  3.1859   0.5659  0.8085\n","Batch:      40: 3.2586  3.2587  3.2584   0.5691  0.7697\n","Batch:      50: 2.9838  2.9877  2.9799   0.5566  0.7792\n","Batch:      60: 3.1994  3.1918  3.2070   0.5659  0.7859\n","Batch:      70: 3.4970  3.4906  3.5033   0.5636  0.7965\n","Batch:      80: 3.2315  3.2272  3.2359   0.5854  0.7703\n","Batch:      90: 3.3753  3.3759  3.3746   0.5525  0.7809\n","====> Validation loss: 3.8446,  X1 loss: 3.8394   X2 loss: 3.8499\n","Batch:       0: 3.0275  3.0260  3.0289   0.5965  0.7734\n","Batch:      10: 3.0610  3.0517  3.0702   0.5664  0.7772\n","Batch:      20: 3.0335  3.0275  3.0396   0.5691  0.8022\n","Batch:      30: 2.9614  2.9634  2.9593   0.5821  0.7925\n","Batch:      40: 3.4850  3.4749  3.4952   0.5857  0.7713\n","Batch:      50: 3.3362  3.3270  3.3454   0.5689  0.7698\n","Batch:      60: 3.4181  3.4128  3.4233   0.5905  0.8063\n","Batch:      70: 3.2876  3.2852  3.2899   0.5914  0.7900\n","Batch:      80: 3.4750  3.4696  3.4803   0.5796  0.7992\n","Batch:      90: 3.3021  3.2859  3.3182   0.5919  0.7871\n","====> Validation loss: 3.8544,  X1 loss: 3.8418   X2 loss: 3.8671\n"]}],"source":["num_train = eegs_train.shape[0]\n","lossi = []\n","udri = [] # update / data ratio \n","ud = []\n","\n","for epoch in range(1, 50):\n","\n","    model.train()\n","    for ix_batch, (Xb_eeg, Xb_env) in enumerate(dataloader):\n","\n","        # send to device\n","        Xb_eeg = Xb_eeg.to(device)\n","        Xb_env = Xb_env.to(device)\n","\n","        # Zero out gradients\n","        optimizer.zero_grad()\n","\n","        # forward pass\n","        eeg_features, env_features, logit_scale = model(Xb_eeg, Xb_env) \n","\n","\n","        # normalize features\n","        eeg_features_n = eeg_features / eeg_features.norm(dim=1, keepdim=True)\n","        env_features_n = env_features / env_features.norm(dim=1, keepdim=True)\n","\n","        # logits\n","        logits_per_eeg = logit_scale * eeg_features_n @ env_features_n.t()\n","        logits_per_env = logits_per_eeg.t()\n","\n","        #loss function\n","        labels = torch.arange(batch_size).to(device)\n","        loss_eeg = F.cross_entropy(logits_per_eeg, labels)\n","        loss_env = F.cross_entropy(logits_per_env, labels)\n","        loss   = (loss_eeg + loss_env)/2\n","\n","        # backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","        # track stats\n","        if ix_batch % 10 == 0:\n","            print(f'Batch: {ix_batch:7d}: {loss.item():.4f}  {loss_eeg.item():.4f}  {loss_env.item():.4f}   {eeg_features.detach().std().item():.4f}  {env_features.detach().std().item():.4f}')\n","        lossi.append(loss.log10().item())\n","\n","        with torch.no_grad():\n","            ud.append([(lr*p.grad.std() / p.data.std()).log10().item() for p in model.parameters() ])\n","        \n","        #break   \n","\n","    eval_model_cl(dl_val, model, device=device)\n","    \n","    # normalize weights\n","    with torch.no_grad():\n","      normalize_weights_eegnet(model.eeg_encoder)\n","\n","    model.train()\n","            \n","    #break   \n"]},{"cell_type":"code","source":["eegs_val.device, envs_val.device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1obDMZU4OG1W","executionInfo":{"status":"ok","timestamp":1677579741669,"user_tz":-60,"elapsed":24,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"ada058af-a577-4827-d417-8416256211e1"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(device(type='cpu'), device(type='cpu'))"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"kksVFmfnZQe7","executionInfo":{"status":"error","timestamp":1677579742586,"user_tz":-60,"elapsed":921,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"613f088a-7fe7-43f2-c2f9-cefba5e12dee"},"outputs":[{"output_type":"stream","name":"stdout","text":["layer 4 (       ELU):  mean  +0.049769, std 6.764015e-01, range [-9.998208e-01  9.879107e+00]\n","torch.Size([64, 16, 1, 640])\n"]},{"output_type":"error","ename":"NotImplementedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-34871b221268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'layer %d (%10s):  mean  %+f, std %e, range [%e  %e]'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlegends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'layer {i} ({layer.__class__.__name__}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::histogram.bin_ct' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::histogram.bin_ct' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:14889 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:14889 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:14889 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:14889 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:14889 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:14889 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:14889 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:14889 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:14889 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:14889 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:14889 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:14889 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:14889 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:14889 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:14889 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:14889 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1440x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["# ELU Activations and their gradients\n","\n","plt.figure(figsize=(20, 4)); # width and height of the plot\n","legends = []\n","for i, layer in enumerate(model.eeg_encoder[:-1]): # note: exclude the output layer\n","    if isinstance(layer, (ELU)):\n","        t = layer.out\n","        print('layer %d (%10s):  mean  %+f, std %e, range [%e  %e]' % (i, layer.__class__.__name__, t.mean(), t.std(), t.min(), t.max()))\n","        print(t.shape)\n","        hy, hx = torch.histogram(t, density=True)\n","        plt.plot(hx[:-1].detach(), hy.detach())\n","        legends.append(f'layer {i} ({layer.__class__.__name__}')\n","    plt.legend(legends);\n","    plt.title('activation distribution')\n","\n","\n","\n","plt.figure(figsize=(20, 4)); # width and height of the plot\n","legends = []\n","for i, layer in enumerate(model.env_encoder[:-1]): # note: exclude the output layer\n","    if isinstance(layer, (ELU)):\n","        t = layer.out\n","        print('layer %d (%10s):  mean  %+f, std %e, range [%e  %e]' % (i, layer.__class__.__name__, t.mean(), t.std(), t.min(), t.max()))\n","        print(t.shape)\n","        hy, hx = torch.histogram(t, density=True)\n","        plt.plot(hx[:-1].detach(), hy.detach())\n","        legends.append(f'layer {i} ({layer.__class__.__name__}')\n","    plt.legend(legends);\n","    plt.title('activation distribution')\n","\n","\n","plt.figure(figsize=(20, 4)) # width and height of the plot\n","legends = []\n","for i, layer in enumerate(model.eeg_encoder[:-1]): # note: exclude the output layer\n","    if isinstance(layer, ELU):\n","        t = layer.out.grad\n","        print('layer %d (%10s):  mean  %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n","        hy, hx = torch.histogram(t, density=True)\n","        plt.plot(hx[:-1].detach(), hy.detach())\n","        legends.append(f'layer {i} ({layer.__class__.__name__})')\n","    plt.legend(legends);\n","    plt.title('gradient distribution')\n","\n","\n","plt.figure(figsize=(20, 4)) # width and height of the plot\n","legends = []\n","for i, layer in enumerate(model.env_encoder[:-1]): # note: exclude the output layer\n","    if isinstance(layer, ELU):\n","        t = layer.out.grad\n","        print('layer %d (%10s):  mean  %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n","        hy, hx = torch.histogram(t, density=True)\n","        plt.plot(hx[:-1].detach(), hy.detach())\n","        legends.append(f'layer {i} ({layer.__class__.__name__})')\n","    plt.legend(legends);\n","    plt.title('gradient distribution')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rd_2oMcSZQe7","executionInfo":{"status":"aborted","timestamp":1677579742587,"user_tz":-60,"elapsed":8,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}}},"outputs":[],"source":["# Conv2d Activations and their gradients\n","if True:\n","    plt.figure(figsize=(20, 4)); # width and height of the plot\n","    legends = []\n","    for i, layer in enumerate(model.eeg_encoder[:-1]): # note: exclude the output layer\n","        if isinstance(layer, (Conv2d)):\n","            t = layer.out.to('cpu')\n","            print('layer %d (%10s):  mean  %+f, std %e, range [%e  %e]' % (i, layer.__class__.__name__, t.mean(), t.std(), t.min(), t.max()))\n","            print(t.shape)\n","            hy, hx = torch.histogram(t, density=True)\n","            plt.plot(hx[:-1].detach(), hy.detach())\n","            #plt.xlim(-.1, .1)\n","            #plt.ylim(0, 1000)\n","            legends.append(f'layer {i} ({layer.__class__.__name__}')\n","        plt.legend(legends);\n","        plt.title('activation distribution')\n","\n","    plt.figure(figsize=(20, 4)); # width and height of the plot\n","    legends = []\n","    for i, layer in enumerate(model.env_encoder[:-1]): # note: exclude the output layer\n","        if isinstance(layer, (Conv2d)):\n","            t = layer.out.to('cpu')\n","            print('layer %d (%10s):  mean  %+f, std %e, range [%e  %e]' % (i, layer.__class__.__name__, t.mean(), t.std(), t.min(), t.max()))\n","            print(t.shape)\n","            hy, hx = torch.histogram(t, density=True)\n","            plt.plot(hx[:-1].detach(), hy.detach())\n","            legends.append(f'layer {i} ({layer.__class__.__name__}')\n","        plt.legend(legends);\n","        plt.title('activation distribution')\n","\n","\n","    plt.figure(figsize=(20, 4)) # width and height of the plot\n","    legends = []\n","    for i, layer in enumerate(model.eeg_encoder[:-1]): # note: exclude the output layer\n","        if isinstance(layer, Conv2d):\n","            t = layer.out.grad\n","            print('layer %d (%10s):  mean  %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n","            hy, hx = torch.histogram(t, density=True)\n","            plt.plot(hx[:-1].detach(), hy.detach())\n","            legends.append(f'layer {i} ({layer.__class__.__name__})')\n","        plt.legend(legends);\n","        plt.title('gradient distribution')\n","\n","\n","    plt.figure(figsize=(20, 4)) # width and height of the plot\n","    legends = []\n","    for i, layer in enumerate(model.env_encoder[:-1]): # note: exclude the output layer\n","        if isinstance(layer, Conv2d):\n","            t = layer.out.grad\n","            print('layer %d (%10s):  mean  %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n","            hy, hx = torch.histogram(t, density=True)\n","            plt.plot(hx[:-1].detach(), hy.detach())\n","            legends.append(f'layer {i} ({layer.__class__.__name__})')\n","        plt.legend(legends);\n","        plt.title('gradient distribution')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoWdKU9gZQe8","executionInfo":{"status":"aborted","timestamp":1677579742587,"user_tz":-60,"elapsed":7,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}}},"outputs":[],"source":["\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H62yQcc4ZQe8","executionInfo":{"status":"aborted","timestamp":1677579742587,"user_tz":-60,"elapsed":6,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}}},"outputs":[],"source":["# Check the update / data ratio\n","\n","plt.figure(figsize=(20, 6))\n","legends = []\n","for i, p in enumerate(model.parameters()):\n","    if p.ndim == 4:\n","        plt.plot([ud[j][i] for j in range(len(ud))])\n","        legends.append('param %d' % i)\n","plt.plot([0, len(ud)], [-3, -3], 'k') # these rations should be ~1e-3, indicate on plot.\n","plt.legend(legends);"]}],"metadata":{"kernelspec":{"display_name":"mne","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"8e19e54895c02f0e9343d0fbd6cee45458aaf6f05de9ab3004d10bba5525a5d0"}},"colab":{"provenance":[{"file_id":"13Poz5tFnEFXpegMbhqAinRdIfSvPnPge","timestamp":1677524195292}],"machine_shape":"hm"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"758996c2171341da8fb111a5f1c3b4ef":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_33c4fb79add44b4b88c96dc6698ded61","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"     \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m131.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">     <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">7.6/7.6 MB</span> <span style=\"color: #800000; text-decoration-color: #800000\">131.1 MB/s</span> eta <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span>\n</pre>\n"},"metadata":{}}]}},"33c4fb79add44b4b88c96dc6698ded61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}