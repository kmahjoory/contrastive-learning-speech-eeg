{"cells":[{"cell_type":"markdown","metadata":{"id":"VhCK252zNjUG"},"source":["## COLAB TOOLS"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22295,"status":"ok","timestamp":1677676335384,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"v-pxnK4OaI-b","outputId":"4c8718b0-3123-4a9f-b0ad-42e36190c740"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":554,"referenced_widgets":["d3bc19eeda074194b3ced45c8d7c7045","a7cbff604a0b4e898d8cda745ebe3a81"]},"executionInfo":{"elapsed":9528,"status":"ok","timestamp":1677676344907,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"ZEopbadxbJH0","outputId":"975b20ba-8e5f-4334-aff4-9304b37cc3fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["['train_cl_eeg2speech_rochester_v3_test_gridsearch.ipynb', '.git', '.DS_Store', '.gitignore', 'EEG', 'LICENSE', 'train_cl_eeg2speech_rochester_v1.ipynb', 'train_cl_eeg2speech_rochester_v2.ipynb', 'train_cl_eeg2speech_rochester_v3.ipynb', '.ipynb_checkpoints', 'train_cl_eeg2speech_rochester_v3_test_old.ipynb', 'runs', 'train_cl_eeg2speech_rochester_v3_test.ipynb', 'train_cl_eeg2speech_rochester_v4_gridseaerch.ipynb', 'train_cl_eeg2speech_2.ipynb', 'train_cl_eeg2speech_rochester_subj_2.ipynb', 'README.md', 'train_eeg2speech_rochester.ipynb']\n"]},{"output_type":"stream","name":"stderr","text":["WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n","Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n","To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"]},{"output_type":"display_data","data":{"text/plain":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Collecting mne\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Collecting mne\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  Downloading mne-1.3.1-py3-none-any.whl (7.6 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Downloading mne-1.3.1-py3-none-any.whl (7.6 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3bc19eeda074194b3ced45c8d7c7045"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":[],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from mne) (23.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from mne) (23.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.8/dist-packages (from mne) (1.22.4)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: numpy&gt;=1.15.4 in /usr/local/lib/python3.8/dist-packages (from mne) (1.22.4)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from mne) (4.64.1)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from mne) (4.64.1)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from mne) (1.7.3)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: scipy&gt;=1.1.0 in /usr/local/lib/python3.8/dist-packages (from mne) (1.7.3)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.8/dist-packages (from mne) (1.6.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: pooch&gt;=1.5 in /usr/local/lib/python3.8/dist-packages (from mne) (1.6.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from mne) (3.1.2)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from mne) (3.1.2)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from mne) (4.4.2)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from mne) (4.4.2)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mne) (3.5.3)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mne) (3.5.3)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.5->mne) (2.25.1)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: requests&gt;=2.19.0 in /usr/local/lib/python3.8/dist-packages (from pooch&gt;=1.5-&gt;mne) (2.25.1)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.5->mne) (1.4.4)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: appdirs&gt;=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch&gt;=1.5-&gt;mne) (1.4.4)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->mne) (2.1.2)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2-&gt;mne) (2.1.2)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (1.4.4)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (1.4.4)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (4.38.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (4.38.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (8.4.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (8.4.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (3.0.9)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: pyparsing&gt;=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (3.0.9)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (0.11.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (0.11.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (2.8.2)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib-&gt;mne) (2.8.2)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib->mne) (1.15.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;mne) (1.15.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.26.14)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests&gt;=2.19.0-&gt;pooch&gt;=1.5-&gt;mne) (1.26.14)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.12.7)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests&gt;=2.19.0-&gt;pooch&gt;=1.5-&gt;mne) (2022.12.7)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (4.0.0)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests&gt;=2.19.0-&gt;pooch&gt;=1.5-&gt;mne) (4.0.0)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.10)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.8/dist-packages (from requests&gt;=2.19.0-&gt;pooch&gt;=1.5-&gt;mne) (2.10)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Installing collected packages: mne\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Installing collected packages: mne\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Successfully installed mne-1.3.1\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Successfully installed mne-1.3.1\n","</pre>\n"]},"metadata":{}}],"source":["\n","import os\n","import sys\n","\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"Colab Notebooks/prj_neuroread_analysis/neuroread/\"\n","GOOGLE_DRIVE_PATH = os.path.join(\"/content\", \"drive\", \"MyDrive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))\n","\n","# Add to sys so we can import .py files.\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","os.chdir(GOOGLE_DRIVE_PATH)\n","\n","# Install unavailable packages\n","import pip\n","def import_or_install(package):\n","    try:\n","        __import__(package)\n","    except ImportError:\n","        pip.main(['install', package])\n","\n","import_or_install(\"mne\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1677676344907,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"M0LuZowEbY29"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":447,"status":"ok","timestamp":1677676345346,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"HTqIrJcOaQLu","outputId":"6a41452c-24f5-4cbc-f75a-c3da3c2e7e70"},"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 89.6 gigabytes of available RAM\n","\n","Wed Mar  1 13:12:23 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   29C    P0    48W / 400W |      0MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n"," print('Not connected to a GPU')\n","else:\n"," print(gpu_info)"]},{"cell_type":"markdown","metadata":{"id":"aXEXbz7ENjUM"},"source":["## Main code"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4078,"status":"ok","timestamp":1677676349422,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"gGggzCoKZQeu","outputId":"b27f34b4-e93d-4176-f387-1049e690b254"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["import os, sys, glob\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","import numpy as np\n","\n","import mne\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import time\n","\n","from torchsummary import summary\n","from torch.utils.tensorboard import SummaryWriter\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1677676349422,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"GYgZe_3OQniG"},"outputs":[],"source":["def eval_model_cl(dl, model, device=torch.device('cpu'), verbose=True):\n","    \"\"\" \n","    This function calculates the loss on data, setting backward gradients and batchnorm\n","    off. This function is written for contrasting learning where the model takes in two\n","    inputs.\n","\n","    Args:\n","\n","    Returns:\n","      loss_test: Mean loss of all test samples (scalar)\n","\n","    \"\"\"\n","    losses, losses_X1, losses_X2 = [], [], []\n","    model.to(device)  # inplace for model\n","    # Set the model in evaluation mode\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for idx_batch, (X1b, X2b) in enumerate(dl):\n","\n","            X1b = X1b.to(device)\n","            X2b = X2b.to(device)\n","\n","            X1b_features, X2b_features, logit_sc = model(X1b, X2b)\n","\n","            # Normalize features\n","            X1b_f_n = X1b_features / X1b_features.norm(dim=1, keepdim=True)\n","            X2b_f_n = X2b_features / X2b_features.norm(dim=1, keepdim=True)\n","\n","            logits_per_X1 = logit_sc * X1b_f_n @ X2b_f_n.t()\n","            logits_per_X2 = logits_per_X1.t()\n","\n","            # Number of labels equals to the 1st dimension of X1b\n","            labels = torch.arange(X1b.shape[0], device=device)\n","\n","            # Batch Loss \n","            loss_X1 = F.cross_entropy(logits_per_X1, labels)\n","            loss_X2 = F.cross_entropy(logits_per_X2, labels)\n","            loss_batch   = (loss_X1 + loss_X2) / 2\n","            losses.append(loss_batch.item())\n","            losses_X1.append(loss_X1.item())\n","            losses_X2.append(loss_X2.item())\n","\n","        # Epoch loss (mean of batch losses)\n","        loss  = sum(losses) / len(losses)\n","        loss_X1 = sum(losses_X1) / len(losses_X1)\n","        loss_X2 = sum(losses_X2) / len(losses_X2)\n","\n","        if verbose:\n","          print(f\"====> Validation loss: {loss:.4f},  X1 loss: {loss_X1:.4f}   X2 loss: {loss_X2:.4f}\")\n","\n","        return loss, loss_X1, loss_X2\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1677676349422,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"EaAsZ-SLZQey"},"outputs":[],"source":["def unfold_raw(raw, window_size=None, stride=None):\n","    \"\"\"\n","    This function unfolds raw MNE object into a list of raw objects\n","    Args:\n","        raw: a raw MNE object cropped by rejecting bad segments.\n","    Returns:\n","        raw_unfolded: a raw MNE object unfolded by applying a sliding window.\n","    \"\"\"\n","    if window_size is None:\n","        window_size = int(5 * raw.info['sfreq'])\n","    if stride is None:\n","        stride = window_size\n","    nchans = len(raw.ch_names)\n","    sig = torch.tensor(raw.get_data(), dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n","    sig_unf = F.unfold(sig, (nchans, window_size), stride=stride , padding=0)\n","    sig_unf = sig_unf.permute(0, 2, 1).reshape(-1, sig_unf.shape[-1], nchans, window_size)\n","    return sig_unf"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1677676349423,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"1cf9EoF3ZQey"},"outputs":[],"source":["def rm_repeated_annotations(raw):\n","    \"\"\"This functions taskes in raw MNE obejct and removes repeated annotations\"\"\"\n","    annots = raw.annotations.copy()\n","    annots_drop = []\n","    for k in annots:\n","        annots_drop.extend([k for kk in annots if (k['onset'] > kk['onset']) and (k['onset']+k['duration'] < kk['onset']+kk['duration']) ])\n","\n","    annots_updated = [i for i in annots if i not in annots_drop]\n","    onsets = [i['onset'] for i in annots_updated]\n","    durations = [i['duration'] for i in annots_updated]\n","    descriptions = [i['description'] for i in annots_updated]\n","    print('Initial num of annots: %d  Num of removed annots: %d  Num of retained annots:  %d' % (len(annots), len(annots_drop), len(annots_updated)))\n","    print(f' New annots: {annots_updated}')\n","    raw.set_annotations(mne.Annotations(onsets, durations, descriptions) ) \n","    return raw"]},{"cell_type":"markdown","metadata":{"id":"q4sGJAdFZQez"},"source":["## Read Data"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1677676349423,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"KdQFfHcJZQe0","outputId":"577beb9a-3b1f-45ce-e1d8-e782efd34cd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------\n","window_size: 640  stride_size_test: 640\n","data_path: ../outputs/rochester_data/natural_speech\n"]}],"source":["subj_ids = [1, 2, 3, 4, 5]\n","fs = 128\n","window_size = int(5 * fs)\n","stride_size_train, stride_size_val, stride_size_test = int(2.5 * fs), int(5 * fs), int(5 * fs)\n","n_channs = 129 # 128 for eeg, 1 for env\n","batch_size = int(32)\n","print('-------------------------------------')\n","print(f'window_size: {window_size}  stride_size_test: {stride_size_test}')\n","\n","dataset_name = ['rochester_data', 'natural_speech']\n","outputs_path = f'../outputs/'\n","data_path = os.path.join(outputs_path, dataset_name[0], dataset_name[1])\n","print(f'data_path: {data_path}')"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":62394,"status":"ok","timestamp":1677676411811,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"fIhJi5IxZQe1","outputId":"a53865c3-c737-4d4e-fb34-eb06a1ba07b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Opening raw data file ../outputs/rochester_data/natural_speech/subj_1/after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 42  Num of removed annots: 19  Num of retained annots:  23\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.784332), ('duration', 2.0346832275390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.548248), ('duration', 1.66888427734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.407166), ('duration', 1.8746337890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 719.272339), ('duration', 0.75445556640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 899.217163), ('duration', 1.8060302734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.1604), ('duration', 3.612060546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1267.022461), ('duration', 1.8746337890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.1521), ('duration', 1.92041015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1613.958984), ('duration', 5.4410400390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.862427), ('duration', 2.42333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1977.397705), ('duration', 8.6873779296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2058.906494), ('duration', 3.932373046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2162.312988), ('duration', 2.354736328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.353516), ('duration', 1.943359375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.958984), ('duration', 4.137939453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2707.411133), ('duration', 3.65771484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.364502), ('duration', 1.4404296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3080.712646), ('duration', 5.760986328125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3259.638184), ('duration', 8.61865234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3439.719971), ('duration', 3.97802734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3486.391846), ('duration', 5.120849609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3610.283691), ('duration', 5.668701171875), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 20  N val: 1  N test: 1\n","Opening raw data file ../outputs/rochester_data/natural_speech/subj_2/after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 65  Num of removed annots: 19  Num of retained annots:  46\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 177.009033), ('duration', 4.4808807373046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 233.547455), ('duration', 3.1549072265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 256.64325), ('duration', 4.549468994140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.877777), ('duration', 1.554595947265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 419.798157), ('duration', 4.18365478515625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 521.238403), ('duration', 3.72650146484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 536.986145), ('duration', 8.36737060546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 591.58136), ('duration', 12.36810302734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 684.541626), ('duration', 37.07598876953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 731.651611), ('duration', 0.9830322265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 760.453308), ('duration', 5.80682373046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 895.355164), ('duration', 14.21990966796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 955.299988), ('duration', 3.9779052734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1010.325623), ('duration', 5.5782470703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1047.856567), ('duration', 3.0177001953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.390625), ('duration', 4.892333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1266.681152), ('duration', 8.984619140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1304.783569), ('duration', 3.2464599609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1385.044556), ('duration', 3.749267578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.464355), ('duration', 2.171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1613.947266), ('duration', 13.3740234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1636.351562), ('duration', 1.600341796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1650.508545), ('duration', 8.5731201171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1683.526245), ('duration', 6.5384521484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1726.90979), ('duration', 6.2640380859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1807.079224), ('duration', 13.076904296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.602295), ('duration', 1.348876953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1991.061035), ('duration', 2.126220703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2049.411133), ('duration', 4.503662109375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2136.050537), ('duration', 4.732421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2162.68042), ('duration', 11.659423828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2234.577881), ('duration', 7.315673828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.926758), ('duration', 14.63134765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.343262), ('duration', 9.076171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2546.635498), ('duration', 3.177734375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2707.869873), ('duration', 15.68310546875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.512939), ('duration', 8.481689453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3081.322021), ('duration', 2.468994140625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3087.928955), ('duration', 6.835693359375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3116.119141), ('duration', 3.886474609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3257.709961), ('duration', 6.652587890625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3280.65918), ('duration', 5.71533203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3440.005615), ('duration', 0.914306640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3452.968018), ('duration', 7.29296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3604.318115), ('duration', 3.817138671875), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 62  N val: 2  N test: 2\n","Opening raw data file ../outputs/rochester_data/natural_speech/subj_3/after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 47  Num of removed annots: 19  Num of retained annots:  28\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 43.135948), ('duration', 4.115089416503906), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 121.796593), ('duration', 5.6010894775390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.748077), ('duration', 1.760345458984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 275.317291), ('duration', 2.994842529296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.205322), ('duration', 2.126129150390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.65863), ('duration', 1.89752197265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.599854), ('duration', 2.12615966796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 898.988525), ('duration', 1.6231689453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1033.410278), ('duration', 0.9830322265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.293579), ('duration', 1.7603759765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1266.313721), ('duration', 3.56640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1307.502563), ('duration', 0.86865234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.074097), ('duration', 2.4461669921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1583.876953), ('duration', 0.3658447265625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1613.867554), ('duration', 1.9432373046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.839478), ('duration', 1.9661865234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.358643), ('duration', 2.1947021484375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2162.061523), ('duration', 2.14892578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.888916), ('duration', 0.8916015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.831299), ('duration', 2.92626953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2707.159668), ('duration', 2.01171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2790.049561), ('duration', 9.510498046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.086426), ('duration', 2.400390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3081.215576), ('duration', 1.874755859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.645996), ('duration', 2.14892578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3439.272217), ('duration', 2.42333984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3525.888916), ('duration', 1.87451171875), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 87  N val: 3  N test: 3\n","Opening raw data file ../outputs/rochester_data/natural_speech/subj_4/after_ica_raw.fif...\n","    Range : 0 ... 464394 =      0.000 ...  3628.078 secs\n","Ready.\n","Reading 0 ... 464394  =      0.000 ...  3628.078 secs...\n","Initial num of annots: 41  Num of removed annots: 19  Num of retained annots:  22\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 176.316391), ('duration', 2.4461822509765625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 357.619507), ('duration', 1.074493408203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.230835), ('duration', 1.851806640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.968323), ('duration', 4.89239501953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 898.236755), ('duration', 2.56048583984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1100.372314), ('duration', 2.5833740234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1265.114136), ('duration', 4.6181640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1437.281372), ('duration', 1.874755859375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1613.327759), ('duration', 1.783203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.417358), ('duration', 1.2344970703125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1981.789795), ('duration', 2.720458984375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2161.278076), ('duration', 4.8466796875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2344.500732), ('duration', 1.80615234375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2523.130127), ('duration', 2.194580078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2706.033447), ('duration', 2.743408203125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2876.088623), ('duration', 1.6689453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2892.718506), ('duration', 0.822998046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3079.622803), ('duration', 2.217529296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.026855), ('duration', 6.172607421875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3438.603516), ('duration', 0.822998046875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3535.585693), ('duration', 2.720458984375), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 106  N val: 4  N test: 4\n","Opening raw data file ../outputs/rochester_data/natural_speech/subj_5/after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 41  Num of removed annots: 19  Num of retained annots:  22\n"," New annots: [OrderedDict([('onset', 0.0), ('duration', 0.0), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 177.408173), ('duration', 0.270477294921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 358.121857), ('duration', 0.270477294921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 537.280273), ('duration', 1.9835205078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 718.369812), ('duration', 2.3441162109375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 899.271362), ('duration', 1.3974609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1101.870972), ('duration', 0.4508056640625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1267.272949), ('duration', 1.4425048828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1438.279663), ('duration', 1.126953125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1614.049805), ('duration', 1.8707275390625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1701.652954), ('duration', 0.96923828125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1806.822144), ('duration', 2.434326171875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 1982.066284), ('duration', 2.186279296875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2161.976074), ('duration', 1.983642578125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2345.484619), ('duration', 1.870849609375), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2524.411133), ('duration', 0.541015625), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2707.077393), ('duration', 2.569580078125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 2893.742432), ('duration', 0.8564453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3081.496094), ('duration', 0.653564453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3229.895752), ('duration', 1.84814453125), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3261.698975), ('duration', 2.727294921875), ('description', 'bad'), ('orig_time', None)]), OrderedDict([('onset', 3439.993408), ('duration', 0.901611328125), ('description', 'bad'), ('orig_time', None)])]\n","-------------------------------------\n","N train: 125  N val: 5  N test: 5\n","Shape Trian: torch.Size([5925, 1, 129, 640])  Shape Val: torch.Size([192, 1, 129, 640])  Shape Test: torch.Size([184, 1, 129, 640])\n","-------------------------------------\n","Shape EEG Train: torch.Size([5925, 1, 128, 640])  Val: torch.Size([192, 1, 128, 640])  Test: torch.Size([184, 1, 128, 640])\n","Mean: 7.502973936590607e-11  Std: 5.03486990055535e-06\n","Shape Env Train: torch.Size([5925, 1, 1, 640])  Val: torch.Size([192, 1, 1, 640])  Test: torch.Size([184, 1, 1, 640])\n","Mean Env: 2.3605871200561523  Std Env: 2.5989928245544434\n"]}],"source":["raws_train_windowed, raws_val_windowed, raws_test_windowed = [], [], []\n","\n","for subj_id in subj_ids:\n","    subj_path = os.path.join(data_path, f'subj_{subj_id}')\n","\n","    # load subject raw MNE object\n","    raw = mne.io.read_raw(os.path.join(subj_path, 'after_ica_raw.fif'), preload=True)\n","    # drop M1 and M2 channels\n","    raw.drop_channels(['M1', 'M2'])\n","    assert raw.info['nchan'] == n_channs\n","\n","    raw = rm_repeated_annotations(raw)\n","    annots = raw.annotations.copy()\n","    raw_split = [raw.copy().crop(t1, t2) for t1, t2 in zip(annots.onset[:-1]+annots.duration[:-1], annots.onset[1:])]\n","\n","    # Pick the split with the longest duration for validation, supposedly less noisy\n","    ix_val = np.argmax([i.get_data().shape[1] for i in raw_split])\n","    raw_val = [raw_split.pop(ix_val)] # create a list to make it iterable. later may be used for multiple splits\n","\n","    # Pick the next split with the longest duration for testing, supposedly less noisy\n","    ix_test = np.argmax([i.get_data().shape[1] for i in raw_split])\n","    raw_test = [raw_split.pop(ix_test)]\n","    \n","    # creat list of unfolded tensor raw objects\n","    fs = raw.info['sfreq']\n","    raws_train_windowed.extend([unfold_raw(i, window_size=window_size, stride=stride_size_train) for i in raw_split if i.get_data().shape[1] > window_size])\n","    raws_val_windowed.extend([unfold_raw(i, window_size=window_size, stride=stride_size_val) for i in raw_val if i.get_data().shape[1] > window_size])\n","    raws_test_windowed.extend([unfold_raw(i, window_size=window_size, stride=stride_size_test) for i in raw_test if i.get_data().shape[1] > window_size])\n","    print(\"-------------------------------------\")\n","    print('N train: %d  N val: %d  N test: %d' % (len(raws_train_windowed), len(raws_val_windowed), len(raws_test_windowed)))\n","\n","# concatenate all in second dimension\n","sigs_train = torch.cat(raws_train_windowed, dim=1).permute(1, 0, 2, 3)\n","sigs_val = torch.cat(raws_val_windowed, dim=1).permute(1, 0, 2, 3)\n","sigs_test = torch.cat(raws_test_windowed, dim=1).permute(1, 0, 2, 3)\n","print(f\"Shape Trian: {sigs_train.shape}  Shape Val: {sigs_val.shape}  Shape Test: {sigs_test.shape}\")\n","\n","eegs_train = sigs_train[:, :, :-1, :]\n","eegs_val = sigs_val[:, :, :-1, :]\n","eegs_test = sigs_test[:, :, :-1, :]\n","print(\"-------------------------------------\")\n","print(f\"Shape EEG Train: {eegs_train.shape}  Val: {eegs_val.shape}  Test: {eegs_test.shape}\")\n","\n","# To avoid information leakage, we estimate the mean and std from the training set only.\n","mean_eeg_train =  eegs_train.mean()\n","std_eeg_train = eegs_train.std()\n","print(f\"Mean: {mean_eeg_train}  Std: {std_eeg_train}\")\n","\n","envs_train = sigs_train[:, :, [-1], :]\n","envs_val = sigs_val[:, :, [-1], :]\n","envs_test = sigs_test[:, :, [-1], :]\n","print(f\"Shape Env Train: {envs_train.shape}  Val: {envs_val.shape}  Test: {envs_test.shape}\")\n","\n","# Estimate mean and std of the Envelope data set\n","mean_env_train =  envs_train.mean()\n","std_env_train = envs_train.std()\n","print(f\"Mean Env: {mean_env_train}  Std Env: {std_env_train}\")\n","\n","# Normalize the data\n","eegs_train = (eegs_train - mean_eeg_train) / std_eeg_train\n","eegs_val = (eegs_val - mean_eeg_train) / std_eeg_train\n","eegs_test = (eegs_test - mean_eeg_train) / std_eeg_train\n","\n","envs_train = (envs_train - mean_env_train) / std_env_train\n","envs_val = (envs_val - mean_env_train) / std_env_train\n","envs_test = (envs_test - mean_env_train) / std_env_train\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tesRTfOdZQe2"},"source":["### Pytorch dataloader"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1677676411812,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"HviGOmH1ZQe2"},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, eeg, env):\n","        self.eeg = eeg\n","        self.env = env\n","    \n","    def __getitem__(self, index):\n","        return self.eeg[index], self.env[index]\n","    \n","    def __len__(self):\n","        return len(self.eeg)\n","    \n","dataset_train = MyDataset(eegs_train, envs_train)\n","dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)\n","\n","dl_val = DataLoader(MyDataset(eegs_val, envs_val), batch_size=batch_size, shuffle=True, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"XMjI2NeFZQe3"},"source":["## Model"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1677676411812,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"57o2oV6VZQe3"},"outputs":[],"source":["class Conv2d(nn.Conv2d):\n","    def __init__(self, in_channels, out_channels, kernel_size, **kargs):\n","        super().__init__(in_channels, out_channels, kernel_size, **kargs)\n","\n","    def __call__(self, inp):\n","        self.out = super().__call__(inp)\n","\n","        if self.out.requires_grad:\n","            self.out.retain_grad()\n","\n","        return self.out\n","    \n","    # -----------------------------------------------------------------------------------------------\n","class Flatten:\n","    \n","  def __call__(self, x):\n","    self.out = x.view(x.shape[0], -1)\n","    return self.out\n","  \n","  def parameters(self):\n","    return []\n","  \n","  # -----------------------------------------------------------------------------------------------\n","class Linear(nn.Linear):\n","    def __init__(self, x, y, **kargs):\n","        super().__init__(x, y, **kargs)\n","\n","    def __call__(self, inp):\n","        self.out = super().__call__(inp)\n","        return self.out\n","  # -----------------------------------------------------------------------------------------------\n","   \n","class ELU(nn.ELU):\n","    def __init__(self, alpha=1.0, inplace=False):\n","        super().__init__(alpha=1.0, inplace=False)\n","\n","    def __call__(self, inp):\n","        self.out = super().__call__(inp)\n","        if self.out.requires_grad:\n","            self.out.retain_grad()\n","        return self.out\n","\n","  # -----------------------------------------------------------------------------------------------\n","class Sequential:\n","  \n","    def __init__(self, layers):\n","        self.layers = layers\n","\n","    def __call__(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        self.out = x\n","        return self.out\n","\n","    def parameters(self):\n","        # get parameters of all layers and stretch them out into one list\n","        return [p for layer in self.layers for p in layer.parameters()]\n","\n","    def named_parameters(self):\n","        # get parameters of all layers and stretch them out into one list\n","        return ((n, p) for layer in self.layers for n, p in layer.named_parameters())"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1677676411812,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"IffWKmD6ZQe3"},"outputs":[],"source":["# My implementation of the shallow convnet\n","\n","fs = 64 # sampling rate\n","T = 5 * fs # number of time points in each trial\n","C = 64 # number of EEG channels\n","F1 = 8 # number of channels (depth) in the first conv layer\n","D = 2 # number of spatial filters in the second conv layer\n","F2 = D * F1 # number of channels (depth) in the pont-wise conv layer\n","num_classes = 4 # number of classes\n","\n","shallow_covnet = Sequential([\n","    Conv2d(1, 40, (1, int(fs//2)), padding='same', bias=True),\n","    Conv2d(40, 40, (C, 1), padding=(0, 0), bias=False), nn.BatchNorm2d(40, affine=True), \n","    nn.AvgPool2d((1, 75), (1, 15)), nn.Dropout(0.5),\n","    Conv2d(40, 4, kernel_size=(1, 30), padding='same', stride=(1, 1), bias=True),\n","    nn.Flatten(1, -1), # Flatten start_dim=1, end_dim=-1\n","    Linear(62*4, 4, bias=True),\n","])\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bT-TwE-oTAHE","executionInfo":{"status":"ok","timestamp":1677676727885,"user_tz":-60,"elapsed":1043,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"cda6164c-e511-43c2-f4c4-ddc4cf242dc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 32])\n"]}],"source":["## EEG Encoder with LINEAR\n","\n","class EEGEncoderWithLinear(nn.Module):\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            C = 128, # number of EEG channels\n","            F1 = 8, # 8 or 4 number of channels (depth) in the first conv layer\n","            D = 2, # number of spatial filters in the second conv layer\n","            F2 = None # number of channels (depth) in the pont-wise conv layer\n","        ):\n","        super(EEGEncoderWithLinear, self).__init__()\n","\n","        if F2 is None:\n","            F2 = D * F1\n","\n","        self.eeg_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs/2)), padding='same', bias=True, groups=1),\n","            nn.BatchNorm2d(F1, affine=True),\n","            Conv2d(F1, out_channels=D*F1, kernel_size=(C, 1), padding=(0, 0), bias=False, groups=F1),\n","            nn.BatchNorm2d(D*F1, affine=True), ELU(), nn.AvgPool2d(1, 4), nn.Dropout(0.25),\n","                    \n","            Conv2d(F2, F2, (1, int(fs/(2*4))), padding='same', bias=False, groups=D*F1),\n","            Conv2d(D*F1, F2, kernel_size=(1, 1), padding=(0, 0), groups=1, bias=False),\n","            nn.BatchNorm2d(F2, affine=True), ELU(), nn.AvgPool2d(1, 8), nn.Dropout(0.25),\n","\n","            nn.Flatten(),\n","            nn.Linear(F2*int((T*fs)//(8*4)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.eeg_encoder(x)\n","        return x\n","\n","\n","def normalize_weights_eegnet(eeg_encoder):\n","\n","    for ix, (name, param) in enumerate(eeg_encoder.named_parameters()):\n","        if  name == 'weight' and param.ndim==4 and ix==1: # normalize conv weights to max norm 1\n","            param.data = torch.renorm(param.data, 2, 0, maxnorm=1)\n","        elif name == 'weight' and param.ndim==2: # normalize fc weights to max norm 0.25\n","            param.data = torch.renorm(param.data, 2, 0, maxnorm=0.25)\n","\n","\n","eeg_encoder_with_linear = EEGEncoderWithLinear()\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(eeg_encoder_with_linear(eegs_train[:32, :, :, :]).shape)\n","\n","#summary(eeg_encoder_with_linear, (1, 128, 640))"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1677676740254,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"sQjEWHA4ZQe4","outputId":"38825e11-ba44-43a2-fb2c-b8aa14edd5f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 320])\n"]}],"source":["## EEG Encoder NO LINEAR\n","\n","class EEGEncoderNoLinear(nn.Module):\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            C = 128, # number of EEG channels\n","            F1 = 8, # 8 or 4 number of channels (depth) in the first conv layer\n","            D = 2, # number of spatial filters in the second conv layer\n","            F2 = None # number of channels (depth) in the pont-wise conv layer\n","        ):\n","        super(EEGEncoderNoLinear, self).__init__()\n","\n","        if F2 is None:\n","            F2 = D * F1\n","\n","        self.eeg_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs/2)), padding='same', bias=True, groups=1),\n","            nn.BatchNorm2d(F1, affine=True),\n","            Conv2d(F1, out_channels=D*F1, kernel_size=(C, 1), padding=(0, 0), bias=False, groups=F1),\n","            nn.BatchNorm2d(D*F1, affine=True), ELU(), nn.AvgPool2d(1, 4), nn.Dropout(0.25),\n","                    \n","            Conv2d(F2, F2, (1, int(fs/(2*4))), padding='same', bias=False, groups=D*F1),\n","            Conv2d(D*F1, F2, kernel_size=(1, 1), padding=(0, 0), groups=1, bias=False),\n","            nn.BatchNorm2d(F2, affine=True), ELU(), nn.AvgPool2d(1, 8), nn.Dropout(0.25),\n","\n","            nn.Flatten(),\n","            #nn.Linear(F2*int((T*fs)//(8*4)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.eeg_encoder(x)\n","        return x\n","\n","\n","def normalize_weights_eegnet(eeg_encoder):\n","\n","    for ix, (name, param) in enumerate(eeg_encoder.named_parameters()):\n","        if  name == 'weight' and param.ndim==4 and ix==1: # normalize conv weights to max norm 1\n","            param.data = torch.renorm(param.data, 2, 0, maxnorm=1)\n","        elif name == 'weight' and param.ndim==2: # normalize fc weights to max norm 0.25\n","            param.data = torch.renorm(param.data, 2, 0, maxnorm=0.25)\n","\n","\n","eeg_encoder_no_linear = EEGEncoderNoLinear()\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(eeg_encoder_no_linear(eegs_train[:32, :, :, :]).shape)\n","\n","#summary(eeg_encoder_no_linear, (1, 128, 640))"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":519,"status":"ok","timestamp":1677676747321,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"SfO-UwlzZQe4","outputId":"9136619d-4714-476b-f173-7e2b38bbbeb3"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 320])\n"]}],"source":["class EnvEncoder3ConvNoLinear(nn.Module):\n","\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            F1 = 4\n","        ):\n","        super(EnvEncoder3ConvNoLinear, self).__init__()\n","\n","        self.env_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs//2)), padding='same', bias=True),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1, (1, int(fs//4)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1*4, (1, int(fs//8)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1*4, affine=True), ELU(), nn.AvgPool2d(1, 8), nn.Dropout(0.5),\n","            nn.Flatten(),\n","            #nn.Linear(F1*int((T*fs)//(2*8)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.env_encoder(x)\n","        return x\n","\n","env_encoder3conv_no_linear = EnvEncoder3ConvNoLinear()\n","\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(env_encoder3conv_no_linear(envs_train[:32, :, :, :]).shape)\n","#summary(env_encoder3conv_no_linear, (1, 1, 640))"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EHbSd3XITAHF","executionInfo":{"status":"ok","timestamp":1677676755562,"user_tz":-60,"elapsed":4,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"58f80412-2481-4e30-f3bc-12711d8ddd58"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 320])\n"]}],"source":["class EnvEncoder2ConvNoLinear(nn.Module):\n","\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            F1 = 4\n","        ):\n","        super(EnvEncoder2ConvNoLinear, self).__init__()\n","\n","        self.env_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs//2)), padding='same', bias=True),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1*4, (1, int(fs//4)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1*4, affine=True), ELU(), nn.AvgPool2d(1, 16), nn.Dropout(0.5),\n","            nn.Flatten(),\n","            #nn.Linear(F1*int((T*fs)//(2*8)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.env_encoder(x)\n","        return x\n","\n","env_encoder2conv_no_linear = EnvEncoder2ConvNoLinear()\n","\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(env_encoder2conv_no_linear(envs_train[:32, :, :, :]).shape)\n","#summary(env_encoder2conv_no_linear, (1, 1, 640))"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2914ppmhTAHG","executionInfo":{"status":"ok","timestamp":1677676763396,"user_tz":-60,"elapsed":5,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"cb9c28e1-a6a7-460f-9361-f47b937ad455"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 32])\n"]}],"source":["class EnvEncoder2ConvWithLinear(nn.Module):\n","\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            F1 = 4\n","        ):\n","        super(EnvEncoder2ConvWithLinear, self).__init__()\n","\n","        self.env_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs//2)), padding='same', bias=True),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1*4, (1, int(fs//4)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1*4, affine=True), ELU(), nn.AvgPool2d(1, 16), nn.Dropout(0.5),\n","            nn.Flatten(),\n","            nn.Linear(F1*4*int((T*fs)//(8*4)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.env_encoder(x)\n","        return x\n","\n","env_encoder2conv_with_linear = EnvEncoder2ConvWithLinear()\n","\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(env_encoder2conv_with_linear(envs_train[:32, :, :, :]).shape)\n","#summary(env_encoder2conv_with_linear, (1, 1, 640))"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ojZIx-6eTAHG","executionInfo":{"status":"ok","timestamp":1677676834657,"user_tz":-60,"elapsed":506,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"4bae9d86-bfd0-4ddc-9059-5b8a4e3a32c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 32])\n"]}],"source":["class EnvEncoder3ConvWithLinear(nn.Module):\n","\n","    def __init__(self,             \n","            fs = 128, # sampling rate\n","            T = 5, # lenght of each trial in seconds\n","            F1 = 4\n","        ):\n","        super(EnvEncoder3ConvWithLinear, self).__init__()\n","\n","        self.env_encoder = nn.Sequential(\n","            Conv2d(1, F1, (1, int(fs//2)), padding='same', bias=True),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1, (1, int(fs//4)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1, affine=True), ELU(), nn.AvgPool2d(1, 2), nn.Dropout(0.5),\n","            Conv2d(F1, F1*4, (1, int(fs//8)), padding='same', bias=False, groups=1),\n","            nn.BatchNorm2d(F1*4, affine=True), ELU(), nn.AvgPool2d(1, 8), nn.Dropout(0.5),\n","            nn.Flatten(),\n","            nn.Linear(F1*4*int((T*fs)//(4*8)), int(fs/4))\n","        ) \n","\n","    def forward(self, x):\n","        x = self.env_encoder(x)\n","        return x\n","\n","env_encoder3conv_with_linear = EnvEncoder3ConvWithLinear()\n","\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(env_encoder3conv_with_linear(envs_train[:32, :, :, :]).shape)\n","#summary(env_encoder3conv_with_linear, (1, 1, 640))"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1677676838456,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"Bp8HVS-aZQe4","outputId":"dce95cc9-1e09-47d7-ee64-2740108fc743"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CES()"]},"metadata":{},"execution_count":28}],"source":["class CES(nn.Module):\n","    def __init__(self, \n","                 eeg_encoder= None,\n","                 env_encoder = None): \n","        super().__init__()\n","\n","        self.eeg_encoder = eeg_encoder\n","        self.env_encoder = env_encoder\n","        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n","\n","    def encode_eeg(self, x):\n","        return self.eeg_encoder(x)\n","    \n","    def encode_env(self, x):\n","        return self.env_encoder(x)\n","    \n","    def forward(self, eeg, env):\n","        eeg_features = self.encode_eeg(eeg)\n","        env_features = self.encode_env(env)\n","        return eeg_features, env_features, self.logit_scale.exp()\n","  \n","\n","model = CES();\n","model.to(device)\n","#for n,p in model.named_parameters():\n","    #print(n, p.shape)\n"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kRK-Pq1VTAHG","executionInfo":{"status":"ok","timestamp":1677676852094,"user_tz":-60,"elapsed":3,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"}},"outputId":"c6e38bc1-8cf2-4c49-a9d6-41c458b9051d"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Models with no Linear Layers\n"," Models with Linear Layers\n"]}],"source":["print(\" Models with no Linear Layers\")\n","eeg_encoder_no_linear = EEGEncoderNoLinear()\n","env_encoder3conv_no_linear = EnvEncoder2ConvNoLinear()\n","ces_eeg_0lin_env_3conv_0lin = CES(eeg_encoder=eeg_encoder_no_linear.eeg_encoder, env_encoder=env_encoder2conv_no_linear.env_encoder)\n","#summary(ces_eeg_0lin_env_3conv_0lin, [(1, 128, 640), (1, 1, 640)])\n","\n","eeg_encoder_no_linear = EEGEncoderNoLinear()\n","env_encoder2conv_no_linear = EnvEncoder2ConvNoLinear()\n","ces_eeg_0lin_env_2conv_0lin = CES(eeg_encoder=eeg_encoder_no_linear.eeg_encoder, env_encoder=env_encoder2conv_no_linear.env_encoder)\n","#summary(ces_eeg_0lin_env_2conv_0lin, [(1, 128, 640), (1, 1, 640)])\n","\n","\n","print(\" Models with Linear Layers\")\n","eeg_encoder_with_linear = EEGEncoderWithLinear()\n","env_encoder3conv_with_linear = EnvEncoder2ConvWithLinear()\n","ces_eeg_1lin_env_3conv_1lin = CES(eeg_encoder=eeg_encoder_with_linear.eeg_encoder, env_encoder=env_encoder3conv_with_linear.env_encoder)\n","#summary(ces_eeg_1lin_env_3conv_1lin, [(1, 128, 640), (1, 1, 640)])\n","\n","eeg_encoder_with_linear = EEGEncoderWithLinear()\n","env_encoder2conv_with_linear = EnvEncoder2ConvWithLinear()\n","ces_eeg_1lin_env_2conv_1lin = CES(eeg_encoder=eeg_encoder_with_linear.eeg_encoder, env_encoder=env_encoder2conv_with_linear.env_encoder)\n","#summary(ces_eeg_1lin_env_2conv_1lin, [(1, 128, 640), (1, 1, 640)])\n","\n","models_name = [\"eeg0lin_env3conv0lin\", \"eeg0lin_env2conv0lin\", \"eeg1lin_env3conv1lin\", \"eeg1lin_env2conv1lin\"]\n","models_dict = {\"eeg0lin_env3conv0lin\": ces_eeg_0lin_env_3conv_0lin, \"eeg0lin_env2conv0lin\": ces_eeg_0lin_env_2conv_0lin, \n","               \"eeg1lin_env3conv1lin\": ces_eeg_1lin_env_3conv_1lin, \"eeg1lin_env2conv1lin\": ces_eeg_1lin_env_2conv_1lin}\n"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1903457,"status":"ok","timestamp":1677678782153,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"d4iJiosNZQe7","outputId":"465b7f78-d439-4dbb-c447-2c5a1ae98aef"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------New model: eeg0lin_env3conv0lin----------------------+\n"]},{"output_type":"display_data","data":{"text/plain":["Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["NumExpr defaulting to 8 threads.\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">NumExpr defaulting to 8 threads.\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["====== Epoch: 1\n","====> Validation loss: 3.2588,  X1 loss: 3.2384   X2 loss: 3.2792\n","====== Epoch: 2\n","====> Validation loss: 3.2573,  X1 loss: 3.2559   X2 loss: 3.2586\n","====== Epoch: 3\n","====> Validation loss: 3.2296,  X1 loss: 3.2194   X2 loss: 3.2397\n","====== Epoch: 4\n","====> Validation loss: 3.1470,  X1 loss: 3.1281   X2 loss: 3.1660\n","====== Epoch: 5\n","====> Validation loss: 3.0635,  X1 loss: 3.0351   X2 loss: 3.0919\n","====== Epoch: 6\n","====> Validation loss: 3.0362,  X1 loss: 3.0204   X2 loss: 3.0519\n","====== Epoch: 7\n","====> Validation loss: 3.0325,  X1 loss: 3.0180   X2 loss: 3.0470\n","====== Epoch: 8\n","====> Validation loss: 3.0200,  X1 loss: 3.0087   X2 loss: 3.0313\n","====== Epoch: 9\n","====> Validation loss: 2.9823,  X1 loss: 2.9603   X2 loss: 3.0044\n","====== Epoch: 10\n","====> Validation loss: 2.9753,  X1 loss: 2.9604   X2 loss: 2.9902\n","====== Epoch: 11\n","====> Validation loss: 3.0359,  X1 loss: 3.0277   X2 loss: 3.0441\n","====== Epoch: 12\n","====> Validation loss: 3.0435,  X1 loss: 3.0265   X2 loss: 3.0605\n","====== Epoch: 13\n","====> Validation loss: 2.9389,  X1 loss: 2.9221   X2 loss: 2.9557\n","====== Epoch: 14\n","====> Validation loss: 2.9226,  X1 loss: 2.9036   X2 loss: 2.9417\n","====== Epoch: 15\n","====> Validation loss: 3.0037,  X1 loss: 2.9955   X2 loss: 3.0120\n","====== Epoch: 16\n","====> Validation loss: 2.8616,  X1 loss: 2.8548   X2 loss: 2.8685\n","====== Epoch: 17\n","====> Validation loss: 2.9167,  X1 loss: 2.9054   X2 loss: 2.9280\n","====== Epoch: 18\n","====> Validation loss: 2.8958,  X1 loss: 2.8745   X2 loss: 2.9172\n","====== Epoch: 19\n","====> Validation loss: 2.8407,  X1 loss: 2.8343   X2 loss: 2.8471\n","====== Epoch: 20\n","====> Validation loss: 2.9581,  X1 loss: 2.9393   X2 loss: 2.9770\n","====== Epoch: 21\n","====> Validation loss: 2.9745,  X1 loss: 2.9399   X2 loss: 3.0091\n","====== Epoch: 22\n","====> Validation loss: 2.9845,  X1 loss: 2.9847   X2 loss: 2.9843\n","====== Epoch: 23\n","====> Validation loss: 2.9745,  X1 loss: 2.9551   X2 loss: 2.9938\n","====== Epoch: 24\n","====> Validation loss: 2.9936,  X1 loss: 2.9750   X2 loss: 3.0122\n","====== Epoch: 25\n","====> Validation loss: 2.9669,  X1 loss: 2.9381   X2 loss: 2.9958\n","====== Epoch: 26\n","====> Validation loss: 2.9405,  X1 loss: 2.9185   X2 loss: 2.9625\n","====== Epoch: 27\n","====> Validation loss: 2.9108,  X1 loss: 2.9022   X2 loss: 2.9194\n","====== Epoch: 28\n","====> Validation loss: 2.9665,  X1 loss: 2.9526   X2 loss: 2.9805\n","====== Epoch: 29\n","====> Validation loss: 2.9419,  X1 loss: 2.9299   X2 loss: 2.9540\n","====== Epoch: 30\n","====> Validation loss: 2.8884,  X1 loss: 2.8896   X2 loss: 2.8872\n","====== Epoch: 31\n","====> Validation loss: 2.8817,  X1 loss: 2.8673   X2 loss: 2.8962\n","====== Epoch: 32\n","====> Validation loss: 2.8565,  X1 loss: 2.8494   X2 loss: 2.8635\n","====== Epoch: 33\n","====> Validation loss: 2.9554,  X1 loss: 2.9344   X2 loss: 2.9765\n","====== Epoch: 34\n","====> Validation loss: 2.9213,  X1 loss: 2.9061   X2 loss: 2.9364\n","====== Epoch: 35\n","====> Validation loss: 2.9324,  X1 loss: 2.9268   X2 loss: 2.9380\n","====== Epoch: 36\n","====> Validation loss: 2.9446,  X1 loss: 2.9193   X2 loss: 2.9699\n","====== Epoch: 37\n","====> Validation loss: 2.8426,  X1 loss: 2.8340   X2 loss: 2.8512\n","====== Epoch: 38\n","====> Validation loss: 2.9857,  X1 loss: 2.9825   X2 loss: 2.9889\n","====== Epoch: 39\n","====> Validation loss: 2.8983,  X1 loss: 2.9113   X2 loss: 2.8853\n","====== Epoch: 40\n","====> Validation loss: 3.0178,  X1 loss: 3.0171   X2 loss: 3.0185\n","====== Epoch: 41\n","====> Validation loss: 2.9214,  X1 loss: 2.8835   X2 loss: 2.9593\n","====== Epoch: 42\n","====> Validation loss: 2.8592,  X1 loss: 2.8612   X2 loss: 2.8572\n","====== Epoch: 43\n","====> Validation loss: 2.9386,  X1 loss: 2.9298   X2 loss: 2.9474\n","====== Epoch: 44\n","====> Validation loss: 2.8993,  X1 loss: 2.8854   X2 loss: 2.9132\n","====== Epoch: 45\n","====> Validation loss: 2.9094,  X1 loss: 2.8777   X2 loss: 2.9410\n","====== Epoch: 46\n","====> Validation loss: 3.0087,  X1 loss: 2.9936   X2 loss: 3.0237\n","====== Epoch: 47\n","====> Validation loss: 2.9358,  X1 loss: 2.9194   X2 loss: 2.9521\n","====== Epoch: 48\n","====> Validation loss: 2.8710,  X1 loss: 2.8562   X2 loss: 2.8859\n","====== Epoch: 49\n","====> Validation loss: 2.9274,  X1 loss: 2.9181   X2 loss: 2.9368\n","====== Epoch: 50\n","====> Validation loss: 2.8642,  X1 loss: 2.8555   X2 loss: 2.8729\n","====== Epoch: 51\n","====> Validation loss: 2.8760,  X1 loss: 2.8776   X2 loss: 2.8744\n","====== Epoch: 52\n","====> Validation loss: 2.9522,  X1 loss: 2.9138   X2 loss: 2.9906\n","====== Epoch: 53\n","====> Validation loss: 2.9219,  X1 loss: 2.9040   X2 loss: 2.9398\n","====== Epoch: 54\n","====> Validation loss: 2.9333,  X1 loss: 2.9294   X2 loss: 2.9372\n","====== Epoch: 55\n","====> Validation loss: 2.8762,  X1 loss: 2.8555   X2 loss: 2.8968\n","====== Epoch: 56\n","====> Validation loss: 2.9839,  X1 loss: 2.9755   X2 loss: 2.9924\n","====== Epoch: 57\n","====> Validation loss: 2.9169,  X1 loss: 2.8922   X2 loss: 2.9417\n","====== Epoch: 58\n","====> Validation loss: 2.9359,  X1 loss: 2.9033   X2 loss: 2.9685\n","====== Epoch: 59\n","====> Validation loss: 2.9546,  X1 loss: 2.9362   X2 loss: 2.9729\n","====== Epoch: 60\n","====> Validation loss: 2.8325,  X1 loss: 2.8186   X2 loss: 2.8464\n","====== Epoch: 61\n","====> Validation loss: 2.9772,  X1 loss: 2.9817   X2 loss: 2.9727\n","====== Epoch: 62\n","====> Validation loss: 2.9169,  X1 loss: 2.9035   X2 loss: 2.9302\n","====== Epoch: 63\n","====> Validation loss: 2.9658,  X1 loss: 2.9704   X2 loss: 2.9612\n","====== Epoch: 64\n","====> Validation loss: 2.8617,  X1 loss: 2.8300   X2 loss: 2.8934\n","====== Epoch: 65\n","====> Validation loss: 2.8988,  X1 loss: 2.8866   X2 loss: 2.9109\n","====== Epoch: 66\n","====> Validation loss: 2.9898,  X1 loss: 2.9818   X2 loss: 2.9978\n","====== Epoch: 67\n","====> Validation loss: 2.9537,  X1 loss: 2.9511   X2 loss: 2.9564\n","====== Epoch: 68\n","====> Validation loss: 2.8818,  X1 loss: 2.8586   X2 loss: 2.9049\n","====== Epoch: 69\n","====> Validation loss: 2.8977,  X1 loss: 2.8897   X2 loss: 2.9057\n","====== Epoch: 70\n","====> Validation loss: 2.8571,  X1 loss: 2.8587   X2 loss: 2.8555\n","====== Epoch: 71\n","====> Validation loss: 2.9089,  X1 loss: 2.8828   X2 loss: 2.9350\n","====== Epoch: 72\n","====> Validation loss: 2.9895,  X1 loss: 2.9839   X2 loss: 2.9951\n","====== Epoch: 73\n","====> Validation loss: 2.9498,  X1 loss: 2.9431   X2 loss: 2.9565\n","====== Epoch: 74\n","====> Validation loss: 2.9298,  X1 loss: 2.9414   X2 loss: 2.9183\n","====== Epoch: 75\n","====> Validation loss: 2.8909,  X1 loss: 2.8948   X2 loss: 2.8870\n","====== Epoch: 76\n","====> Validation loss: 2.9261,  X1 loss: 2.9034   X2 loss: 2.9487\n","====== Epoch: 77\n","====> Validation loss: 2.8654,  X1 loss: 2.8695   X2 loss: 2.8614\n","====== Epoch: 78\n","====> Validation loss: 2.8577,  X1 loss: 2.8379   X2 loss: 2.8775\n","====== Epoch: 79\n","====> Validation loss: 2.8414,  X1 loss: 2.8459   X2 loss: 2.8370\n","====== Epoch: 80\n","====> Validation loss: 2.8971,  X1 loss: 2.9095   X2 loss: 2.8848\n","====== Epoch: 81\n","====> Validation loss: 2.8678,  X1 loss: 2.8532   X2 loss: 2.8823\n","====== Epoch: 82\n","====> Validation loss: 2.8454,  X1 loss: 2.8302   X2 loss: 2.8606\n","====== Epoch: 83\n","====> Validation loss: 2.8344,  X1 loss: 2.8331   X2 loss: 2.8357\n","====== Epoch: 84\n","====> Validation loss: 2.8768,  X1 loss: 2.8480   X2 loss: 2.9055\n","====== Epoch: 85\n","====> Validation loss: 2.9369,  X1 loss: 2.9255   X2 loss: 2.9483\n","====== Epoch: 86\n","====> Validation loss: 2.8824,  X1 loss: 2.8574   X2 loss: 2.9074\n","====== Epoch: 87\n","====> Validation loss: 2.8991,  X1 loss: 2.8829   X2 loss: 2.9154\n","====== Epoch: 88\n","====> Validation loss: 2.9530,  X1 loss: 2.9378   X2 loss: 2.9681\n","====== Epoch: 89\n","====> Validation loss: 2.8693,  X1 loss: 2.8402   X2 loss: 2.8984\n","====== Epoch: 90\n","====> Validation loss: 2.8627,  X1 loss: 2.8711   X2 loss: 2.8544\n","====== Epoch: 91\n","====> Validation loss: 2.8752,  X1 loss: 2.8542   X2 loss: 2.8962\n","====== Epoch: 92\n","====> Validation loss: 2.9822,  X1 loss: 2.9517   X2 loss: 3.0127\n","====== Epoch: 93\n","====> Validation loss: 2.8426,  X1 loss: 2.8389   X2 loss: 2.8462\n","====== Epoch: 94\n","====> Validation loss: 2.8392,  X1 loss: 2.8370   X2 loss: 2.8414\n","====== Epoch: 95\n","====> Validation loss: 2.9099,  X1 loss: 2.8893   X2 loss: 2.9305\n","====== Epoch: 96\n","====> Validation loss: 2.8776,  X1 loss: 2.8754   X2 loss: 2.8798\n","====== Epoch: 97\n","====> Validation loss: 2.8875,  X1 loss: 2.8718   X2 loss: 2.9032\n","====== Epoch: 98\n","====> Validation loss: 2.8816,  X1 loss: 2.8923   X2 loss: 2.8708\n","====== Epoch: 99\n","====> Validation loss: 2.8707,  X1 loss: 2.8642   X2 loss: 2.8772\n","+--------------New model: eeg0lin_env2conv0lin----------------------+\n","====== Epoch: 1\n","====> Validation loss: 3.4099,  X1 loss: 3.4050   X2 loss: 3.4148\n","====== Epoch: 2\n","====> Validation loss: 3.3019,  X1 loss: 3.3043   X2 loss: 3.2994\n","====== Epoch: 3\n","====> Validation loss: 3.1983,  X1 loss: 3.2071   X2 loss: 3.1895\n","====== Epoch: 4\n","====> Validation loss: 3.1607,  X1 loss: 3.1754   X2 loss: 3.1461\n","====== Epoch: 5\n","====> Validation loss: 3.1320,  X1 loss: 3.1307   X2 loss: 3.1332\n","====== Epoch: 6\n","====> Validation loss: 3.1008,  X1 loss: 3.0900   X2 loss: 3.1116\n","====== Epoch: 7\n","====> Validation loss: 3.1040,  X1 loss: 3.1078   X2 loss: 3.1001\n","====== Epoch: 8\n","====> Validation loss: 2.9767,  X1 loss: 2.9667   X2 loss: 2.9868\n","====== Epoch: 9\n","====> Validation loss: 2.9335,  X1 loss: 2.9282   X2 loss: 2.9389\n","====== Epoch: 10\n","====> Validation loss: 2.9770,  X1 loss: 2.9647   X2 loss: 2.9892\n","====== Epoch: 11\n","====> Validation loss: 2.8566,  X1 loss: 2.8457   X2 loss: 2.8675\n","====== Epoch: 12\n","====> Validation loss: 2.8767,  X1 loss: 2.8676   X2 loss: 2.8858\n","====== Epoch: 13\n","====> Validation loss: 2.9657,  X1 loss: 2.9618   X2 loss: 2.9695\n","====== Epoch: 14\n","====> Validation loss: 2.8984,  X1 loss: 2.8853   X2 loss: 2.9115\n","====== Epoch: 15\n","====> Validation loss: 2.8564,  X1 loss: 2.8459   X2 loss: 2.8669\n","====== Epoch: 16\n","====> Validation loss: 2.8730,  X1 loss: 2.8630   X2 loss: 2.8829\n","====== Epoch: 17\n","====> Validation loss: 2.8789,  X1 loss: 2.8438   X2 loss: 2.9139\n","====== Epoch: 18\n","====> Validation loss: 2.9435,  X1 loss: 2.9220   X2 loss: 2.9650\n","====== Epoch: 19\n","====> Validation loss: 2.9161,  X1 loss: 2.9042   X2 loss: 2.9280\n","====== Epoch: 20\n","====> Validation loss: 2.9079,  X1 loss: 2.9010   X2 loss: 2.9149\n","====== Epoch: 21\n","====> Validation loss: 2.8441,  X1 loss: 2.8437   X2 loss: 2.8444\n","====== Epoch: 22\n","====> Validation loss: 2.8643,  X1 loss: 2.8672   X2 loss: 2.8614\n","====== Epoch: 23\n","====> Validation loss: 2.9281,  X1 loss: 2.9185   X2 loss: 2.9377\n","====== Epoch: 24\n","====> Validation loss: 2.8935,  X1 loss: 2.8903   X2 loss: 2.8966\n","====== Epoch: 25\n","====> Validation loss: 2.9463,  X1 loss: 2.9296   X2 loss: 2.9630\n","====== Epoch: 26\n","====> Validation loss: 2.9712,  X1 loss: 2.9524   X2 loss: 2.9900\n","====== Epoch: 27\n","====> Validation loss: 2.8994,  X1 loss: 2.8832   X2 loss: 2.9156\n","====== Epoch: 28\n","====> Validation loss: 2.9221,  X1 loss: 2.9237   X2 loss: 2.9205\n","====== Epoch: 29\n","====> Validation loss: 2.9208,  X1 loss: 2.9044   X2 loss: 2.9372\n","====== Epoch: 30\n","====> Validation loss: 2.9270,  X1 loss: 2.9214   X2 loss: 2.9326\n","====== Epoch: 31\n","====> Validation loss: 2.8470,  X1 loss: 2.8361   X2 loss: 2.8579\n","====== Epoch: 32\n","====> Validation loss: 2.9394,  X1 loss: 2.9179   X2 loss: 2.9609\n","====== Epoch: 33\n","====> Validation loss: 2.9506,  X1 loss: 2.9435   X2 loss: 2.9576\n","====== Epoch: 34\n","====> Validation loss: 2.8232,  X1 loss: 2.8158   X2 loss: 2.8305\n","====== Epoch: 35\n","====> Validation loss: 2.8837,  X1 loss: 2.8667   X2 loss: 2.9006\n","====== Epoch: 36\n","====> Validation loss: 2.8766,  X1 loss: 2.8811   X2 loss: 2.8721\n","====== Epoch: 37\n","====> Validation loss: 2.8832,  X1 loss: 2.8892   X2 loss: 2.8772\n","====== Epoch: 38\n","====> Validation loss: 2.9280,  X1 loss: 2.9165   X2 loss: 2.9395\n","====== Epoch: 39\n","====> Validation loss: 2.9010,  X1 loss: 2.8955   X2 loss: 2.9064\n","====== Epoch: 40\n","====> Validation loss: 2.9379,  X1 loss: 2.9356   X2 loss: 2.9402\n","====== Epoch: 41\n","====> Validation loss: 2.8864,  X1 loss: 2.8721   X2 loss: 2.9006\n","====== Epoch: 42\n","====> Validation loss: 2.9904,  X1 loss: 2.9756   X2 loss: 3.0053\n","====== Epoch: 43\n","====> Validation loss: 3.0086,  X1 loss: 2.9916   X2 loss: 3.0257\n","====== Epoch: 44\n","====> Validation loss: 2.9580,  X1 loss: 2.9566   X2 loss: 2.9594\n","====== Epoch: 45\n","====> Validation loss: 2.9912,  X1 loss: 2.9916   X2 loss: 2.9908\n","====== Epoch: 46\n","====> Validation loss: 2.9821,  X1 loss: 2.9946   X2 loss: 2.9696\n","====== Epoch: 47\n","====> Validation loss: 2.9117,  X1 loss: 2.9049   X2 loss: 2.9184\n","====== Epoch: 48\n","====> Validation loss: 2.8870,  X1 loss: 2.8792   X2 loss: 2.8949\n","====== Epoch: 49\n","====> Validation loss: 2.9893,  X1 loss: 2.9868   X2 loss: 2.9919\n","====== Epoch: 50\n","====> Validation loss: 2.9111,  X1 loss: 2.8999   X2 loss: 2.9223\n","====== Epoch: 51\n","====> Validation loss: 2.9768,  X1 loss: 2.9684   X2 loss: 2.9852\n","====== Epoch: 52\n","====> Validation loss: 2.9827,  X1 loss: 2.9598   X2 loss: 3.0057\n","====== Epoch: 53\n","====> Validation loss: 2.9672,  X1 loss: 2.9662   X2 loss: 2.9682\n","====== Epoch: 54\n","====> Validation loss: 2.9815,  X1 loss: 2.9691   X2 loss: 2.9940\n","====== Epoch: 55\n","====> Validation loss: 2.8744,  X1 loss: 2.8875   X2 loss: 2.8612\n","====== Epoch: 56\n","====> Validation loss: 2.9215,  X1 loss: 2.8992   X2 loss: 2.9437\n","====== Epoch: 57\n","====> Validation loss: 2.9482,  X1 loss: 2.9266   X2 loss: 2.9698\n","====== Epoch: 58\n","====> Validation loss: 2.9410,  X1 loss: 2.9314   X2 loss: 2.9506\n","====== Epoch: 59\n","====> Validation loss: 2.9009,  X1 loss: 2.8815   X2 loss: 2.9202\n","====== Epoch: 60\n","====> Validation loss: 2.9445,  X1 loss: 2.9452   X2 loss: 2.9437\n","====== Epoch: 61\n","====> Validation loss: 2.8750,  X1 loss: 2.8669   X2 loss: 2.8830\n","====== Epoch: 62\n","====> Validation loss: 2.9725,  X1 loss: 2.9676   X2 loss: 2.9774\n","====== Epoch: 63\n","====> Validation loss: 2.9349,  X1 loss: 2.9231   X2 loss: 2.9466\n","====== Epoch: 64\n","====> Validation loss: 2.9867,  X1 loss: 2.9918   X2 loss: 2.9816\n","====== Epoch: 65\n","====> Validation loss: 2.9291,  X1 loss: 2.9417   X2 loss: 2.9166\n","====== Epoch: 66\n","====> Validation loss: 2.9815,  X1 loss: 2.9586   X2 loss: 3.0045\n","====== Epoch: 67\n","====> Validation loss: 2.9699,  X1 loss: 2.9859   X2 loss: 2.9538\n","====== Epoch: 68\n","====> Validation loss: 2.9669,  X1 loss: 2.9511   X2 loss: 2.9827\n","====== Epoch: 69\n","====> Validation loss: 2.9545,  X1 loss: 2.9786   X2 loss: 2.9305\n","====== Epoch: 70\n","====> Validation loss: 2.9624,  X1 loss: 2.9381   X2 loss: 2.9867\n","====== Epoch: 71\n","====> Validation loss: 2.9994,  X1 loss: 3.0096   X2 loss: 2.9892\n","====== Epoch: 72\n","====> Validation loss: 3.0121,  X1 loss: 3.0133   X2 loss: 3.0109\n","====== Epoch: 73\n","====> Validation loss: 2.9885,  X1 loss: 2.9556   X2 loss: 3.0215\n","====== Epoch: 74\n","====> Validation loss: 2.9988,  X1 loss: 2.9678   X2 loss: 3.0297\n","====== Epoch: 75\n","====> Validation loss: 3.0188,  X1 loss: 3.0108   X2 loss: 3.0268\n","====== Epoch: 76\n","====> Validation loss: 3.0142,  X1 loss: 2.9803   X2 loss: 3.0481\n","====== Epoch: 77\n","====> Validation loss: 2.9367,  X1 loss: 2.9478   X2 loss: 2.9256\n","====== Epoch: 78\n","====> Validation loss: 2.9023,  X1 loss: 2.9056   X2 loss: 2.8990\n","====== Epoch: 79\n","====> Validation loss: 2.9340,  X1 loss: 2.9236   X2 loss: 2.9443\n","====== Epoch: 80\n","====> Validation loss: 2.9446,  X1 loss: 2.9604   X2 loss: 2.9288\n","====== Epoch: 81\n","====> Validation loss: 2.9823,  X1 loss: 2.9754   X2 loss: 2.9891\n","====== Epoch: 82\n","====> Validation loss: 2.9439,  X1 loss: 2.9222   X2 loss: 2.9656\n","====== Epoch: 83\n","====> Validation loss: 3.0533,  X1 loss: 3.0286   X2 loss: 3.0779\n","====== Epoch: 84\n","====> Validation loss: 2.9106,  X1 loss: 2.9138   X2 loss: 2.9073\n","====== Epoch: 85\n","====> Validation loss: 3.0012,  X1 loss: 2.9898   X2 loss: 3.0127\n","====== Epoch: 86\n","====> Validation loss: 2.9735,  X1 loss: 2.9763   X2 loss: 2.9708\n","====== Epoch: 87\n","====> Validation loss: 2.9784,  X1 loss: 2.9639   X2 loss: 2.9928\n","====== Epoch: 88\n","====> Validation loss: 2.8867,  X1 loss: 2.8978   X2 loss: 2.8755\n","====== Epoch: 89\n","====> Validation loss: 2.9318,  X1 loss: 2.9160   X2 loss: 2.9477\n","====== Epoch: 90\n","====> Validation loss: 2.9118,  X1 loss: 2.8959   X2 loss: 2.9278\n","====== Epoch: 91\n","====> Validation loss: 2.9981,  X1 loss: 2.9603   X2 loss: 3.0360\n","====== Epoch: 92\n","====> Validation loss: 2.9569,  X1 loss: 2.9394   X2 loss: 2.9743\n","====== Epoch: 93\n","====> Validation loss: 2.9501,  X1 loss: 2.9229   X2 loss: 2.9774\n","====== Epoch: 94\n","====> Validation loss: 2.9519,  X1 loss: 2.9535   X2 loss: 2.9503\n","====== Epoch: 95\n","====> Validation loss: 2.8912,  X1 loss: 2.8923   X2 loss: 2.8900\n","====== Epoch: 96\n","====> Validation loss: 2.8755,  X1 loss: 2.8748   X2 loss: 2.8762\n","====== Epoch: 97\n","====> Validation loss: 2.9725,  X1 loss: 2.9709   X2 loss: 2.9741\n","====== Epoch: 98\n","====> Validation loss: 2.9712,  X1 loss: 2.9661   X2 loss: 2.9762\n","====== Epoch: 99\n","====> Validation loss: 2.9918,  X1 loss: 2.9644   X2 loss: 3.0193\n","+--------------New model: eeg1lin_env3conv1lin----------------------+\n","====== Epoch: 1\n","====> Validation loss: 3.6056,  X1 loss: 3.6349   X2 loss: 3.5763\n","====== Epoch: 2\n","====> Validation loss: 3.4607,  X1 loss: 3.4612   X2 loss: 3.4601\n","====== Epoch: 3\n","====> Validation loss: 3.3738,  X1 loss: 3.3729   X2 loss: 3.3748\n","====== Epoch: 4\n","====> Validation loss: 3.2841,  X1 loss: 3.2828   X2 loss: 3.2854\n","====== Epoch: 5\n","====> Validation loss: 3.2877,  X1 loss: 3.2829   X2 loss: 3.2924\n","====== Epoch: 6\n","====> Validation loss: 3.2180,  X1 loss: 3.2139   X2 loss: 3.2221\n","====== Epoch: 7\n","====> Validation loss: 3.1676,  X1 loss: 3.1630   X2 loss: 3.1721\n","====== Epoch: 8\n","====> Validation loss: 3.2252,  X1 loss: 3.2171   X2 loss: 3.2332\n","====== Epoch: 9\n","====> Validation loss: 3.1534,  X1 loss: 3.1461   X2 loss: 3.1606\n","====== Epoch: 10\n","====> Validation loss: 3.1059,  X1 loss: 3.0994   X2 loss: 3.1124\n","====== Epoch: 11\n","====> Validation loss: 3.1362,  X1 loss: 3.1283   X2 loss: 3.1440\n","====== Epoch: 12\n","====> Validation loss: 3.1160,  X1 loss: 3.1136   X2 loss: 3.1185\n","====== Epoch: 13\n","====> Validation loss: 3.1684,  X1 loss: 3.1617   X2 loss: 3.1751\n","====== Epoch: 14\n","====> Validation loss: 3.1910,  X1 loss: 3.1819   X2 loss: 3.2001\n","====== Epoch: 15\n","====> Validation loss: 3.1452,  X1 loss: 3.1428   X2 loss: 3.1477\n","====== Epoch: 16\n","====> Validation loss: 3.1478,  X1 loss: 3.1499   X2 loss: 3.1457\n","====== Epoch: 17\n","====> Validation loss: 3.1427,  X1 loss: 3.1432   X2 loss: 3.1422\n","====== Epoch: 18\n","====> Validation loss: 3.1263,  X1 loss: 3.1282   X2 loss: 3.1244\n","====== Epoch: 19\n","====> Validation loss: 3.1517,  X1 loss: 3.1459   X2 loss: 3.1574\n","====== Epoch: 20\n","====> Validation loss: 3.1688,  X1 loss: 3.1625   X2 loss: 3.1751\n","====== Epoch: 21\n","====> Validation loss: 3.1735,  X1 loss: 3.1661   X2 loss: 3.1810\n","====== Epoch: 22\n","====> Validation loss: 3.1428,  X1 loss: 3.1351   X2 loss: 3.1505\n","====== Epoch: 23\n","====> Validation loss: 3.1440,  X1 loss: 3.1393   X2 loss: 3.1487\n","====== Epoch: 24\n","====> Validation loss: 3.1520,  X1 loss: 3.1460   X2 loss: 3.1581\n","====== Epoch: 25\n","====> Validation loss: 3.2186,  X1 loss: 3.2190   X2 loss: 3.2183\n","====== Epoch: 26\n","====> Validation loss: 3.0948,  X1 loss: 3.0932   X2 loss: 3.0964\n","====== Epoch: 27\n","====> Validation loss: 3.2271,  X1 loss: 3.2181   X2 loss: 3.2361\n","====== Epoch: 28\n","====> Validation loss: 3.1804,  X1 loss: 3.1685   X2 loss: 3.1923\n","====== Epoch: 29\n","====> Validation loss: 3.1698,  X1 loss: 3.1694   X2 loss: 3.1702\n","====== Epoch: 30\n","====> Validation loss: 3.1875,  X1 loss: 3.1814   X2 loss: 3.1936\n","====== Epoch: 31\n","====> Validation loss: 3.2079,  X1 loss: 3.1956   X2 loss: 3.2203\n","====== Epoch: 32\n","====> Validation loss: 3.2423,  X1 loss: 3.2386   X2 loss: 3.2461\n","====== Epoch: 33\n","====> Validation loss: 3.2561,  X1 loss: 3.2463   X2 loss: 3.2659\n","====== Epoch: 34\n","====> Validation loss: 3.2778,  X1 loss: 3.2611   X2 loss: 3.2946\n","====== Epoch: 35\n","====> Validation loss: 3.3068,  X1 loss: 3.2969   X2 loss: 3.3167\n","====== Epoch: 36\n","====> Validation loss: 3.2076,  X1 loss: 3.2120   X2 loss: 3.2032\n","====== Epoch: 37\n","====> Validation loss: 3.1978,  X1 loss: 3.2088   X2 loss: 3.1868\n","====== Epoch: 38\n","====> Validation loss: 3.2813,  X1 loss: 3.2760   X2 loss: 3.2867\n","====== Epoch: 39\n","====> Validation loss: 3.2723,  X1 loss: 3.2694   X2 loss: 3.2752\n","====== Epoch: 40\n","====> Validation loss: 3.3178,  X1 loss: 3.3069   X2 loss: 3.3288\n","====== Epoch: 41\n","====> Validation loss: 3.2880,  X1 loss: 3.2868   X2 loss: 3.2892\n","====== Epoch: 42\n","====> Validation loss: 3.3300,  X1 loss: 3.3217   X2 loss: 3.3383\n","====== Epoch: 43\n","====> Validation loss: 3.3179,  X1 loss: 3.3156   X2 loss: 3.3202\n","====== Epoch: 44\n","====> Validation loss: 3.2902,  X1 loss: 3.2888   X2 loss: 3.2916\n","====== Epoch: 45\n","====> Validation loss: 3.3160,  X1 loss: 3.3066   X2 loss: 3.3253\n","====== Epoch: 46\n","====> Validation loss: 3.3334,  X1 loss: 3.3289   X2 loss: 3.3380\n","====== Epoch: 47\n","====> Validation loss: 3.3593,  X1 loss: 3.3574   X2 loss: 3.3613\n","====== Epoch: 48\n","====> Validation loss: 3.3423,  X1 loss: 3.3409   X2 loss: 3.3437\n","====== Epoch: 49\n","====> Validation loss: 3.4019,  X1 loss: 3.4121   X2 loss: 3.3916\n","====== Epoch: 50\n","====> Validation loss: 3.3725,  X1 loss: 3.3676   X2 loss: 3.3775\n","====== Epoch: 51\n","====> Validation loss: 3.3813,  X1 loss: 3.3776   X2 loss: 3.3850\n","====== Epoch: 52\n","====> Validation loss: 3.4081,  X1 loss: 3.4028   X2 loss: 3.4135\n","====== Epoch: 53\n","====> Validation loss: 3.4695,  X1 loss: 3.4824   X2 loss: 3.4566\n","====== Epoch: 54\n","====> Validation loss: 3.4860,  X1 loss: 3.4716   X2 loss: 3.5005\n","====== Epoch: 55\n","====> Validation loss: 3.4775,  X1 loss: 3.4665   X2 loss: 3.4885\n","====== Epoch: 56\n","====> Validation loss: 3.4501,  X1 loss: 3.4520   X2 loss: 3.4482\n","====== Epoch: 57\n","====> Validation loss: 3.4216,  X1 loss: 3.4129   X2 loss: 3.4302\n","====== Epoch: 58\n","====> Validation loss: 3.4562,  X1 loss: 3.4495   X2 loss: 3.4628\n","====== Epoch: 59\n","====> Validation loss: 3.4854,  X1 loss: 3.4702   X2 loss: 3.5006\n","====== Epoch: 60\n","====> Validation loss: 3.5060,  X1 loss: 3.5003   X2 loss: 3.5116\n","====== Epoch: 61\n","====> Validation loss: 3.5088,  X1 loss: 3.4960   X2 loss: 3.5217\n","====== Epoch: 62\n","====> Validation loss: 3.4572,  X1 loss: 3.4570   X2 loss: 3.4573\n","====== Epoch: 63\n","====> Validation loss: 3.5732,  X1 loss: 3.5642   X2 loss: 3.5821\n","====== Epoch: 64\n","====> Validation loss: 3.4896,  X1 loss: 3.4940   X2 loss: 3.4853\n","====== Epoch: 65\n","====> Validation loss: 3.5337,  X1 loss: 3.5384   X2 loss: 3.5290\n","====== Epoch: 66\n","====> Validation loss: 3.4741,  X1 loss: 3.4745   X2 loss: 3.4737\n","====== Epoch: 67\n","====> Validation loss: 3.5957,  X1 loss: 3.5961   X2 loss: 3.5953\n","====== Epoch: 68\n","====> Validation loss: 3.5859,  X1 loss: 3.5971   X2 loss: 3.5747\n","====== Epoch: 69\n","====> Validation loss: 3.5963,  X1 loss: 3.5960   X2 loss: 3.5967\n","====== Epoch: 70\n","====> Validation loss: 3.6126,  X1 loss: 3.5946   X2 loss: 3.6306\n","====== Epoch: 71\n","====> Validation loss: 3.5646,  X1 loss: 3.5542   X2 loss: 3.5750\n","====== Epoch: 72\n","====> Validation loss: 3.5247,  X1 loss: 3.5064   X2 loss: 3.5430\n","====== Epoch: 73\n","====> Validation loss: 3.6597,  X1 loss: 3.6420   X2 loss: 3.6774\n","====== Epoch: 74\n","====> Validation loss: 3.5538,  X1 loss: 3.5448   X2 loss: 3.5627\n","====== Epoch: 75\n","====> Validation loss: 3.6415,  X1 loss: 3.6376   X2 loss: 3.6454\n","====== Epoch: 76\n","====> Validation loss: 3.4881,  X1 loss: 3.4839   X2 loss: 3.4923\n","====== Epoch: 77\n","====> Validation loss: 3.5696,  X1 loss: 3.5573   X2 loss: 3.5818\n","====== Epoch: 78\n","====> Validation loss: 3.6252,  X1 loss: 3.6141   X2 loss: 3.6364\n","====== Epoch: 79\n","====> Validation loss: 3.5257,  X1 loss: 3.5270   X2 loss: 3.5244\n","====== Epoch: 80\n","====> Validation loss: 3.5987,  X1 loss: 3.5971   X2 loss: 3.6004\n","====== Epoch: 81\n","====> Validation loss: 3.5952,  X1 loss: 3.5913   X2 loss: 3.5992\n","====== Epoch: 82\n","====> Validation loss: 3.6271,  X1 loss: 3.6126   X2 loss: 3.6415\n","====== Epoch: 83\n","====> Validation loss: 3.6850,  X1 loss: 3.6692   X2 loss: 3.7009\n","====== Epoch: 84\n","====> Validation loss: 3.6151,  X1 loss: 3.6134   X2 loss: 3.6167\n","====== Epoch: 85\n","====> Validation loss: 3.6364,  X1 loss: 3.6310   X2 loss: 3.6418\n","====== Epoch: 86\n","====> Validation loss: 3.6134,  X1 loss: 3.6068   X2 loss: 3.6201\n","====== Epoch: 87\n","====> Validation loss: 3.6784,  X1 loss: 3.6806   X2 loss: 3.6761\n","====== Epoch: 88\n","====> Validation loss: 3.6378,  X1 loss: 3.6452   X2 loss: 3.6304\n","====== Epoch: 89\n","====> Validation loss: 3.6457,  X1 loss: 3.6254   X2 loss: 3.6660\n","====== Epoch: 90\n","====> Validation loss: 3.7099,  X1 loss: 3.7012   X2 loss: 3.7186\n","====== Epoch: 91\n","====> Validation loss: 3.6443,  X1 loss: 3.6533   X2 loss: 3.6353\n","====== Epoch: 92\n","====> Validation loss: 3.5828,  X1 loss: 3.5761   X2 loss: 3.5894\n","====== Epoch: 93\n","====> Validation loss: 3.7584,  X1 loss: 3.7413   X2 loss: 3.7755\n","====== Epoch: 94\n","====> Validation loss: 3.6975,  X1 loss: 3.6835   X2 loss: 3.7115\n","====== Epoch: 95\n","====> Validation loss: 3.6711,  X1 loss: 3.6776   X2 loss: 3.6645\n","====== Epoch: 96\n","====> Validation loss: 3.7442,  X1 loss: 3.7410   X2 loss: 3.7474\n","====== Epoch: 97\n","====> Validation loss: 3.7246,  X1 loss: 3.7272   X2 loss: 3.7219\n","====== Epoch: 98\n","====> Validation loss: 3.7378,  X1 loss: 3.7359   X2 loss: 3.7397\n","====== Epoch: 99\n","====> Validation loss: 3.6709,  X1 loss: 3.6578   X2 loss: 3.6840\n","+--------------New model: eeg1lin_env2conv1lin----------------------+\n","====== Epoch: 1\n","====> Validation loss: 3.6266,  X1 loss: 3.6284   X2 loss: 3.6249\n","====== Epoch: 2\n","====> Validation loss: 3.4867,  X1 loss: 3.4863   X2 loss: 3.4871\n","====== Epoch: 3\n","====> Validation loss: 3.4690,  X1 loss: 3.4688   X2 loss: 3.4692\n","====== Epoch: 4\n","====> Validation loss: 3.4312,  X1 loss: 3.4311   X2 loss: 3.4313\n","====== Epoch: 5\n","====> Validation loss: 3.3730,  X1 loss: 3.3734   X2 loss: 3.3725\n","====== Epoch: 6\n","====> Validation loss: 3.2803,  X1 loss: 3.2798   X2 loss: 3.2809\n","====== Epoch: 7\n","====> Validation loss: 3.2313,  X1 loss: 3.2287   X2 loss: 3.2340\n","====== Epoch: 8\n","====> Validation loss: 3.1988,  X1 loss: 3.1952   X2 loss: 3.2023\n","====== Epoch: 9\n","====> Validation loss: 3.2216,  X1 loss: 3.2209   X2 loss: 3.2223\n","====== Epoch: 10\n","====> Validation loss: 3.2223,  X1 loss: 3.2173   X2 loss: 3.2273\n","====== Epoch: 11\n","====> Validation loss: 3.1863,  X1 loss: 3.1861   X2 loss: 3.1865\n","====== Epoch: 12\n","====> Validation loss: 3.1961,  X1 loss: 3.1935   X2 loss: 3.1987\n","====== Epoch: 13\n","====> Validation loss: 3.1993,  X1 loss: 3.1903   X2 loss: 3.2083\n","====== Epoch: 14\n","====> Validation loss: 3.2161,  X1 loss: 3.2170   X2 loss: 3.2152\n","====== Epoch: 15\n","====> Validation loss: 3.1727,  X1 loss: 3.1726   X2 loss: 3.1728\n","====== Epoch: 16\n","====> Validation loss: 3.1994,  X1 loss: 3.1955   X2 loss: 3.2032\n","====== Epoch: 17\n","====> Validation loss: 3.2064,  X1 loss: 3.2079   X2 loss: 3.2049\n","====== Epoch: 18\n","====> Validation loss: 3.2195,  X1 loss: 3.2194   X2 loss: 3.2195\n","====== Epoch: 19\n","====> Validation loss: 3.2023,  X1 loss: 3.1953   X2 loss: 3.2093\n","====== Epoch: 20\n","====> Validation loss: 3.2044,  X1 loss: 3.2063   X2 loss: 3.2025\n","====== Epoch: 21\n","====> Validation loss: 3.2232,  X1 loss: 3.2203   X2 loss: 3.2260\n","====== Epoch: 22\n","====> Validation loss: 3.2034,  X1 loss: 3.2019   X2 loss: 3.2048\n","====== Epoch: 23\n","====> Validation loss: 3.2695,  X1 loss: 3.2696   X2 loss: 3.2695\n","====== Epoch: 24\n","====> Validation loss: 3.2895,  X1 loss: 3.2836   X2 loss: 3.2954\n","====== Epoch: 25\n","====> Validation loss: 3.2352,  X1 loss: 3.2385   X2 loss: 3.2320\n","====== Epoch: 26\n","====> Validation loss: 3.2585,  X1 loss: 3.2575   X2 loss: 3.2595\n","====== Epoch: 27\n","====> Validation loss: 3.3373,  X1 loss: 3.3334   X2 loss: 3.3411\n","====== Epoch: 28\n","====> Validation loss: 3.2912,  X1 loss: 3.2840   X2 loss: 3.2984\n","====== Epoch: 29\n","====> Validation loss: 3.3619,  X1 loss: 3.3608   X2 loss: 3.3631\n","====== Epoch: 30\n","====> Validation loss: 3.3764,  X1 loss: 3.3724   X2 loss: 3.3804\n","====== Epoch: 31\n","====> Validation loss: 3.3519,  X1 loss: 3.3443   X2 loss: 3.3594\n","====== Epoch: 32\n","====> Validation loss: 3.3161,  X1 loss: 3.3119   X2 loss: 3.3203\n","====== Epoch: 33\n","====> Validation loss: 3.3059,  X1 loss: 3.3042   X2 loss: 3.3076\n","====== Epoch: 34\n","====> Validation loss: 3.3286,  X1 loss: 3.3293   X2 loss: 3.3280\n","====== Epoch: 35\n","====> Validation loss: 3.3268,  X1 loss: 3.3276   X2 loss: 3.3260\n","====== Epoch: 36\n","====> Validation loss: 3.3577,  X1 loss: 3.3413   X2 loss: 3.3740\n","====== Epoch: 37\n","====> Validation loss: 3.4459,  X1 loss: 3.4458   X2 loss: 3.4460\n","====== Epoch: 38\n","====> Validation loss: 3.3942,  X1 loss: 3.3889   X2 loss: 3.3995\n","====== Epoch: 39\n","====> Validation loss: 3.3895,  X1 loss: 3.3862   X2 loss: 3.3927\n","====== Epoch: 40\n","====> Validation loss: 3.4002,  X1 loss: 3.3929   X2 loss: 3.4076\n","====== Epoch: 41\n","====> Validation loss: 3.3815,  X1 loss: 3.3778   X2 loss: 3.3852\n","====== Epoch: 42\n","====> Validation loss: 3.3478,  X1 loss: 3.3384   X2 loss: 3.3572\n","====== Epoch: 43\n","====> Validation loss: 3.4224,  X1 loss: 3.4078   X2 loss: 3.4371\n","====== Epoch: 44\n","====> Validation loss: 3.4173,  X1 loss: 3.4207   X2 loss: 3.4138\n","====== Epoch: 45\n","====> Validation loss: 3.4150,  X1 loss: 3.4029   X2 loss: 3.4271\n","====== Epoch: 46\n","====> Validation loss: 3.4518,  X1 loss: 3.4460   X2 loss: 3.4575\n","====== Epoch: 47\n","====> Validation loss: 3.3825,  X1 loss: 3.3810   X2 loss: 3.3840\n","====== Epoch: 48\n","====> Validation loss: 3.4081,  X1 loss: 3.4051   X2 loss: 3.4111\n","====== Epoch: 49\n","====> Validation loss: 3.4096,  X1 loss: 3.4066   X2 loss: 3.4125\n","====== Epoch: 50\n","====> Validation loss: 3.4259,  X1 loss: 3.4237   X2 loss: 3.4281\n","====== Epoch: 51\n","====> Validation loss: 3.4169,  X1 loss: 3.4203   X2 loss: 3.4134\n","====== Epoch: 52\n","====> Validation loss: 3.4638,  X1 loss: 3.4542   X2 loss: 3.4733\n","====== Epoch: 53\n","====> Validation loss: 3.4647,  X1 loss: 3.4529   X2 loss: 3.4766\n","====== Epoch: 54\n","====> Validation loss: 3.5022,  X1 loss: 3.4957   X2 loss: 3.5088\n","====== Epoch: 55\n","====> Validation loss: 3.5349,  X1 loss: 3.5350   X2 loss: 3.5348\n","====== Epoch: 56\n","====> Validation loss: 3.5136,  X1 loss: 3.5111   X2 loss: 3.5161\n","====== Epoch: 57\n","====> Validation loss: 3.4919,  X1 loss: 3.4799   X2 loss: 3.5039\n","====== Epoch: 58\n","====> Validation loss: 3.4575,  X1 loss: 3.4618   X2 loss: 3.4532\n","====== Epoch: 59\n","====> Validation loss: 3.5435,  X1 loss: 3.5338   X2 loss: 3.5532\n","====== Epoch: 60\n","====> Validation loss: 3.5077,  X1 loss: 3.5123   X2 loss: 3.5031\n","====== Epoch: 61\n","====> Validation loss: 3.5262,  X1 loss: 3.5267   X2 loss: 3.5256\n","====== Epoch: 62\n","====> Validation loss: 3.4657,  X1 loss: 3.4689   X2 loss: 3.4624\n","====== Epoch: 63\n","====> Validation loss: 3.4919,  X1 loss: 3.4819   X2 loss: 3.5018\n","====== Epoch: 64\n","====> Validation loss: 3.5568,  X1 loss: 3.5618   X2 loss: 3.5517\n","====== Epoch: 65\n","====> Validation loss: 3.5441,  X1 loss: 3.5372   X2 loss: 3.5510\n","====== Epoch: 66\n","====> Validation loss: 3.6119,  X1 loss: 3.6007   X2 loss: 3.6230\n","====== Epoch: 67\n","====> Validation loss: 3.5383,  X1 loss: 3.5414   X2 loss: 3.5353\n","====== Epoch: 68\n","====> Validation loss: 3.5974,  X1 loss: 3.5885   X2 loss: 3.6062\n","====== Epoch: 69\n","====> Validation loss: 3.5446,  X1 loss: 3.5489   X2 loss: 3.5403\n","====== Epoch: 70\n","====> Validation loss: 3.5137,  X1 loss: 3.5039   X2 loss: 3.5235\n","====== Epoch: 71\n","====> Validation loss: 3.5334,  X1 loss: 3.5271   X2 loss: 3.5396\n","====== Epoch: 72\n","====> Validation loss: 3.4900,  X1 loss: 3.4912   X2 loss: 3.4888\n","====== Epoch: 73\n","====> Validation loss: 3.5172,  X1 loss: 3.5260   X2 loss: 3.5084\n","====== Epoch: 74\n","====> Validation loss: 3.5918,  X1 loss: 3.5824   X2 loss: 3.6013\n","====== Epoch: 75\n","====> Validation loss: 3.5845,  X1 loss: 3.5913   X2 loss: 3.5778\n","====== Epoch: 76\n","====> Validation loss: 3.5269,  X1 loss: 3.5155   X2 loss: 3.5384\n","====== Epoch: 77\n","====> Validation loss: 3.5548,  X1 loss: 3.5574   X2 loss: 3.5523\n","====== Epoch: 78\n","====> Validation loss: 3.5828,  X1 loss: 3.5789   X2 loss: 3.5867\n","====== Epoch: 79\n","====> Validation loss: 3.6676,  X1 loss: 3.6640   X2 loss: 3.6712\n","====== Epoch: 80\n","====> Validation loss: 3.5427,  X1 loss: 3.5372   X2 loss: 3.5482\n","====== Epoch: 81\n","====> Validation loss: 3.5542,  X1 loss: 3.5541   X2 loss: 3.5542\n","====== Epoch: 82\n","====> Validation loss: 3.6137,  X1 loss: 3.6099   X2 loss: 3.6174\n","====== Epoch: 83\n","====> Validation loss: 3.6172,  X1 loss: 3.6075   X2 loss: 3.6269\n","====== Epoch: 84\n","====> Validation loss: 3.5685,  X1 loss: 3.5581   X2 loss: 3.5789\n","====== Epoch: 85\n","====> Validation loss: 3.6046,  X1 loss: 3.6045   X2 loss: 3.6046\n","====== Epoch: 86\n","====> Validation loss: 3.6028,  X1 loss: 3.5996   X2 loss: 3.6059\n","====== Epoch: 87\n","====> Validation loss: 3.6423,  X1 loss: 3.6292   X2 loss: 3.6554\n","====== Epoch: 88\n","====> Validation loss: 3.5677,  X1 loss: 3.5599   X2 loss: 3.5755\n","====== Epoch: 89\n","====> Validation loss: 3.6173,  X1 loss: 3.6150   X2 loss: 3.6195\n","====== Epoch: 90\n","====> Validation loss: 3.5900,  X1 loss: 3.5760   X2 loss: 3.6041\n","====== Epoch: 91\n","====> Validation loss: 3.6643,  X1 loss: 3.6537   X2 loss: 3.6749\n","====== Epoch: 92\n","====> Validation loss: 3.6565,  X1 loss: 3.6472   X2 loss: 3.6659\n","====== Epoch: 93\n","====> Validation loss: 3.5431,  X1 loss: 3.5429   X2 loss: 3.5433\n","====== Epoch: 94\n","====> Validation loss: 3.6237,  X1 loss: 3.6144   X2 loss: 3.6330\n","====== Epoch: 95\n","====> Validation loss: 3.6569,  X1 loss: 3.6367   X2 loss: 3.6771\n","====== Epoch: 96\n","====> Validation loss: 3.6019,  X1 loss: 3.6058   X2 loss: 3.5980\n","====== Epoch: 97\n","====> Validation loss: 3.5928,  X1 loss: 3.5889   X2 loss: 3.5967\n","====== Epoch: 98\n","====> Validation loss: 3.6602,  X1 loss: 3.6465   X2 loss: 3.6739\n","====== Epoch: 99\n","====> Validation loss: 3.5756,  X1 loss: 3.5686   X2 loss: 3.5826\n"]}],"source":["\n","lossi = []\n","udri = [] # update / data ratio \n","ud = []\n","\n","lr = 0.001\n","\n","for name, model in models_dict.items():\n","\n","    # Reset for the new model in the loop\n","    print(f\"+--------------New model: {name}----------------------+\")\n","    writer = SummaryWriter(log_dir=f\"runs/{name}_{time.strftime('%Y%m%d_%H%M%S')}\")\n","    model.to(device)\n","    optimizer = optim.NAdam(model.parameters(), lr=lr)\n","    cnt = 0\n","    loss_batches = []\n","\n","\n","    for epoch in range(1, 100):\n","\n","        print(f\"====== Epoch: {epoch}\")\n","\n","        model.train()\n","        for ix_batch, (Xb_eeg, Xb_env) in enumerate(dataloader):\n","\n","            # send to device\n","            Xb_eeg = Xb_eeg.to(device)\n","            Xb_env = Xb_env.to(device)\n","\n","            # Zero out gradients\n","            optimizer.zero_grad()\n","\n","            # forward pass\n","            eeg_features, env_features, logit_scale = model(Xb_eeg, Xb_env) \n","\n","\n","            # normalize features\n","            eeg_features_n = eeg_features / eeg_features.norm(dim=1, keepdim=True)\n","            env_features_n = env_features / env_features.norm(dim=1, keepdim=True)\n","\n","            # logits\n","            logits_per_eeg = logit_scale * eeg_features_n @ env_features_n.t()\n","            logits_per_env = logits_per_eeg.t()\n","\n","            #loss function\n","            labels = torch.arange(batch_size).to(device)\n","            loss_eeg = F.cross_entropy(logits_per_eeg, labels)\n","            loss_env = F.cross_entropy(logits_per_env, labels)\n","            loss   = (loss_eeg + loss_env)/2\n","\n","            # backward pass\n","            loss.backward()\n","            optimizer.step()\n","\n","            loss_batches.append(loss.item())\n","            cnt += 1\n","\n","            with torch.no_grad():\n","                #ud = {f\"p{ix}\":(lr*p.grad.std() / p.data.std()).log10().item() for ix, p in enumerate(model.parameters()) if p.ndim==4 }\n","                #writer.add_scalars('UpdateOData/ud', ud, cnt)\n","                writer.add_scalar('Loss/train_batch', loss.item(), cnt)\n","            \n","            #break   \n","\n","        loss_epoch = loss_batches[-(ix_batch + 1):]  # mean loss across batches\n","        loss_epoch = sum(loss_epoch) / len(loss_epoch)\n","        writer.add_scalar('Loss/train_epoch', loss_epoch, epoch)\n","        #for pname, p in model.named_parameters():\n","        #writer.add_histogram(f'Params/{pname}', p, epoch)\n","        #writer.add_histogram(f'Grads/{pname}', p.grad, epoch)\n","\n","        loss_val, *_ = eval_model_cl(dl_val, model, device=device)\n","        writer.add_scalar('Loss/val_epoch', loss_val, epoch)\n","\n","        \n","        # normalize weights\n","        with torch.no_grad():\n","            normalize_weights_eegnet(model.eeg_encoder)\n","\n","        model.train()\n","            \n","    #break   \n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"13Poz5tFnEFXpegMbhqAinRdIfSvPnPge","timestamp":1677524195292}]},"gpuClass":"premium","kernelspec":{"display_name":"mne","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"8e19e54895c02f0e9343d0fbd6cee45458aaf6f05de9ab3004d10bba5525a5d0"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"d3bc19eeda074194b3ced45c8d7c7045":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_a7cbff604a0b4e898d8cda745ebe3a81","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"     \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">     <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">7.6/7.6 MB</span> <span style=\"color: #800000; text-decoration-color: #800000\">82.5 MB/s</span> eta <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span>\n</pre>\n"},"metadata":{}}]}},"a7cbff604a0b4e898d8cda745ebe3a81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}