{"cells":[{"cell_type":"markdown","metadata":{"id":"aXEXbz7ENjUM"},"source":["## Main code"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1677770794962,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"EOmC9QbE-kZz"},"outputs":[],"source":["#%reset"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5065,"status":"ok","timestamp":1677770800021,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"gGggzCoKZQeu","outputId":"520b7a55-a9ff-4ea6-9339-951589cd19bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["import os, sys, glob\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","import numpy as np\n","\n","import mne\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import time\n","\n","from torchsummary import summary\n","from torch.utils.tensorboard import SummaryWriter\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","from tools.train import eval_model_cl\n","from tools.data import unfold_raw, rm_repeated_annotations\n"]},{"cell_type":"markdown","metadata":{"id":"q4sGJAdFZQez"},"source":["## Read Data"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1677770800022,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"KdQFfHcJZQe0","outputId":"277bf1f6-a0d2-4ab0-d473-164315b2b943"},"outputs":[{"name":"stdout","output_type":"stream","text":["-------------------------------------\n","window_size: 640  stride_size_test: 640\n","data_path: ../outputs/rochester_data/natural_speech\n"]}],"source":["subj_ids = [1] #list(range(1, 20))\n","fs = 128\n","window_size = int(5 * fs)\n","stride_size_train, stride_size_val, stride_size_test = int(2.5 * fs), int(5 * fs), int(5 * fs)\n","n_channs = 129 # 128 for eeg, 1 for env\n","batch_size = int(32)\n","print('-------------------------------------')\n","print(f'window_size: {window_size}  stride_size_test: {stride_size_test}')\n","\n","dataset_name = ['rochester_data', 'natural_speech']\n","outputs_path = f'../outputs/'\n","data_path = os.path.join(outputs_path, dataset_name[0], dataset_name[1])\n","after_ica_path = os.path.join(data_path, 'after_ica_raw')\n","print(f'data_path: {data_path}')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Opening raw data file ../outputs/rochester_data/natural_speech/after_ica_raw/subj_1_after_ica_raw.fif...\n","    Range : 0 ... 464571 =      0.000 ...  3629.461 secs\n","Ready.\n","Reading 0 ... 464571  =      0.000 ...  3629.461 secs...\n","Initial num of annots: 48  Num of removed annots: 19  Num of retained annots:  29\n","-------------------------------------\n","N train: 26  N val: 1  N test: 1\n","Shape Trian: torch.Size([1204, 1, 129, 640])  Shape Val: torch.Size([36, 1, 129, 640])  Shape Test: torch.Size([36, 1, 129, 640])\n","-------------------------------------\n","Shape EEG Train: torch.Size([1204, 1, 128, 640])  Val: torch.Size([36, 1, 128, 640])  Test: torch.Size([36, 1, 128, 640])\n","Mean: 9.050915399100301e-12  Std: 5.065590812591836e-06\n","Shape Env Train: torch.Size([1204, 1, 1, 640])  Val: torch.Size([36, 1, 1, 640])  Test: torch.Size([36, 1, 1, 640])\n","Mean Env: 2.364877700805664  Std Env: 2.5979256629943848\n"]}],"source":["def load_data(subject_ids, subjects_dir):\n","\n","    raws_train_windowed, raws_val_windowed, raws_test_windowed = [], [], []\n","\n","    for subj_id in subj_ids:\n","        \n","\n","        # load subject raw MNE object\n","        raw = mne.io.read_raw(os.path.join(after_ica_path, f'subj_{subj_id}_after_ica_raw.fif'), preload=True)\n","        # drop M1 and M2 channels\n","        raw.drop_channels(['M1', 'M2'])\n","        assert raw.info['nchan'] == n_channs\n","\n","        raw = rm_repeated_annotations(raw)\n","        annots = raw.annotations.copy()\n","        raw_split = [raw.copy().crop(t1, t2) for t1, t2 in zip(annots.onset[:-1]+annots.duration[:-1], annots.onset[1:])]\n","\n","        # Pick the split with the longest duration for validation, supposedly less noisy\n","        ix_val = np.argmax([i.get_data().shape[1] for i in raw_split])\n","        raw_val = [raw_split.pop(ix_val)] # create a list to make it iterable. later may be used for multiple splits\n","\n","        # Pick the next split with the longest duration for testing, supposedly less noisy\n","        ix_test = np.argmax([i.get_data().shape[1] for i in raw_split])\n","        raw_test = [raw_split.pop(ix_test)]\n","        \n","        # creat list of unfolded tensor raw objects\n","        fs = raw.info['sfreq']\n","        raws_train_windowed.extend([unfold_raw(i, window_size=window_size, stride=stride_size_train) for i in raw_split if i.get_data().shape[1] > window_size])\n","        raws_val_windowed.extend([unfold_raw(i, window_size=window_size, stride=stride_size_val) for i in raw_val if i.get_data().shape[1] > window_size])\n","        raws_test_windowed.extend([unfold_raw(i, window_size=window_size, stride=stride_size_test) for i in raw_test if i.get_data().shape[1] > window_size])\n","        print(\"-------------------------------------\")\n","        print('N train: %d  N val: %d  N test: %d' % (len(raws_train_windowed), len(raws_val_windowed), len(raws_test_windowed)))\n","\n","    # concatenate all in second dimension\n","    sigs_train = torch.cat(raws_train_windowed, dim=1).permute(1, 0, 2, 3)\n","    sigs_val = torch.cat(raws_val_windowed, dim=1).permute(1, 0, 2, 3)\n","    sigs_test = torch.cat(raws_test_windowed, dim=1).permute(1, 0, 2, 3)\n","    print(f\"Shape Trian: {sigs_train.shape}  Shape Val: {sigs_val.shape}  Shape Test: {sigs_test.shape}\")\n","\n","    eegs_train = sigs_train[:, :, :-1, :]\n","    eegs_val = sigs_val[:, :, :-1, :]\n","    eegs_test = sigs_test[:, :, :-1, :]\n","    print(\"-------------------------------------\")\n","    print(f\"Shape EEG Train: {eegs_train.shape}  Val: {eegs_val.shape}  Test: {eegs_test.shape}\")\n","\n","    # To avoid information leakage, we estimate the mean and std from the training set only.\n","    mean_eeg_train =  eegs_train.mean()\n","    std_eeg_train = eegs_train.std()\n","    print(f\"Mean: {mean_eeg_train}  Std: {std_eeg_train}\")\n","\n","    envs_train = sigs_train[:, :, [-1], :]\n","    envs_val = sigs_val[:, :, [-1], :]\n","    envs_test = sigs_test[:, :, [-1], :]\n","    print(f\"Shape Env Train: {envs_train.shape}  Val: {envs_val.shape}  Test: {envs_test.shape}\")\n","\n","    # Estimate mean and std of the Envelope data set\n","    mean_env_train =  envs_train.mean()\n","    std_env_train = envs_train.std()\n","    print(f\"Mean Env: {mean_env_train}  Std Env: {std_env_train}\")\n","\n","    # Normalize the data\n","    eegs_train = (eegs_train - mean_eeg_train) / std_eeg_train\n","    eegs_val = (eegs_val - mean_eeg_train) / std_eeg_train\n","    eegs_test = (eegs_test - mean_eeg_train) / std_eeg_train\n","\n","    envs_train = (envs_train - mean_env_train) / std_env_train\n","    envs_val = (envs_val - mean_env_train) / std_env_train\n","    envs_test = (envs_test - mean_env_train) / std_env_train\n","\n","    return {'eegs_train': eegs_train, 'eegs_val': eegs_val, 'eegs_test': eegs_test, \n","            'envs_train': envs_train, 'envs_val': envs_val, 'envs_test': envs_test}\n","\n","X = load_data(subj_ids, after_ica_path)"]},{"cell_type":"markdown","metadata":{"id":"tesRTfOdZQe2"},"source":["### Pytorch dataloader"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1677771078458,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"HviGOmH1ZQe2"},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, eeg, env):\n","        self.eeg = eeg\n","        self.env = env\n","    \n","    def __getitem__(self, index):\n","        return self.eeg[index], self.env[index]\n","    \n","    def __len__(self):\n","        return len(self.eeg)\n","    \n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["dataset_train = MyDataset(X['eegs_train'], X['envs_train'])\n","dl_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)\n","dl_val = DataLoader(MyDataset(X[\"eegs_val\"], X[\"envs_val\"]), batch_size=batch_size, shuffle=True, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"XMjI2NeFZQe3"},"source":["## Model"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([32, 320])\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 8, 128, 640]             520\n","       BatchNorm2d-2          [-1, 8, 128, 640]              16\n","            Conv2d-3           [-1, 16, 1, 640]           2,048\n","       BatchNorm2d-4           [-1, 16, 1, 640]              32\n","               ELU-5           [-1, 16, 1, 640]               0\n","         AvgPool2d-6           [-1, 16, 1, 160]               0\n","           Dropout-7           [-1, 16, 1, 160]               0\n","            Conv2d-8           [-1, 16, 1, 160]             256\n","            Conv2d-9           [-1, 16, 1, 160]             256\n","      BatchNorm2d-10           [-1, 16, 1, 160]              32\n","              ELU-11           [-1, 16, 1, 160]               0\n","        AvgPool2d-12            [-1, 16, 1, 20]               0\n","          Dropout-13            [-1, 16, 1, 20]               0\n","          Flatten-14                  [-1, 320]               0\n","================================================================\n","Total params: 3,160\n","Trainable params: 3,160\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.31\n","Forward/backward pass size (MB): 10.36\n","Params size (MB): 0.01\n","Estimated Total Size (MB): 10.68\n","----------------------------------------------------------------\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 4, 1, 640]             260\n","       BatchNorm2d-2            [-1, 4, 1, 640]               8\n","               ELU-3            [-1, 4, 1, 640]               0\n","         AvgPool2d-4            [-1, 4, 1, 320]               0\n","           Dropout-5            [-1, 4, 1, 320]               0\n","            Conv2d-6            [-1, 4, 1, 320]             512\n","       BatchNorm2d-7            [-1, 4, 1, 320]               8\n","               ELU-8            [-1, 4, 1, 320]               0\n","         AvgPool2d-9            [-1, 4, 1, 160]               0\n","          Dropout-10            [-1, 4, 1, 160]               0\n","           Conv2d-11           [-1, 16, 1, 160]           1,024\n","      BatchNorm2d-12           [-1, 16, 1, 160]              32\n","              ELU-13           [-1, 16, 1, 160]               0\n","        AvgPool2d-14            [-1, 16, 1, 20]               0\n","          Dropout-15            [-1, 16, 1, 20]               0\n","          Flatten-16                  [-1, 320]               0\n","================================================================\n","Total params: 1,844\n","Trainable params: 1,844\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.18\n","Params size (MB): 0.01\n","Estimated Total Size (MB): 0.19\n","----------------------------------------------------------------\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 8, 128, 640]             520\n","       BatchNorm2d-2          [-1, 8, 128, 640]              16\n","            Conv2d-3           [-1, 16, 1, 640]           2,048\n","       BatchNorm2d-4           [-1, 16, 1, 640]              32\n","               ELU-5           [-1, 16, 1, 640]               0\n","         AvgPool2d-6           [-1, 16, 1, 160]               0\n","           Dropout-7           [-1, 16, 1, 160]               0\n","            Conv2d-8           [-1, 16, 1, 160]             256\n","            Conv2d-9           [-1, 16, 1, 160]             256\n","      BatchNorm2d-10           [-1, 16, 1, 160]              32\n","              ELU-11           [-1, 16, 1, 160]               0\n","        AvgPool2d-12            [-1, 16, 1, 20]               0\n","          Dropout-13            [-1, 16, 1, 20]               0\n","          Flatten-14                  [-1, 320]               0\n","       EEGEncoder-15                  [-1, 320]               0\n","           Conv2d-16            [-1, 4, 1, 640]             260\n","      BatchNorm2d-17            [-1, 4, 1, 640]               8\n","              ELU-18            [-1, 4, 1, 640]               0\n","        AvgPool2d-19            [-1, 4, 1, 320]               0\n","          Dropout-20            [-1, 4, 1, 320]               0\n","           Conv2d-21            [-1, 4, 1, 320]             512\n","      BatchNorm2d-22            [-1, 4, 1, 320]               8\n","              ELU-23            [-1, 4, 1, 320]               0\n","        AvgPool2d-24            [-1, 4, 1, 160]               0\n","          Dropout-25            [-1, 4, 1, 160]               0\n","           Conv2d-26           [-1, 16, 1, 160]           1,024\n","      BatchNorm2d-27           [-1, 16, 1, 160]              32\n","              ELU-28           [-1, 16, 1, 160]               0\n","        AvgPool2d-29            [-1, 16, 1, 20]               0\n","          Dropout-30            [-1, 16, 1, 20]               0\n","          Flatten-31                  [-1, 320]               0\n","  EnvelopeEncoder-32                  [-1, 320]               0\n","================================================================\n","Total params: 5,004\n","Trainable params: 5,004\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 200.00\n","Forward/backward pass size (MB): 10.55\n","Params size (MB): 0.02\n","Estimated Total Size (MB): 210.57\n","----------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["/Users/keyvan.mahjoory/opt/anaconda3/envs/mne/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525699189/work/aten/src/ATen/native/Convolution.cpp:896.)\n","  return F.conv2d(input, weight, bias, self.stride,\n"]}],"source":["import torch\n","from models.eeg_encoder import EEGEncoder\n","from models.envelope_encoder import EnvelopeEncoder\n","from models.contrastive_eeg_speech import CLEE\n","eeg_encoder = EEGEncoder()\n","env_encoder = EnvelopeEncoder()\n","model = CLEE(eeg_encoder, env_encoder)\n","\n","# Test the model, add no grad\n","with torch.no_grad():\n","    print(eeg_encoder(X['eegs_train'][:32, :, :, :]).shape)\n","\n","summary(eeg_encoder, (1, 128, 640))\n","summary(env_encoder, (1, 1, 640))\n","summary(model, [(1, 128, 640), (1, 1, 640)])"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":8511458,"status":"ok","timestamp":1677781695965,"user":{"displayName":"Keyvan Mahjoory","userId":"07918596913136132352"},"user_tz":-60},"id":"d4iJiosNZQe7","outputId":"7cee43df-ead8-41a4-e403-46d0eee82c01"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------New model: SIMPLE----------------------+\n","====== Epoch: 1\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","====> Validation loss: 3.3886,  X1 loss: 3.3917   X2 loss: 3.3854\n","====== Epoch: 2\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n","_____\n"]}],"source":["models_dict = {'SIMPLE': model}\n","lossi = []\n","udri = [] # update / data ratio \n","ud = []\n","\n","lr = 0.001\n","\n","for name, model in models_dict.items():\n","\n","    # Reset for the new model in the loop\n","    print(f\"+--------------New model: {name}----------------------+\")\n","    writer = SummaryWriter(log_dir=f\"runs/{name}_{time.strftime('%Y%m%d_%H%M%S')}\")\n","    model.to(device)\n","    optimizer = optim.NAdam(model.parameters(), lr=lr)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.7)\n","    cnt = 0\n","    loss_batches = []\n","\n","\n","    for epoch in range(1, 50):\n","\n","        print(f\"====== Epoch: {epoch}\")\n","\n","        model.train()\n","        for ix_batch, (Xb_eeg, Xb_env) in enumerate(dl_train):\n","\n","            # send to device\n","            Xb_eeg = Xb_eeg.to(device)\n","            Xb_env = Xb_env.to(device)\n","\n","            # Zero out gradients\n","            optimizer.zero_grad()\n","\n","            # forward pass\n","            eeg_features, env_features, logit_scale = model(Xb_eeg, Xb_env) \n","\n","\n","            # normalize features\n","            eeg_features_n = eeg_features / eeg_features.norm(dim=1, keepdim=True)\n","            env_features_n = env_features / env_features.norm(dim=1, keepdim=True)\n","\n","            # logits\n","            logits_per_eeg = logit_scale * eeg_features_n @ env_features_n.t()\n","            logits_per_env = logits_per_eeg.t()\n","\n","            #loss function\n","            labels = torch.arange(batch_size).to(device)\n","            loss_eeg = F.cross_entropy(logits_per_eeg, labels)\n","            loss_env = F.cross_entropy(logits_per_env, labels)\n","            loss   = (loss_eeg + loss_env)/2\n","\n","            # backward pass\n","            loss.backward()\n","            optimizer.step()\n","\n","            loss_batches.append(loss.item())\n","            cnt += 1\n","\n","            with torch.no_grad():\n","                #ud = {f\"p{ix}\":(lr*p.grad.std() / p.data.std()).log10().item() for ix, p in enumerate(model.parameters()) if p.ndim==4 }\n","                #writer.add_scalars('UpdateOData/ud', ud, cnt)\n","                writer.add_scalar('Loss/train_batch', loss.item(), cnt)\n","\n","            # normalize weights\n","            with torch.no_grad():\n","                model.eeg_encoder.normalize_weights()\n","            \n","            #break   \n","\n","        loss_epoch = loss_batches[-(ix_batch + 1):]  # mean loss across batches\n","        loss_epoch = sum(loss_epoch) / len(loss_epoch)\n","        writer.add_scalar('Loss/train_epoch', loss_epoch, epoch)\n","        #for pname, p in model.named_parameters():\n","        #writer.add_histogram(f'Params/{pname}', p, epoch)\n","        #writer.add_histogram(f'Grads/{pname}', p.grad, epoch)\n","\n","        loss_val, *_ = eval_model_cl(dl_val, model, device=device)\n","        writer.add_scalar('Loss/val_epoch', loss_val, epoch)\n","\n","        \n","\n","        model.train()\n","\n","        # Update learning rate based on epoch\n","        scheduler.step()\n","            \n","    #break   \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["VhCK252zNjUG"],"machine_shape":"hm","provenance":[{"file_id":"13Poz5tFnEFXpegMbhqAinRdIfSvPnPge","timestamp":1677524195292}]},"gpuClass":"premium","kernelspec":{"display_name":"Python (mne)","language":"python","name":"mne"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"8e19e54895c02f0e9343d0fbd6cee45458aaf6f05de9ab3004d10bba5525a5d0"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"736632aa328443f7a1e9a85d67da40da":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0bb2a650ba0447d832f6a1bf47dd4b4":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_736632aa328443f7a1e9a85d67da40da","msg_id":"","outputs":[{"data":{"text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">     <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">7.6/7.6 MB</span> <span style=\"color: #800000; text-decoration-color: #800000\">64.9 MB/s</span> eta <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span>\n</pre>\n","text/plain":"     \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n"},"metadata":{},"output_type":"display_data"}]}}}}},"nbformat":4,"nbformat_minor":0}
